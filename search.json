[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\nStockton, B., Strange, C. C., & Harel, O. (2023). Now You See It, Now You Don’t: A Simulation and Illustration of the Importance of Treating Incomplete Data in Estimating Race Effects in Sentencing. Journal of Quantitative Criminology. https://doi.org/10.1007/s10940-023-09577-w"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog!\nIn this feed I will be making posts about both my statistical interests (missing data, Bayesian data analysis, etc) and possibly also about Major League Baseball. The posts will supplement my research and provide a space for sharing tutorials, interesting findings, and general thoughts."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#installation",
    "href": "posts/intro-to-stan/intro_to_stan.html#installation",
    "title": "Getting to know Stan - SWOSC",
    "section": "Installation",
    "text": "Installation\nTo get started, we’ll head over to Stan’s documentation to see how to get set up in R. For this presentation, I’ll use the CmdStan toolchain that’s implemented in R by the cmdstanr package (Gabry, Češnovar, and Johnson 2023). There are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called CmdStanPy (“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation,” n.d.).\n\n\n\nintro-to-stan.R\n\nset.seed(98463)\n\n# install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nlibrary(cmdstanr)\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/stocb/OneDrive/Documents/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\nCode\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\ncmdstanr::cmdstan_path()\ncmdstanr::check_cmdstan_toolchain()\n\n\nThe C++ toolchain required for CmdStan is setup properly!\n\n\nCode\n# install.packages(c(\"bayesplot\", \"ggplot2\", \"posterior\"))\n\n\n[1] \"2.33.1\"\n[1] \"C:/Users/stocb/OneDrive/Documents/.cmdstan/cmdstan-2.33.1\""
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "href": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "title": "Getting to know Stan - SWOSC",
    "section": "A Very Brief Introduction to Bayesian Data Analysis",
    "text": "A Very Brief Introduction to Bayesian Data Analysis\nThe broadest conceptual overview of Bayesian data analysis is that this methodology allows us to incorporate prior knowledge about the model/data into our analysis which is based on a posterior distribution that is derived from the prior and likelihood. Bayesian inference treats the parameters of the model as random variables whose distribution we are interested in either deriving analytically or approximating through computation. This is a key distinction from frequentist methods taught in math stat and applied stat where parameters are fixed values and their estimators are functions of a random sample and the sampling distribution of the estimator is used for inference.\n\n\n\n\n\n\nBayes Rule\n\n\n\nA quick reminder of Bayes Rule.\nLet \\(\\theta\\) be a random variable with (prior) distribution \\(p(\\theta)\\), \\(Y\\) be a random variable that depends on \\(\\theta\\) with conditional distribution or likelihood \\(p(y | \\theta)\\). Then their joint distribution is \\(p(y, \\theta) = p(y|\\theta) p(\\theta)\\).\nBayes rule lets us flip the conditioning from the likelihood to get \\(p(\\theta | y)\\)\n\\[\np(\\theta | y) = \\frac{p(y, \\theta)}{p(y)} = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta} \\propto p(y|\\theta) p(\\theta)\n\\]\n\n\nTo be a little more specific, we have three central components to the model. For ease of exposition, let’s consider the simple regression model \\(E(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) where we have \\(i = 1,\\dots,N\\) observations of \\((Y_i, X_i)'\\) from Ch. 14 of Bayesian Data Analysis (Gelman et al. 2013, 354–58). From here on I will suppress conditioning on the observed predictor \\(X_i = x_i\\) since we are considering the design matrix \\(X = (1_N, \\mathbf{x})\\) to be fixed and known where \\(\\mathbf{x} = (x_1,\\dots, x_N)'\\).\n\nThe Prior: a distribution for the parameters that doesn’t depend on the data \\(p(\\theta)\\).\nThe (Data) Likelihood: a model for the data that depends on the parameters \\(p(y | \\theta)\\).\nThe Posterior: a distribution that uses Bayes rule to define distribution of the parameters given the data \\(p(\\theta|y)\\)."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#why-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#why-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nStan is one of several ways to run MCMC for Bayesian inference\n\nNimble and OpenBUGS are two other languages dedicated to probabilistic programming\nR, Rcpp, Julia, and Python are other more general purpose options for writing the sampler from scratch\n\nOther methods use combinations of Gibbs, Metropolis-Hastings, and slice sampling; Stan uses Hamiltonian Monte Carlo and the No-U-Turn Sampler (NUTS) which is more efficient and typically requires less thinning\nStan only allows for continuous parameters"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some examples in Stan",
    "text": "Some examples in Stan\nWe’ll continue with the single predictor regression model to model NCAA Women’s Basketball team’s total wins by their 3 point field goal percentage from the 2022-2023 season. Data collected from NCAA.\n\n\n\nintro-to-stan.R\n\nncaaw &lt;- readr::read_csv(file = \"Data/NCAAW-freethrows-threes-2022-2023.csv\")\n\n\nRows: 350 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Team, WL\ndbl (9): G, FT, FTA, FTpct, FG3, FG3A, FG3pct, W, L\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the 2022-2023 season there were \\(N = 350\\) teams. The relationship between their wins and three point percentage is displayed in Figure 1.\n\n\n\nintro-to-stan.R\n\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\nFigure 1: Scatter plot of the Total Wins by 3 pt Field Goal %.\n\n\n\n\nAs a baseline, we’ll find the maximum likelihood estimates for the regression parameters and variance.\n\n\n\nintro-to-stan.R\n\nfit_ml &lt;- lm(W ~ FG3pct, data = ncaaw)\n(beta_ml &lt;- coef(fit_ml))\nsmry_ml &lt;- summary(fit_ml)\n(sigma_ml &lt;- smry_ml$sigma)\nmles &lt;- data.frame(Parameters = c(\"beta_0\", \"beta_1\", \"sigma\"),\n                   Estimates = c(beta_ml, sigma_ml))\n\n\n(Intercept)      FG3pct \n  -14.94468     1.00929 \n[1] 5.908144\n\n\nThe fitted line is displayed in Figure 2.\n\n\n\nintro-to-stan.R\n\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 2: Now the OLS regression line is super-imposed in blue.\n\n\n\n\n\nNon-informative Prior Regression Model\nLet’s consider the simple regression model \\(E(Y_i | X_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) (Gelman et al. 2013, 354–58).\n\nThe Prior: \\(p(\\boldsymbol{\\beta}, \\log\\sigma) = 1 \\equiv p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}\\)\nThe (Data) Likelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nThe Posterior:\n\n\\[\\begin{align*}\n    p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &= p(\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{y}) \\times p(\\sigma^2 | \\mathbf{y}) \\\\\n        &= N_2(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\beta}}, \\sigma^2 (X'X)^{-1}) \\times Inv-\\chi^2 (\\sigma^2 | N-2, s^2) \\\\\n    \\hat{\\boldsymbol{\\beta}} &= (X'X)^{-1} X'\\mathbf{y} \\\\\n    s^2 &= \\frac{1}{N-2} (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})' (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})   \n\\end{align*}\\]\nFirst we write the Stan code in a separate file. See the Stan User’s Guide Part 1.1 for programming this model without the analytic posteriors. (Download the file here)\n\n\nnon-informative-regression.stan\n\n// The input data is two vectors 'y' and 'X' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n}\n\ntransformed data {\n    matrix[N, 2] X_c = append_col(rep_vector(1, N), x);\n    matrix[2,2] XtX_inv = inverse(X_c' * X_c);\n\n    vector[2] beta_hat = XtX_inv * X_c' * y;\n    vector[N] y_hat = X_C * beta_hat;\n    \n    real&lt;lower=0&gt; s_2 = 1 / (N - 2) * (y - y_hat)' * (y - y_hat);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'beta' and 'sigma'.\nparameters {\n  vector beta;\n  real&lt;lower=0&gt; sigma; // Note that this is the variance\n}\n\n// The model to be estimated. We model the output\n// 'y' ~ N(x beta, sigma) by specifying the analytic\n// posterior defined above.\nmodel {\n  beta ~ multi_normal(beta_hat, sigma^2 * XtX_inv);\n  \n  sigma^2 ~ scaled_inv_chi_square(N-2, sqrt(s_2));\n}\n\ngenerated quantities {\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n\nNext we fit the model using cmdstanr. In this case, I will use 1000 warmup iterations, 1000 sampling iterations, with no thinning (thinning includes only every \\(k\\)th draw), and will refresh the print screen to see progress every 500 iterations. We can run several chains to see if where the chains start dictates any part of the posterior shape or location, and chains can be run in parallel to get more draws &lt;=&gt; better posterior approximation more quickly.\n\n\n\nintro-to-stan.R\n\ndata_list &lt;- list(\n    N = nrow(ncaaw),\n    y = ncaaw$W,\n    x = ncaaw$FG3pct\n)\n\nfile &lt;- file.path(\"non-informative-regression.stan\")\nnon_inf_model &lt;- cmdstan_model(file)\n\nfit1 &lt;- non_inf_model$sample(\n    data = data_list,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.4 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.9 seconds.\n\n\nNext we’ll check diagnostics for the sampler. First, we will look at the numeric diagnostic output from the method $diagnostic_summary() which reports if any transitions were divergent, if maximum tree depth was reached, and EBFMI. For this model and data set we don’t see any issues in these summaries.\nNext, we check the traceplots in Figure 3 (a). The MCMC draws can be collected from the fit object using the $draws() method. These plots display the sampled values for each parameter in a line plot. We are looking for a horizontal fuzzy bar. Then we can also look at density plots in Figure 3 (b) which will tell us if the chains reached reasonably similar densities. On both counts, we are in good shape. The plots are created using the bayesplot package.\n\n\n\nintro-to-stan.R\n\nfit1$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.158355 1.007918\n\n\n\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nCode\nmcmc_trace(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n\nintro-to-stan.R\n\nlibrary(bayesplot)\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\nFigure 3: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\n\nFinally, after checking that the MCMC chains and diagnostics look satisfactory, we can continue to inference. The summary statistics for the parameters are displayed in Table 1. This are generated by default with the $summary() method. The statistics include the posterior mean (mean), median (median), standard deviation (sd), mean absolute deviation (mad), and lower (q5) and upper bounds (q95) for a 90% credible interval. The statistics rhat, ess_bulk, and ess_tail are additional diagnostic measures that indicate how well the chains are sampling the posterior and how many effective draws we have made. Ideally rhat is very near 1, even 1.01 can be a significant problem. The effective sample sizes should be large/near the number of sampling iterations.\n\n\n\nintro-to-stan.R\n\nfit1$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\nTable 1: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-15.170201\n-15.17435\n2.7264741\n2.6386574\n-19.6968650\n-10.670060\n1.003857\n600.4229\n668.3881\n\n\nbeta[2]\n1.016536\n1.01743\n0.0876394\n0.0866691\n0.8718595\n1.159618\n1.004031\n596.0433\n630.5212\n\n\nsigma\n5.927539\n5.92005\n0.2185852\n0.2262596\n5.5846900\n6.297106\n1.001305\n862.2062\n822.5273\n\n\n\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nmles |&gt; \n    kableExtra::kbl(booktabs = TRUE, \n                    format = \"html\", digits = 3)\n\n\n\n\nTable 2: MLE estimates for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\nParameters\nEstimates\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n\n\nFG3pct\nbeta_1\n1.009\n\n\n\nsigma\n5.908\n\n\n\n\n\n\n\n\nWe can also check graphical summaries of this same information such as interval plots for each parameter’s credible intervals or density/area plots. In Figure 4 (a) we have the 50% (thick bar) and 95% (thin bar) credible intervals with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 4 (b) with areas shaded underneath to indicate the 50% interval and the width of the density indicates the 95% interval.\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_intervals(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit1$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\nFigure 4: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\nOne additional way to check model fit is to assess posterior predictive checks. To do so we draw samples from the posterior predictive distribution \\(p(y^{new} | y) = \\int p(y^{new} | \\boldsymbol{\\beta}, \\sigma) p(\\boldsymbol{\\beta}, \\sigma | y) d\\boldsymbol{\\beta}d\\sigma\\) by first sampling from the posterior (i.e. the draws in the MCMC chains) and then for each set of draws sampling \\(y^{new}\\) given the corresponding values for \\(x^{new}\\). In Stan this is easily accomplished using the generated quantities block. The generated quantities block generates new samples that we define using the current iteration’s posterior draws of \\(\\beta\\) and \\(\\sigma\\).\ngenerated quantities {\n    // create a vector of N new observations\n    vector[N] y_ppd; \n    \n    // for each observation, sample from the regression likelihod\n    // using the posterior draws\n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\nWe collect the PPD draws from the fit object using the draws method. From Figure 5 we can see that while the predictive densities are centered in the correct location, the variances are far too large.\n\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:bayesplot':\n\n    rhat\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nCode\ny_ppd &lt;- as.matrix(as_draws_df(fit1$draws(variables = \"y_ppd\")))\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\nintro-to-stan.R\n\nlibrary(posterior)\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\nFigure 5: Posterior Predictive Check plots from bayesplot.\n\n\n\n\n\nConjugate Prior Regression Model\nNext, we’ll implement the regression model with conjugate priors. Conjugacy refers to the situation where the prior and posterior distribution are from the same family. We’ll start by re-defining our model.1\n\nConjugate prior: \\(p(\\boldsymbol{\\beta}, \\sigma^2) = p(\\boldsymbol{\\beta} | \\sigma^2) p(\\sigma^2)\\)\n\n\\(\\boldsymbol{\\beta} | \\sigma^2 ~ N_2(\\boldsymbol{\\beta}_0, \\sigma^2 \\Lambda_0^{-1})\\) where \\(\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^2\\) is a vector of prior coefficients, we’ll set it to zero, and \\(\\Lambda_0\\) is a \\(2\\times2\\) prior correlation matrix. We will set \\(\\Lambda_0 = 10 I_2\\) to get a weakly informative prior that is equivalent to ridge regression.\n\\(\\sigma^2 \\sim InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2)\\) where \\(\\nu_0\\) is a prior sample size and \\(s_0\\) is the prior standard deviation. We’ll set these to \\(\\nu_0 = 1\\) and \\(s_0^2 = 47\\) which is approximately the sample variance of the NCAA women’s basketball teams’ wins.\nThe parameters \\(\\boldsymbol{\\beta}_0, \\Lambda_0, \\nu_0, s_0^2\\) that define the prior are referred to as hyperparameters. We will set them before running the model, although they could also be modeled if we wanted.\n\nThe (Data) Likelihood: the same as before, \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nPosterior:\n\n\\(\\boldsymbol{\\beta} | \\sigma^2, y \\sim N_2(\\boldsymbol{\\beta}_N, \\sigma^2 \\Lambda_N^{-1})\\) where \\(\\boldsymbol{\\beta}_N = \\Lambda_N^{-1}(\\mathbf{X}'\\mathbf{X} \\hat{\\boldsymbol{\\beta}} + \\Lambda_0 \\boldsymbol{\\beta}_0)\\) and \\(\\Lambda_N = (\\mathbf{X}'\\mathbf{X} + \\Lambda_0).\\)\n\\(\\sigma^2 | y \\sim InvGamma(\\sigma^2 | \\frac{\\nu_0 + N}{2}, \\frac{1}{2} \\nu_0 s_0^2 + \\frac{1}{2}(\\mathbf{y}'\\mathbf{y} + \\boldsymbol{\\beta}_0'\\Lambda_0 \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_N' \\Lambda_N \\boldsymbol{\\beta}_N)).\\)\n\n\nWe could again program this model using the analytic posterior. Instead, we’ll program it only through the priors and likelihood and let Stan approximate the posterior. I will also allow the model to include more than one predictor so that \\(\\mathbf{X}\\) is a \\(N \\times (K+1)\\) matrix augmented with a column of ones. (Download the file here)\n\n\nconjugate-regression.stan\n\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; K;\n  vector[N] y;\n  matrix[N, K] X;\n  \n  // hyperparameters\n  real beta_0;\n  real&lt;lower=0&gt; lambda_0;\n  real&lt;lower=0&gt; nu_0;\n  real&lt;lower=0&gt; s_02;\n}\n\ntransformed data {\n    matrix[N, K+1] X_mat = append_col(rep_vector(1, N), X);\n    vector[K+1] beta_0_vec = rep_vector(beta_0, K+1);\n    matrix[K+1, K+1] Lambda_0 = lambda_0 * identity_matrix(K+1);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  vector[K+1] beta;\n  real&lt;lower=0&gt; sigma2;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  beta ~ multi_normal(beta_0_vec, sigma2 * Lambda_0);\n  sigma2 ~ scaled_inv_chi_square(nu_0, s_02);\n  \n  y ~ normal(X_mat * beta, sqrt(sigma2));\n}\n\ngenerated quantities {\n    real sigma = sqrt(sigma2);\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_mat[i,] * beta, sqrt(sigma2));\n    }\n}\n\n\n\n\nintro-to-stan.R\n\ndata_list2 &lt;- list(\n    N = nrow(ncaaw),\n    K = 1,\n    y = ncaaw$W,\n    X = as.matrix(ncaaw$FG3pct, nrow = nrow(ncaaw)),\n    \n    # hyperparameters\n    beta_0 = 0,\n    lambda_0 = 0.5,\n    nu_0 = 1,\n    s_02 = 47\n)\n\nfile2 &lt;- file.path(\"conjugate-regression.stan\")\nconj_model &lt;- cmdstan_model(file2)\n\nfit2 &lt;- conj_model$sample(\n    data = data_list2,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.5 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 1.2 seconds.\n\n\n\n\n\nintro-to-stan.R\n\nfit2$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 0.9378670 0.9963986\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_trace(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\nFigure 6: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\nAgain the MCMC chains and diagnostics look satisfactory. The summary statistics for the parameters are displayed in Table 3.\n\n\n\nintro-to-stan.R\n\nfit2$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\nTable 3: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-10.1791619\n-10.239600\n2.5123209\n2.6138979\n-14.3625850\n-6.2677275\n1.001439\n521.5164\n574.8934\n\n\nbeta[2]\n0.8572707\n0.859365\n0.0810363\n0.0845927\n0.7317652\n0.9932626\n1.001122\n537.6317\n556.3745\n\n\nsigma\n5.9874354\n5.976395\n0.2422073\n0.2370974\n5.6090195\n6.4224640\n1.003337\n627.6261\n577.8970\n\n\n\n\n\n\n\n\nAnd here’s a quick reminder of our results for the non-informative prior and MLE fits in Table 4.\n\n\n\nintro-to-stan.R\n\nmcmc_summary &lt;- cbind(mles,\n  fit1$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\nmcmc_summary &lt;- cbind(mcmc_summary,\n  fit2$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\ncolnames(mcmc_summary) &lt;- c(\"Variable\", \"MLE\", \"Non-info Est\", \"Non-info SD\", \"Conj Est\", \"Conj SD\")\n\nmcmc_summary |&gt;\n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n\n\n\n\nTable 4: Comparison of the estimates for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\nVariable\nMLE\nNon-info Est\nNon-info SD\nConj Est\nConj SD\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n-15.170\n2.726\n-10.179\n2.512\n\n\nFG3pct\nbeta_1\n1.009\n1.017\n0.088\n0.857\n0.081\n\n\n\nsigma\n5.908\n5.928\n0.219\n5.987\n0.242\n\n\n\n\n\n\n\n\nIn Figure 7 (a) we have the 50% and 95% CIs with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 7 (b).\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_intervals(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit2$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\nFigure 7: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\nWe collect the PPD draws from the fit object using the draws method. From Figure 8 we can see that while the predictive densities match pretty well and the intervals are centered on the OLS line of best fit.\n\n\n\n\n\n\nintro-to-stan.R\n\ny_ppd &lt;- as.matrix(as_draws_df(fit2$draws(variables = \"y_ppd\")))\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\nFigure 8: Posterior Predictive Check plots from bayesplot."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some Other Useful Resources for Stan",
    "text": "Some Other Useful Resources for Stan\nFirst, here’s the three essential guides for using Stan:\n\nStan Function Guide - reference for all the built-in functions and distributions as well as guides for writing custom functions and distributions\nStan User’s Guide - reference for example models, how to build efficient models, and some inference techniques\nStan Reference Manual - reference for programming in Stan with a focus on how the language works\n\nHere are some other useful packages to use for Bayesian data analysis with Stan (or other packages). We used some of these in this tutorial!\n\nbrms: Bayesian regression models using Stan\nposterior: Useful for working with Stan output\nbayesplot: ggplot2-based plotting functions for MCMC draws designed work well with Stan\nloo: Leave-one-out cross validation for model checking and selection that works with the log-posterior. Works best with rstanarm but can work with cmdstanr too.\n\nHere’s a list of useful resources for debugging issues with divergences, hitting maximum tree-depth, low EBFMI, and understanding diagnostics:\n\nStan’s Guide to Runtime warnings and convergence problems\nPrior Choices and Selection\nConvergence Diagnostics for MCMC\nOfficial Stan Forum"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#bonus-regression-modeling-with-incomplete-data",
    "href": "posts/intro-to-stan/intro_to_stan.html#bonus-regression-modeling-with-incomplete-data",
    "title": "Getting to know Stan - SWOSC",
    "section": "Bonus: Regression Modeling with Incomplete Data",
    "text": "Bonus: Regression Modeling with Incomplete Data\nAs a bonus section, we’ll use the brms package to fit a regression model where we have incomplete predictor observations. Incomplete data analysis ranges from complete case analysis (incomplete cases are dropped) and mean imputation to multiple imputation, joint modeling, and EM algorithm (Schafer and Graham 2002).2 We’re going to use mice (Buuren and Groothuis-Oudshoorn 2010) and brms (Bürkner 2018) to demonstrate the imputation and fitting Bayesian regression models with a convenient front-end that writes the Stan code for us.\nIn our case, we are going to use junior year scoring (points per game) to predict senior year scoring for women’s college basketball players from 2020-21 to the 2022-23 seasons. The data set only contains players who played in at least 75% of games each season, so partial seasons due to injury or being a bench player are excluded. Players who only have a junior season are excluded from the analysis.\n\n\n\nintro-to-stan.R\n\nncaaw_i &lt;- read.csv(\"Data/ncaaw-individuals.csv\", header = TRUE)\nhead(ncaaw_i)\n\n\n             Name Pos_jr Pos_sr G_jr G_sr PPG_jr PPG_sr Cl_jr\n1     A'Jah Davis      F      F   29   32   16.6   16.2   Jr.\n2 Abby Brockmeyer      F      F   NA   31     NA   16.3   Jr.\n3       Abby Feit      F      F   29   28   15.1   15.5   Jr.\n4     Abby Meyers      G      G   NA   30     NA   17.9   Jr.\n5     Abby Meyers      G      G   NA   35     NA   14.3   Jr.\n6   Adriana Shipp      G      G   NA   30     NA   13.9   Jr.\n\n\nOur imputation model will be univariate linear regression that use all other variables as predictors. For example, imputing \\(PPG_{jr}\\) will be done by regressing on \\(PPG_{sr}, G_{jr}, G_{sr}\\). \\(PPG_{jr}\\) and \\(G_{jr}\\) are incomplete for \\(n_{mis} = 176\\) players while \\(n_{obs} = 98\\) players have stats from both years. This missing data pattern is displayed in Figure 9.\n\n\n\nintro-to-stan.R\n\nlibrary(mice)\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nCode\nm_pat &lt;- md.pattern(ncaaw_i, plot = TRUE)\n\n\n\n\n\nFigure 9: Missing data patterns for the NCAA women’s basketball players from 2020-2023 who played in their junior and senior year. The red boxes correspond to missing values, so there are 176 players who recorded full senior seasons (played in &gt;75% of total games) but missing or shortened junior seasons.\n\n\n\n\n\n\n\nintro-to-stan.R\n\nggplot(ncaaw_i, aes(PPG_jr, PPG_sr, color = G_jr)) +\n    geom_point() +\n    scale_color_viridis_c(name = \"G - Jr\") +\n    labs(x = \"PPG - Jr\",\n         y = \"PPG - Sr\") +\n    theme_bw()\n\n\n\n\n\nFigure 10: Points per game (PPG) from Junior and Senior seasons.\n\n\n\n\n\nMultiple Imputation with mice\nFirst, we’ll try out imputing before model fitting using mice. MICE stands for Multiple Imputation by Chained Equations and is procedure that creates a set of \\(M\\) completed data sets from an incomplete data set. Multiple Imputation is a three stage procedure:\n\nEach incomplete variable is imputed with posterior predictive draws from a regression model with all other variables as predictors. The procedure iterates through the incomplete variables several times to converge to the posterior predictive distribution of the missing data given the observed.\nThese completed data sets are then analyzed individually with a standard complete data method.\nResults from each analysis are combined. Typically this is done with Rubin’s rules (Rubin 1987), but brms follows the advice of Zhou and Reiter (2010) and simply stacks the posterior draw matrices from each fitted model.\n\n\n\n\nintro-to-stan.R\n\nlibrary(brms)\n\nimps &lt;- mice(ncaaw_i, m = 10, method = \"norm\", maxit = 10, printFlag = FALSE)\n\nfit_brm_mice &lt;- brm_multiple(PPG_sr ~ G_jr * PPG_jr, data = imps, chains = 2,\n                             refresh = 0)\nsummary(fit_brm_mice)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: PPG_sr ~ G_jr * PPG_jr \n   Data: imps (Number of observations: 274) \n  Draws: 20 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      18.23      6.45     5.81    31.16 1.49       37      106\nG_jr           -0.26      0.25    -0.77     0.23 1.51       36       94\nPPG_jr         -0.12      0.39    -0.89     0.62 1.55       35      110\nG_jr:PPG_jr     0.02      0.02    -0.01     0.05 1.57       34       94\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.21      0.10     2.01     2.42 1.11      118      372\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nImputation During Model Fitting\nImputation during model fitting takes a different approach. Imputations are made for each incomplete variable using a different conditional model for each variable. This approach differs from MI and MICE in two key ways: (i) the model is only fit once since the imputation model is part of the analysis model, (ii) the model must be constructed uniquely for each analysis scenario whereas MI completed data sets can be re-used with different analyses.\n\n\n\nintro-to-stan.R\n\nbform &lt;- bf(PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr)) +\n    bf(PPG_jr | mi() ~ G_sr + PPG_sr) +\n    bf(G_jr | mi() ~ G_sr + PPG_sr) + set_rescor(FALSE)\n\nfit_brm_mi &lt;- brm(bform, data = ncaaw_i, \n                  refresh = 500, iter = 2000, thin = 1,\n                  backend = \"cmdstanr\", \n                  control = list(adapt_delta = 0.8, \n                                 max_depth = 10,\n                                show_exceptions = FALSE),\n                  chains = 2,\n                  cores = 2)\n\nsummary(fit_brm_mi)\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr) \n         PPG_jr | mi() ~ G_sr + PPG_sr \n         G_jr | mi() ~ G_sr + PPG_sr \n   Data: ncaaw_i (Number of observations: 274) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nPPGsr_Intercept          16.10      2.29    11.60    20.74 1.00      615\nPPGjr_Intercept           5.95      2.33     1.14    10.46 1.00      900\nGjr_Intercept             2.98      6.31    -9.27    15.38 1.00      789\nPPGjr_G_sr               -0.04      0.06    -0.17     0.09 1.00     1053\nPPGjr_PPG_sr              0.72      0.08     0.56     0.89 1.00      732\nGjr_G_sr                  0.56      0.18     0.19     0.90 1.00      807\nGjr_PPG_sr                0.28      0.23    -0.16     0.72 1.00      622\nPPGsr_miG_jr             -0.30      0.09    -0.48    -0.12 1.00      565\nPPGsr_miPPG_jr           -0.04      0.15    -0.32     0.26 1.00      591\nPPGsr_miG_jr:miPPG_jr     0.02      0.01     0.01     0.03 1.00      544\n                      Tail_ESS\nPPGsr_Intercept            631\nPPGjr_Intercept           1221\nGjr_Intercept             1290\nPPGjr_G_sr                1244\nPPGjr_PPG_sr              1345\nGjr_G_sr                  1182\nGjr_PPG_sr                 874\nPPGsr_miG_jr               589\nPPGsr_miPPG_jr             736\nPPGsr_miG_jr:miPPG_jr      605\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_PPGsr     1.86      0.09     1.70     2.04 1.00     1706     1355\nsigma_PPGjr     2.31      0.15     2.04     2.63 1.00      634     1066\nsigma_Gjr       5.33      0.38     4.66     6.15 1.00      650     1214\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nbrms is built on Stan, so we can also take a look at the traceplots of the samples in Figure 11.\n\n\n\n\n\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGsr\", \"bsp_\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 11: Traceplots of brms analysis model parameters.\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGjr\", \"b_Gjr\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n(a) \\(PPG_{jr}\\) imputation model parameters\n\n\n\n\n\n\n\n\n\n(b) \\(G_{jr}\\) imputation model parameters\n\n\n\n\nFigure 12: Traceplots of brms imputation model parameters.\n\n\n\n\n\n\nintro-to-stan.R\n\nplot(brms::conditional_effects(fit_brm_mice, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\nplot(brms::conditional_effects(fit_brm_mi, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\n\n\n\n\n\n(a) Estimates after MICE imputation\n\n\n\n\n\n\n\n(b) Estimates with joint model\n\n\n\nFigure 13: The estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "href": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "title": "Getting to know Stan - SWOSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee wikipedia for more details and derivations: https://en.wikipedia.org/wiki/Bayesian_linear_regression↩︎\nSee White, Royston, and Wood (2011) for more details on incomplete data analysis.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben Stockton's Stats Site",
    "section": "",
    "text": "I am a 5th year PhD Candidate in the Department of Statistics at the University of Connecticut."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ben Stockton's Stats Site",
    "section": "",
    "text": "I am a 5th year PhD Candidate in the Department of Statistics at the University of Connecticut."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Ben Stockton's Stats Site",
    "section": "Research Interests",
    "text": "Research Interests\n\nMissing data\nAir pollution\nDirectional statistics\nSpatial statistics\nCausal inference"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ben Stockton's Stats Site",
    "section": "Education",
    "text": "Education\n\nGraduate\nUniversity of Connecticut | Storrs, CT\n\nPhD in Statistics (2019-Present)\n\n\n\nUndergraduate\nUniversity of Wisconsin-Whitewater | Whitewater, WI\n\nBS in Mathematics (emp. Statistics) with a minor in computer science (2016-2019)\n\nUniversity of Wisconsin-Madison | Madison, WI\n\nUndeclared (2015-2016)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD candidate in Statistics in my fifth year at the University of Connecticut under the supervision of Prof. Ofer Harel who serves as my major advisor. At UConn, I have served as a Teaching Assistant and Research Assistant. I graduated from the University of Wisconsin-Whitewater with a BS in mathematics (emphasis in statistics) and minor in computer science in the spring of 2015. I am originally from Fort Atkinson, WI.\n\n\n\n\nCV\nList of Statistics Journals for the aims and scopes of Statistics and Statistics-adjacent journals organized by general topic\n\n\n\n\n\n\n\n\nIn my non-stats time, I enjoy cooking, running, reading, and following Major League Baseball. I have run several 5ks, 10ks, half marathons, and the Madison Marathon in 2018."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I am a PhD candidate in Statistics in my fifth year at the University of Connecticut under the supervision of Prof. Ofer Harel who serves as my major advisor. At UConn, I have served as a Teaching Assistant and Research Assistant. I graduated from the University of Wisconsin-Whitewater with a BS in mathematics (emphasis in statistics) and minor in computer science in the spring of 2015. I am originally from Fort Atkinson, WI."
  },
  {
    "objectID": "about.html#cv-other-files",
    "href": "about.html#cv-other-files",
    "title": "About",
    "section": "",
    "text": "CV\nList of Statistics Journals for the aims and scopes of Statistics and Statistics-adjacent journals organized by general topic"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "",
    "text": "In my non-stats time, I enjoy cooking, running, reading, and following Major League Baseball. I have run several 5ks, 10ks, half marathons, and the Madison Marathon in 2018."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Getting to know Stan - SWOSC\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nBen Stockton\n\n\n\n\n\n\n  \n\n\n\n\nGetting to Know Stan\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nBen Stockton\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nBen Stockton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#installation",
    "href": "posts/stan-pres-swosc/stan_pres.html#installation",
    "title": "Getting to Know Stan",
    "section": "Installation",
    "text": "Installation\n\n\n\nI’ll use the CmdStan toolchain in R with the cmdstanr package (Gabry, Češnovar, and Johnson 2023).\nThere are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called CmdStanPy (“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation,” n.d.).\nYou need to have Rtools installed to then install CmdStan and brms.\n\n\n\n\n\nintro-to-stan.R\n\n# install.packages(\"cmdstanr\",\n#                  repos = c(\"https://mc-stan.org/r-packages/\",\n#                            getOption(\"repos\")))\nlibrary(cmdstanr)\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\ncmdstanr::cmdstan_path()\ncmdstanr::check_cmdstan_toolchain(fix = TRUE)\n\n# install.packages(c(\"bayesplot\", \"ggplot2\", \"posterior\"))\n\nset.seed(98463)\n\n\n[1] \"2.33.1\"\n[1] \"C:/Users/stocb/OneDrive/Documents/.cmdstan/cmdstan-2.33.1\"\n\n\n\n\n\n\nRun the install here and hopefully it’ll finish by the time we get to running code\nIf you want to follow along, copy and paste code from the post as we go\nAnd if you have Qs or if I’m going too fast, feel free to ask or interrupt me"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "href": "posts/stan-pres-swosc/stan_pres.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "title": "Getting to Know Stan",
    "section": "A Very Brief Introduction to Bayesian Data Analysis",
    "text": "A Very Brief Introduction to Bayesian Data Analysis\n\nIncorporate prior knowledge about the model/data into our analysis\nBayesian inference treats the parameters of the model as random variables\n\n\n\n\n\n\n\nBayes Rule\n\n\nLet \\(\\theta\\) be a r.v. with (prior) distribution \\(p(\\theta)\\), \\(Y\\) be a r.v. with likelihood \\(p(y | \\theta)\\). Their joint distribution is \\(p(y, \\theta) = p(y|\\theta) p(\\theta)\\).\nBayes rule lets us flip the conditioning from the likelihood to get\n\\[\np(\\theta | y) = \\frac{p(y, \\theta)}{p(y)} = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta} \\propto p(y|\\theta) p(\\theta)\n\\]"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#the-components-of-bayesian-inference",
    "href": "posts/stan-pres-swosc/stan_pres.html#the-components-of-bayesian-inference",
    "title": "Getting to Know Stan",
    "section": "The Components of Bayesian Inference",
    "text": "The Components of Bayesian Inference\nTwo sets of random variables to model: parameters \\(\\theta\\) and data \\(Y\\)\n\nThe Prior: \\(p(\\theta)\\) does not depend on \\(Y\\)\nThe (Data) Likelihood: \\(p(y | \\theta)\\) models \\(Y\\) dependent on \\(\\theta\\)\nThe Posterior: \\(p(\\theta | y)\\) models \\(\\theta\\) given \\(Y\\)\n\n\\[\\begin{align*}\n    p(\\theta | y) &= \\frac{p(\\theta, y)}{p(y)} \\\\\n        &= \\frac{p(\\theta) \\times p(y | \\theta)}{\\int p(y|\\theta) p(\\theta)} \\\\\n        &\\propto p(\\theta) p(y | \\theta) \\\\\n\\end{align*}\\]\nMake inferences about \\(\\theta\\) with the posterior distribution"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#why-stan",
    "href": "posts/stan-pres-swosc/stan_pres.html#why-stan",
    "title": "Getting to Know Stan",
    "section": "Why Stan?",
    "text": "Why Stan?\n\n\n\nStan is one of several ways to run MCMC for Bayesian inference\n\nNimble, OpenBUGS, R, Rcpp, Julia are other options\n\nOther methods use combinations of Gibbs, Metropolis-Hastings, and slice sampling; Stan uses Hamiltonian Monte Carlo and the No-U-Turn Sampler (NUTS) which is more efficient\nStan only allows for continuous parameters"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#ncaa-womens-basketball",
    "href": "posts/stan-pres-swosc/stan_pres.html#ncaa-womens-basketball",
    "title": "Getting to Know Stan",
    "section": "NCAA Women’s Basketball",
    "text": "NCAA Women’s Basketball\n\n\nWe’ll model NCAA Women’s Basketball team’s total wins by their 3 point field goal percentage from the 2022-2023 season. CSV file available here\n\n\n\nintro-to-stan.R\n\nncaaw &lt;- readr::read_csv(file = \"Data/NCAAW-freethrows-threes-2022-2023.csv\")\n\n\nIn the 2022-2023 season there were \\(N = 350\\) teams.\n\n\n\n\nintro-to-stan.R\n\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\", y = \"Wins\") +\n    theme_bw()\n\n\n\n\n\nFigure 1: Scatter plot of the Total Wins by 3 pt Field Goal %.\n\n\n\n\n\n\n\n\nThe data are 2022-2023 season NCAA women’s basketball team 3pt % and Wins\nN=350\nData are basically linear, and moderately correlated"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#baseline-ml-estimates",
    "href": "posts/stan-pres-swosc/stan_pres.html#baseline-ml-estimates",
    "title": "Getting to Know Stan",
    "section": "Baseline ML Estimates",
    "text": "Baseline ML Estimates\n\n\nAs a baseline, we’ll find the maximum likelihood estimates for the regression parameters and variance.\n\n\n\n\nintro-to-stan.R\n\nfit_ml &lt;- lm(W ~ FG3pct, data = ncaaw)\n(beta_ml &lt;- coef(fit_ml))\nsmry_ml &lt;- summary(fit_ml)\n(sigma_ml &lt;- smry_ml$sigma)\nmles &lt;- data.frame(Parameters = c(\"beta_0\", \"beta_1\", \"sigma\"),\n                   Estimates = c(beta_ml, sigma_ml))\n\n\n(Intercept)      FG3pct \n  -14.94468     1.00929 \n[1] 5.908144\n\n\n\n\n\nintro-to-stan.R\n\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\", y = \"Wins\") +\n    theme_bw()\n\n\n\n\n\nFigure 2: Now the OLS regression line is super-imposed in blue.\n\n\n\n\n\n\n\n\nWe’ll do the MLE fit with the lm() and keep the point estimates for comparison"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-set-up",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-set-up",
    "title": "Getting to Know Stan",
    "section": "Model #1 Set-up",
    "text": "Model #1 Set-up\nLet’s consider the simple regression model \\(E(Y_i | X_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) (Gelman et al. 2013, 354–58).\n\nThe Prior: \\(p(\\boldsymbol{\\beta}, \\log\\sigma) = 1 \\equiv p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}\\)\nThe (Data) Likelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nThe Posterior:\n\n\\[\\begin{align*}\n    p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &= p(\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{y}) \\times p(\\sigma^2 | \\mathbf{y}) \\\\\n        &= N_2(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\beta}}, \\sigma^2 (X'X)^{-1}) \\times Inv-\\chi^2 (\\sigma^2 | N-2, s^2) \\\\\n    \\hat{\\boldsymbol{\\beta}} &= (X'X)^{-1} X'\\mathbf{y} \\\\\n    s^2 &= \\frac{1}{N-2} (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})' (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})   \n\\end{align*}\\]\n\n\nPrior is uniform on beta and log sigma which then gives the prior propto sigma^-2, example of Jeffreys prior\nStandard vector notation for linear regression model. Observations are independent since covariance is diagonal\nPosterior is a product of Normal and Scaled Inv Chisq\nNote that beta’s posterior is centered at MLE of beta and scale parameter of sigma^2’s posterior is residual variance so that it’s posterior mean is s^2\nThis posterior has a closed form"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#non-informative-prior-regression-stan-code",
    "href": "posts/stan-pres-swosc/stan_pres.html#non-informative-prior-regression-stan-code",
    "title": "Getting to Know Stan",
    "section": "Non-informative Prior Regression Stan Code",
    "text": "Non-informative Prior Regression Stan Code\n\n\nnon-informative-regression.stan\n\n// The input data is two vectors 'y' and 'X' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n}\n\ntransformed data {\n    matrix[N, 2] X_c = append_col(rep_vector(1, N), x);\n    matrix[2,2] XtX_inv = inverse(X_c' * X_c);\n\n    vector[2] beta_hat = XtX_inv * X_c' * y;\n    vector[N] y_hat = X_C * beta_hat;\n    \n    real&lt;lower=0&gt; s_2 = 1 / (N - 2) * (y - y_hat)' * (y - y_hat);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'beta' and 'sigma'.\nparameters {\n  vector[2] beta;\n  real&lt;lower=0&gt; sigma; // Note that this is the variance\n}\n\n// The model to be estimated. We model the output\n// 'y' ~ N(x beta, sigma) by specifying the analytic\n// posterior defined above.\nmodel {\n  beta ~ multi_normal(beta_hat, sigma^2 * XtX_inv);\n  \n  sigma^2 ~ scaled_inv_chi_square(N-2, sqrt(s_2));\n}\n\ngenerated quantities {\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n\n\n\nDiscuss each section in detail\nNote size of beta vector, parameterizations of scaled_inv_chi_square() and normal()\nRestate that we use the analytic posterior for efficiency when we can"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-fitting",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-fitting",
    "title": "Getting to Know Stan",
    "section": "Model #1 Fitting",
    "text": "Model #1 Fitting\n\n\n\nFirst we write the Stan code in a separate file1.\n1000 warmup iterations, 1000 sampling iterations\nNo thinning (thinning includes only every \\(n\\)th draw)\nRefresh the print screen to see progress every 500 iterations.\nRun several chains (in parallel)\n\n\n\n\n\nintro-to-stan.R\n\ndata_list &lt;- list(\n    N = nrow(ncaaw),\n    y = ncaaw$W,\n    x = ncaaw$FG3pct\n)\n\nfile &lt;- file.path(\"non-informative-regression.stan\")\nnon_inf_model &lt;- cmdstan_model(file)\n\nfit1 &lt;- non_inf_model$sample(\n    data = data_list,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    # show_exceptions = FALSE\n)\n\n\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.4 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.4 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 1.0 seconds.\n\n\n\n\n\n\nDiscuss the code in depth\nExplain why we use warmup and sampling iterations\nThat 2000 is far fewer than required by standard MCMC\nNo thinning, could be used to avoid high autocorrelation at more than lag 1\nWhy do we run chains?\nDiscuss output\nOrange exception text\n\n\n\nSee the Stan User’s Guide Part 1.1 for programming this model without the analytic posteriors."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-diagnostics",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-diagnostics",
    "title": "Getting to Know Stan",
    "section": "Model #1 Diagnostics",
    "text": "Model #1 Diagnostics\n\n\n\nDiagnostic summary generated by Stan\nPlots created using the bayesplot package.\n\nTrace plots\nDensity Plots\nACF Plots\n\n\n\n\n\n\nintro-to-stan.R\n\nfit1$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.158642 1.141270\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nlibrary(bayesplot)\n\nmcmc_trace(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_dens_overlay(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_acf_bar(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\nFigure 3: ?(caption)"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-diagnostics-output",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-diagnostics-output",
    "title": "Getting to Know Stan",
    "section": "Model #1 Diagnostics",
    "text": "Model #1 Diagnostics\n\n\n\n\n\nFigure 4: Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 5: Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 6: Autocorrelation Function (ACF) plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\nDiagnostic plots from the bayesplot package."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-inference",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-inference",
    "title": "Getting to Know Stan",
    "section": "Model #1 Inference",
    "text": "Model #1 Inference\n\nThe summary statistics are displayed in Table 1.\nThe statistics rhat, ess_bulk, and ess_tail are additional diagnostics.\n\n\n\n\nintro-to-stan.R\n\nfit1$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n\n\n\n\n\n\nTable 1: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-15.011\n-15.004\n2.840\n2.733\n-19.745\n-10.360\n1.003\n650.741\n632.850\n\n\nbeta[2]\n1.011\n1.011\n0.091\n0.087\n0.865\n1.163\n1.003\n643.710\n581.296\n\n\nsigma\n5.916\n5.916\n0.213\n0.220\n5.578\n6.280\n1.003\n735.732\n664.384\n\n\n\n\n\n\n\n\n\n\nmles |&gt; \n    kableExtra::kbl(booktabs = TRUE, \n                    format = \"html\", digits = 3)\n\n\n\n\nTable 2: MLE estimates for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\nParameters\nEstimates\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n\n\nFG3pct\nbeta_1\n1.009\n\n\n\nsigma\n5.908"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-graphical-summaries",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-graphical-summaries",
    "title": "Getting to Know Stan",
    "section": "Model #1 Graphical Summaries",
    "text": "Model #1 Graphical Summaries\n\nFigure 8 displays 50% (thick bar) and 95% (thin bar) credible intervals with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 9.\nPlots were made using the bayesplot package.\n\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_intervals(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit1$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\n\n\n\nFigure 8: Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 9: Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-posterior-predictive-checks",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-posterior-predictive-checks",
    "title": "Getting to Know Stan",
    "section": "Model #1 Posterior Predictive Checks",
    "text": "Model #1 Posterior Predictive Checks\n\nOne way to check model fit is to assess posterior predictive distrubtion.\nDraw samples from the posterior predictive distribution \\(p(y^{new} | y) = \\int p(y^{new} | \\boldsymbol{\\beta}, \\sigma) p(\\boldsymbol{\\beta}, \\sigma | y) d\\boldsymbol{\\beta}d\\sigma\\) by\n\n\nSampling from the posterior (i.e. the draws in the MCMC chains)\nFor each set of draws sampling \\(y^{new}\\) given the corresponding values for \\(x^{new}\\)\n\n\nIn Stan this is easily accomplished using the generated quantities block.\n\n\n\nnon-informative-regression.stan\n\ngenerated quantities {\n    // create a vector of N new observations\n    vector[N] y_ppd; \n    \n    // for each observation, sample from the regression likelihod\n    // using the posterior draws\n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n\n\nSay at the beginning of the slide that this doesn’t need to be copied"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-1-posterior-predictive-plots",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-1-posterior-predictive-plots",
    "title": "Getting to Know Stan",
    "section": "Model #1 Posterior Predictive Plots",
    "text": "Model #1 Posterior Predictive Plots\n\n\n\n\n\n\nintro-to-stan.R\n\nlibrary(posterior)\ny_ppd &lt;- as.matrix(as_draws_df(fit1$draws(variables = \"y_ppd\")))\nppc_dens_overlay(ncaaw$W, y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\", x = \"Wins\")\nppc_intervals(ncaaw$W, y_ppd[1:50, 1:350], x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\", y = \"Wins\")\n\n\n\nFigure 10: ?(caption)\n\n\n\n\n\n\n\n\nFigure 11: PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\nFigure 12: PPD intervals for the wins plotted by 3pt%.\n\n\n\n\n\n\nPosterior Predictive Check plots from bayesplot."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-set-up",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-set-up",
    "title": "Getting to Know Stan",
    "section": "Model #2 Set-up",
    "text": "Model #2 Set-up\nNext, we’ll implement the regression model with conjugate priors. Conjugacy refers to the situation where the prior and posterior distribution are from the same family.1\n\nConjugate prior: \\(p(\\boldsymbol{\\beta}, \\sigma^2) = p(\\boldsymbol{\\beta} | \\sigma^2) p(\\sigma^2)\\)\n\n\\(\\boldsymbol{\\beta} | \\sigma^2 ~ N_2(\\boldsymbol{\\beta}_0, \\sigma^2 \\Lambda_0^{-1})\\) where \\(\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^2\\) is a vector of prior coefficients, and \\(\\Lambda_0\\) is a \\(2\\times2\\) prior correlation matrix. \\(\\boldsymbol{\\beta}_0 = 0\\) and \\(\\Lambda_0 = \\lambda I_2 = 10 I_2\\) to get a weakly informative prior that is equivalent to ridge regression.\n\\(\\sigma^2 \\sim InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2)\\) where \\(\\nu_0\\) is a prior sample size and \\(s_0\\) is the prior standard deviation. We’ll set these to \\(\\nu_0 = 1\\) and \\(s_0^2 = 47\\) the sample variance of the teams’ wins.\nThe parameters \\(\\boldsymbol{\\beta}_0, \\Lambda_0, \\nu_0, s_0^2\\) are hyperparameters.\n\nThe (Data) Likelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nPosterior:\n\n\\(\\boldsymbol{\\beta} | \\sigma^2, y \\sim N_2(\\boldsymbol{\\beta}_N, \\sigma^2 \\Lambda_N^{-1})\\) where \\(\\boldsymbol{\\beta}_N = \\Lambda_N^{-1}(\\mathbf{X}'\\mathbf{X} \\hat{\\boldsymbol{\\beta}} + \\Lambda_0 \\boldsymbol{\\beta}_0)\\) and \\(\\Lambda_N = (\\mathbf{X}'\\mathbf{X} + \\Lambda_0).\\)\n\\(\\sigma^2 | y \\sim InvGamma(\\sigma^2 | \\frac{\\nu_0 + N}{2}, \\frac{1}{2} \\nu_0 s_0^2 + \\frac{1}{2}(\\mathbf{y}'\\mathbf{y} + \\boldsymbol{\\beta}_0'\\Lambda_0 \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_N' \\Lambda_N \\boldsymbol{\\beta}_N)).\\)\n\n\n\n\nNote that this is equivalent to ridge regression with shrinkage parameter 1/lambda\n\n\nWorks best with rstanarm but can work with cmdstanr too."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#conjugate-prior-regression-stan-code",
    "href": "posts/stan-pres-swosc/stan_pres.html#conjugate-prior-regression-stan-code",
    "title": "Getting to Know Stan",
    "section": "Conjugate Prior Regression Stan Code",
    "text": "Conjugate Prior Regression Stan Code\n\n\nconjugate-regression.stan\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; K;\n  vector[N] y;\n  matrix[N, K] X;\n  \n  // hyperparameters\n  real beta_0;\n  real&lt;lower=0&gt; lambda_0;\n  real&lt;lower=0&gt; nu_0;\n  real&lt;lower=0&gt; s_02;\n}\n\ntransformed data {\n    matrix[N, K+1] X_mat = append_col(rep_vector(1, N), X);\n    vector[K+1] beta_0_vec = rep_vector(beta_0, K+1);\n    matrix[K+1, K+1] Lambda_0 = lambda_0 * identity_matrix(K+1);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  vector[K+1] beta;\n  real&lt;lower=0&gt; sigma2;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  beta ~ multi_normal(beta_0_vec, sigma2 * Lambda_0);\n  sigma2 ~ scaled_inv_chi_square(nu_0, sqrt(s_02));\n  \n  y ~ normal(X_mat * beta, sqrt(sigma2));\n}\n\ngenerated quantities {\n    real sigma = sqrt(sigma2);\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_mat[i,] * beta, sqrt(sigma2));\n    }\n}\n\n\n\nNote that we are allowing for additional predictors here"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-fitting",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-fitting",
    "title": "Getting to Know Stan",
    "section": "Model #2 Fitting",
    "text": "Model #2 Fitting\n\n\n\nProgram the model only through the priors and likelihood and let Stan approximate the posterior\n1000 warmup iterations, 1000 sampling iterations\nNo thinning (thinning includes only every \\(k\\)th draw)\nRefresh the print screen to see progress every 500 iterations.\nRun several chains (in parallel)\n\n\n\n\n\nintro-to-stan.R\n\ndata_list2 &lt;- list(\n    N = nrow(ncaaw),\n    K = 1,\n    y = ncaaw$W,\n    X = as.matrix(ncaaw$FG3pct, nrow = nrow(ncaaw)),\n    \n    # hyperparameters\n    beta_0 = 0,\n    lambda_0 = 0.5,\n    nu_0 = 1,\n    s_02 = 47\n)\n\nfile2 &lt;- file.path(\"conjugate-regression.stan\")\nconj_model &lt;- cmdstan_model(file2)\n\nfit2 &lt;- conj_model$sample(\n    data = data_list2,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    # show_exceptions = FALSE\n)\n\n\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.6 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.5 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 1.2 seconds."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-diagnostics",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-diagnostics",
    "title": "Getting to Know Stan",
    "section": "Model #2 Diagnostics",
    "text": "Model #2 Diagnostics\n\n\n\nintro-to-stan.R\n\nfit2$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.059917 1.022045\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_trace(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_dens_overlay(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_acf_bar(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\nFigure 13: ?(caption)\n\n\n\n\n\n\n\n\nFigure 14: Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 15: Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 16: ACF plots for \\(\\beta\\) and \\(\\sigma\\)."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-inference",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-inference",
    "title": "Getting to Know Stan",
    "section": "Model #2 Inference",
    "text": "Model #2 Inference\n\nEstimates are similar to Model #1, but regression coefficients are shrunk slightly to zero and variance is slightly higher\nFigure 18 displayes credible intervals and densities are plotted as ridgelines in Figure 19.\n\n\n\n\nintro-to-stan.R\n\nmcmc_summary &lt;- cbind(mles,\n  fit1$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\nmcmc_summary &lt;- cbind(mcmc_summary,\n  fit2$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\ncolnames(mcmc_summary) &lt;- c(\"Variable\", \"MLE\", \"Non-info Est\", \"Non-info SD\", \"Conj Est\", \"Conj SD\")\n\nmcmc_summary |&gt;\n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n\n\n\n\n\n\nTable 3: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\nVariable\nMLE\nNon-info Est\nNon-info SD\nConj Est\nConj SD\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n-15.011\n2.840\n-10.077\n2.373\n\n\nFG3pct\nbeta_1\n1.009\n1.011\n0.091\n0.854\n0.076\n\n\n\nsigma\n5.908\n5.916\n0.213\n5.986\n0.228"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-graphical-summaries",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-graphical-summaries",
    "title": "Getting to Know Stan",
    "section": "Model #2 Graphical Summaries",
    "text": "Model #2 Graphical Summaries\n\n\n\n\n\n\nintro-to-stan.R\n\nmcmc_intervals(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit2$draws(variables = c(\"beta\", \"sigma\")), prob_outer = 0.95, prob = 0.5)\n\n\n\nFigure 17: ?(caption)\n\n\n\n\n\n\n\n\nFigure 18: Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 19: Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\n\n\nPlots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#model-2-posterior-predictive-distribution",
    "href": "posts/stan-pres-swosc/stan_pres.html#model-2-posterior-predictive-distribution",
    "title": "Getting to Know Stan",
    "section": "Model #2 Posterior Predictive Distribution",
    "text": "Model #2 Posterior Predictive Distribution\n\n\n\n\n\n\nintro-to-stan.R\n\ny_ppd &lt;- as.matrix(as_draws_df(fit2$draws(variables = \"y_ppd\")))\nppc_dens_overlay(ncaaw$W, y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\", x = \"Wins\")\nppc_intervals(ncaaw$W,\n              y_ppd[1:50, 1:350], x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\", y = \"Wins\")\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\nFigure 20: ?(caption)"
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#guides-for-stan",
    "href": "posts/stan-pres-swosc/stan_pres.html#guides-for-stan",
    "title": "Getting to Know Stan",
    "section": "Guides for Stan",
    "text": "Guides for Stan\n\n\nFirst, here’s the three essential guides for using Stan:\n\nStan Function Guide - reference for all the built-in functions and distributions\nStan User’s Guide - reference for example models, how to build efficient models, and some inference techniques\nStan Reference Manual - reference for programming in Stan with a focus on how the language works\n\n\nOther Stan Packages\n\nbrms: Bayesian regression models using Stan\nposterior: Useful for working with Stan output\nbayesplot: ggplot2-based plotting functions for MCMC draws designed work well with Stan\nloo: Leave-one-out cross validation for model checking and selection that works with the log-posterior.1\n\nGuides to Debugging and Diagnostics\n\nStan’s Guide to Runtime warnings and convergence problems\nPrior Choices and Selection\nConvergence Diagnostics for MCMC\nOfficial Stan Forum\n\n\n\nWorks best with rstanarm but can work with cmdstanr too."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#bonus-regression-modeling-with-incomplete-data",
    "href": "posts/stan-pres-swosc/stan_pres.html#bonus-regression-modeling-with-incomplete-data",
    "title": "Getting to Know Stan",
    "section": "Bonus: Regression Modeling with Incomplete Data",
    "text": "Bonus: Regression Modeling with Incomplete Data\n\nLet’s use the brms package to fit a regression model with incomplete predictor observations.\nIncomplete data analysis ranges from complete case analysis to multiple imputation, joint modeling, and EM algorithm (Schafer and Graham 2002).1\nWe’re going to use mice (Buuren and Groothuis-Oudshoorn 2010) and brms (Bürkner 2018) to demonstrate the imputation and fitting Bayesian regression models.\n\nSee White, Royston, and Wood (2011) for more details on incomplete data analysis."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#ncaa-womens-basketball-players-junior-and-senior-years",
    "href": "posts/stan-pres-swosc/stan_pres.html#ncaa-womens-basketball-players-junior-and-senior-years",
    "title": "Getting to Know Stan",
    "section": "NCAA Women’s Basketball Player’s Junior and Senior Years",
    "text": "NCAA Women’s Basketball Player’s Junior and Senior Years\n\n\n\nWe’ll use junior year scoring (points per game/PPG) to predict senior year scoring for 2020-21 to the 2022-23 seasons. CSV available here\nThe data set only contains players who played in at least 75% of games each season, so partial seasons due to injury or being a bench player are excluded.\nPlayers who only have a junior season are excluded from the analysis.\n\n\n\n\n\nintro-to-stan.R\n\nncaaw_i &lt;- read.csv(\"Data/ncaaw-individuals.csv\", header = TRUE)\nhead(ncaaw_i)\n\n\n             Name Pos_jr Pos_sr G_jr G_sr PPG_jr PPG_sr Cl_jr\n1     A'Jah Davis      F      F   29   32   16.6   16.2   Jr.\n2 Abby Brockmeyer      F      F   NA   31     NA   16.3   Jr.\n3       Abby Feit      F      F   29   28   15.1   15.5   Jr.\n4     Abby Meyers      G      G   NA   30     NA   17.9   Jr.\n5     Abby Meyers      G      G   NA   35     NA   14.3   Jr.\n6   Adriana Shipp      G      G   NA   30     NA   13.9   Jr.\n\n\n\n\n\nintro-to-stan.R\n\nggplot(ncaaw_i, aes(PPG_jr, PPG_sr, color = G_jr)) +\n    geom_point(size = 1.5) +\n    scale_color_viridis_c(name = \"G - Jr\") +\n    labs(x = \"PPG - Jr\", y = \"PPG - Sr\") +\n    theme_bw()\n\n\n\n\n\n\n\nFigure 21: Points per game (PPG) from Junior and Senior seasons."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#incomplete-data-structure-imputation-model",
    "href": "posts/stan-pres-swosc/stan_pres.html#incomplete-data-structure-imputation-model",
    "title": "Getting to Know Stan",
    "section": "Incomplete Data Structure & Imputation Model",
    "text": "Incomplete Data Structure & Imputation Model\n\nThe imputation model will be univariate linear regression that use all other variables as predictors.\n\nFor example, imputing \\(PPG_{jr}\\) will be done by regressing on \\(PPG_{sr}, G_{jr}, G_{sr}\\).\n\n\\(PPG_{jr}\\) and \\(G_{jr}\\) are incomplete for \\(n_{mis} = 176\\) players while \\(n_{obs} = 98\\) players have stats from both years as displayed in Figure 22.\n\n\n\n\nintro-to-stan.R\n\n# install.packages(c(\"mice\", \"brms\"))\n\nlibrary(mice)\nm_pat &lt;- md.pattern(ncaaw_i, plot = TRUE)\n\n\n\nFigure 22: Missing data patterns for the NCAA women’s basketball players from 2020-2023 who played in their junior and senior year. The red boxes correspond to missing values, so there are 176 players who recorded full senior seasons (played in &gt;75% of total games) but missing or shortened junior seasons."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#multiple-imputation-with-mice",
    "href": "posts/stan-pres-swosc/stan_pres.html#multiple-imputation-with-mice",
    "title": "Getting to Know Stan",
    "section": "Multiple Imputation with mice",
    "text": "Multiple Imputation with mice\nFirst, we’ll impute before model fitting using mice.\nMultiple Imputation (by Chained Equations) is a three stage procedure:\n\nEach incomplete variable is imputed \\(M\\) times with posterior predictive draws from a regression model with all other variables as predictors. The procedure iterates through the incomplete variables several times to converge to the posterior predictive distribution of the missing data given the observed.\nThese completed data sets are then analyzed individually with a standard complete data method.\nResults from each analysis are combined. Typically this is done with Rubin’s rules (Rubin 1987), but brms follows the advice of Zhou and Reiter (2010) and simply stacks the posterior draw matrices from each fitted model.\n\n\n\n\nintro-to-stan.R\n\nlibrary(brms)\nimps &lt;- mice(ncaaw_i, m = 10, method = \"norm\", maxit = 10, printFlag = FALSE)\nfit_brm_mice &lt;- brm_multiple(PPG_sr ~ G_jr * PPG_jr, data = imps, chains = 2, refresh = 0)\nsummary(fit_brm_mice)\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: PPG_sr ~ G_jr * PPG_jr \n   Data: imps (Number of observations: 274) \n  Draws: 20 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      17.59      4.94     8.04    27.62 1.18       75      253\nG_jr           -0.24      0.20    -0.65     0.14 1.24       59      251\nPPG_jr         -0.11      0.29    -0.70     0.45 1.20       70      230\nG_jr:PPG_jr     0.02      0.01    -0.01     0.04 1.27       55      268\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.19      0.11     1.98     2.41 1.15       85      260\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#imputation-during-model-fitting",
    "href": "posts/stan-pres-swosc/stan_pres.html#imputation-during-model-fitting",
    "title": "Getting to Know Stan",
    "section": "Imputation During Model Fitting",
    "text": "Imputation During Model Fitting\n\nImputations are made for each incomplete variable using a different conditional model for each variable.\nThis approach differs from MI and MICE in two key ways:\n\nThe model is only fit once since the imputation model is part of the analysis model.\nThe model must be constructed uniquely for each analysis scenario.\n\n\n\n\nbform &lt;- bf(PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr)) +\n    bf(PPG_jr | mi() ~ G_sr + PPG_sr) +\n    bf(G_jr | mi() ~ G_sr + PPG_sr) + set_rescor(FALSE)\nfit_brm_mi &lt;- brm(bform, data = ncaaw_i, \n                  refresh = 500, iter = 2000, thin = 1,\n                  backend = \"cmdstanr\",\n                  control = list(show_exceptions = FALSE),\n                  chains = 2, cores = 2)\nsummary(fit_brm_mi)\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr) \n         PPG_jr | mi() ~ G_sr + PPG_sr \n         G_jr | mi() ~ G_sr + PPG_sr \n   Data: ncaaw_i (Number of observations: 274) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nPPGsr_Intercept          16.12      2.42    11.19    21.07 1.00      436\nPPGjr_Intercept           5.89      2.45     1.22    10.50 1.00      793\nGjr_Intercept             3.13      5.87    -7.74    15.25 1.00      736\nPPGjr_G_sr               -0.04      0.07    -0.17     0.09 1.00      829\nPPGjr_PPG_sr              0.71      0.08     0.54     0.87 1.00      649\nGjr_G_sr                  0.55      0.17     0.21     0.89 1.00      733\nGjr_PPG_sr                0.28      0.23    -0.16     0.73 1.01      414\nPPGsr_miG_jr             -0.30      0.10    -0.49    -0.10 1.00      432\nPPGsr_miPPG_jr           -0.04      0.15    -0.34     0.28 1.00      421\nPPGsr_miG_jr:miPPG_jr     0.02      0.01     0.01     0.03 1.01      422\n                      Tail_ESS\nPPGsr_Intercept            513\nPPGjr_Intercept           1430\nGjr_Intercept             1326\nPPGjr_G_sr                 992\nPPGjr_PPG_sr              1135\nGjr_G_sr                  1143\nGjr_PPG_sr                 654\nPPGsr_miG_jr               537\nPPGsr_miPPG_jr             467\nPPGsr_miG_jr:miPPG_jr      427\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_PPGsr     1.86      0.09     1.69     2.06 1.00     1062     1226\nsigma_PPGjr     2.30      0.15     2.04     2.63 1.00      566     1217\nsigma_Gjr       5.33      0.39     4.64     6.11 1.00      519     1160\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#diagnostics",
    "href": "posts/stan-pres-swosc/stan_pres.html#diagnostics",
    "title": "Getting to Know Stan",
    "section": "Diagnostics",
    "text": "Diagnostics\nSince brms is built on Stan we can also take a look at the traceplots of the samples in Figure 23.\n\n\n\n\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGsr\", \"bsp_\"), regex = TRUE, ask = FALSE, N = 4)\n\n\n\n\n\nFigure 23: Traceplots of brms analysis model parameters.\n\n\n\n\n\n\n\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGjr\", \"b_Gjr\"), regex = TRUE, ask = FALSE, N = 6)\n\n\n\n\n\nFigure 24: Traceplots of brms imputation model parameters."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#comparison-of-estimated-effects",
    "href": "posts/stan-pres-swosc/stan_pres.html#comparison-of-estimated-effects",
    "title": "Getting to Know Stan",
    "section": "Comparison of Estimated Effects",
    "text": "Comparison of Estimated Effects\n\n\n\n\n\n\nintro-to-stan.R\n\nplot(brms::conditional_effects(fit_brm_mice, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\nplot(brms::conditional_effects(fit_brm_mi, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\n\n\n\n\n\n\n(a) Estimates after MICE imputation\n\n\n\n\n\n\n\n\n\n(b) Estimates with joint model\n\n\n\n\nFigure 25: The estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior."
  },
  {
    "objectID": "posts/stan-pres-swosc/stan_pres.html#references",
    "href": "posts/stan-pres-swosc/stan_pres.html#references",
    "title": "Getting to Know Stan",
    "section": "References",
    "text": "References\n\n\nBürkner, Paul-Christian. 2018. “Advanced Bayesian Multilevel Modeling with the R Package Brms.” The R Journal 10 (1): 395. https://doi.org/10.32614/RJ-2018-017.\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 168.\n\n\n“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation.” n.d. https://mc-stan.org/cmdstanpy/index.html.\n\n\nDempster, A. P., Martin Schatzoff, and Nanny Wermuth. 1977. “A Simulation Study of Alternatives to Ordinary Least Squares.” Journal of the American Statistical Association 72 (357): 77–91. https://doi.org/10.2307/2286909.\n\n\nGabry, Jonah, Rok Češnovar, and Andrew Johnson. 2023. Cmdstanr: R Inferfacet to ’CmdStan’. https://mc-stan.org/cmdstanr/, https://discourse.mc-stan.org.\n\n\nGelman, Andrew, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. Chapman; Hall/CRC.\n\n\nHarel, Ofer, and Xiao-Hua Zhou. 2007. “Multiple Imputation: Review of Theory, Implementation and Software.” Statistics in Medicine 26 (16): 30573077.\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92. https://www.jstor.org/stable/2335739.\n\n\n———. 1987. Multiple Imputation for Nonresponse in Surveys | Wiley Series in Probability and Statistics. Wiley series in probability and mathematical statistics : Applied probability and statistics. New York: Wiley. https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316696.\n\n\nSchafer, Joseph L., and John W. Graham. 2002. “Missing Data: Our View of the State of the Art.” Psychological Methods 7 (2): 147–77. https://doi.org/10.1037/1082-989X.7.2.147.\n\n\nWhite, Ian R., Patrick Royston, and Angela M. Wood. 2011. “Multiple Imputation Using Chained Equations: Issues and Guidance for Practice.” Statistics in Medicine 30 (4): 377–99. https://doi.org/10.1002/sim.4067.\n\n\nZhou, Xiang, and Jerome P. Reiter. 2010. “A Note on Bayesian Inference After Multiple Imputation.” The American Statistician 64 (2): 159–63. https://doi.org/10.1198/tast.2010.09109."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "imputeangles - GitHub\nProjected Normal Multiple Imputation - GitHub\nMissing Data in Sentencing Research - GitHub"
  },
  {
    "objectID": "projects.html#research-projects",
    "href": "projects.html#research-projects",
    "title": "Projects",
    "section": "",
    "text": "imputeangles - GitHub\nProjected Normal Multiple Imputation - GitHub\nMissing Data in Sentencing Research - GitHub"
  },
  {
    "objectID": "projects.html#course-projects",
    "href": "projects.html#course-projects",
    "title": "Projects",
    "section": "Course Projects",
    "text": "Course Projects\n\nIntro to Data Visualization - A Dashboard for MLB Pitch Data - GitHub\nInference I - Literature Review of Cylindrical Models and Example Analysis of Milwaukee Co. Air Pollution and Wind Directions - GitHub\nIntro to Time Series Analysis - Analysis of Ultra-long Traffic Data with Distributed ARIMA - GitHub"
  },
  {
    "objectID": "projects.html#other-projects",
    "href": "projects.html#other-projects",
    "title": "Projects",
    "section": "Other Projects",
    "text": "Other Projects"
  },
  {
    "objectID": "projects.html#presentations",
    "href": "projects.html#presentations",
    "title": "Projects",
    "section": "Presentations",
    "text": "Presentations\n\nStudent Workshop on Statistical Computing - “Getting to Know Stan” (post)\nNESS 2023 - “Multiple Imputation with Angular Covariates” (pdf)\nENAR 2023 - “Multiple Imputation with Angular Covariates” (pdf)"
  }
]