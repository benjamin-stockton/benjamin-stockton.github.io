{
  "hash": "fa4ea657de1495e21003a8a1d2368bd6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Missingness Mechanisms\nauthor: Ben\ndate: 2025-02-20\nformat: \n    html:\n        code-fold: true\n        code-summary: \"Show code\"\n        code-tools: true\n        number-depth: 3\n        toc: true\n        toc-location: left\n        toc-expand: 2\n        toc-depth: 3\n        link-external-newwindow: true\n        citations-hover: true\ncache: true\nlightbox: true\nbibliography: references.bib\n---\n\n\n\nIn this post, I will go through some brief simulations demonstrating how to induce bias in complete case analysis (CCA) for regression coefficient estimates with incomplete data based on the results noted in [@oberman2023 Section 2.3; @vanbuuren2018 Section 2.7 and Section 3.2.4]. Please refer to these great resources for further details.\n\n## How to Get Biased Estimates with CCA\n\n[@oberman2023] notes that you don't always get biased estimates with CCA. In fact there are special cases where a seemingly MAR mechanism can function in practice as MCAR during simulations or where CCA is super-efficient while MI is biased under certain MNAR mechanisms. They discuss one condition in particular that is required for bias: the variable to be amputed must be correlated with the probability of being missing.\n\nOther conditions and cases are discussed in [@vanbuuren2018 Section 2.7].\n\n-   In single predictor regression of $Y = X\\beta + \\epsilon,$ the CCA is equivalent to MI if only $Y$ is incomplete when estimating regression coefficients.\n\n    -   If $X$ is also incomplete or there are other variables to include in an imputation model, then MI is preferred.\n\n-   If missingness does not depend on $Y,$ then the regression coefficients are unbiased under CCA with missing data on either (or both) $X$ and $Y.$\n\n-   Logistic regression is unbiased under CCA with missing data on only $Y$ or only $X,$ but not both.\n\nSo to get a biased estimate with CCA we need:\n\n1.  An association between $X$ and $Y; \\beta \\neq 0.$\n2.  $Y$ must be associated with $P(R = 0)$ where $R = 0$ indicates missingness.\n3.  The to-be-amputed variable must be associated with $P(R = 0).$\n4.  Missingness on the predictors $X.$\n5.  For MI to be more effective than CCA, more than $Y$ must be related to the amputed variable.\n\n## Some Univariate Missingness Mechanisms\n\nConsider a data set with a response denoted by $Y$ and predictors denoted by $X_1, \\dots, X_3$ respectively. We want to analyze the data using a standard multiple linear regression model,\n\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon,\n$$ {#eq-model}\n\nwhere $\\epsilon$ is an iid normally distributed error term $\\epsilon \\overset{iid}{\\sim} N(0, \\sigma^2_\\epsilon).$ Let $R$ be an indicator for whether the observation for $X_1$ is observed; $R = 1$ means $X_1$ is observed and $R = 0$ means it is missing.\n\nThree mechanisms inspired by [@vanbuuren2018 Section 3.2.4] include:\n\n1.  MAR Right: $\\mathrm{logit}(P(R = 0)) = -\\alpha_0 + Y$\n2.  MAR Mid: $\\mathrm{logit}(P(R = 0)) = -\\alpha_0 - |Y - \\tilde{Y}|$ where $\\tilde{Y}$ is the median of $Y.$\n3.  MAR Tail: $\\mathrm{logit}(P(R = 0)) = -\\alpha_0 + |Y - \\tilde{Y}|$\n\nwhere $\\alpha_0$ is set in each case to guarantee the simulated missingness matches the desired proportion $p_{miss}; ~~\\alpha_0 = - \\bar{U} - \\log(1 / p_{miss} - 1)$ and $\\bar{U} = \\frac{1}{n} \\sum_{i=1}^n U_i$ and $U_i = 3Y_i; ~-|Y_i - \\tilde{Y}|; \\mathrm{~ or ~~} |Y_i - \\tilde{Y}|$ respectively.\n\n## On to the Simulation!\n\nWith these simulations, I am trying to induce bias in the CCA estimate of the regression coefficient corresponding to $X_1$ when predicting the response $Y.$ I will run these simulations over a range of missingness proportions with each of the three mechanisms, but also over several values of $\\beta.$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(compositions, quietly = TRUE, warn.conflicts = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(mice, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(zCompositions, quietly = TRUE, warn.conflicts = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'NADA'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:compositions':\n\n    cor\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    cor\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(zoo, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(dplyr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(generics, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(readr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(ggplot2, quietly = TRUE, warn.conflicts = FALSE)\n\ntheme_set(theme_classic())\n```\n:::\n\n\n\n### Data Generation\n\nThe data are generated from the linear model in @eq-model. There's a pre-set intercept of $\\beta_0 = 5$ but we can change any of the other coefficients as desired.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data <- function(N = 100, beta = c(1, 2, 3)) {\n  X <- matrix(rnorm(N * 3), nrow = N, ncol = 3)\n  df <- as.data.frame(X)\n  colnames(df) <- paste0(\"X\", 1:3)\n\n  df$Y <- (5 + as.matrix(df[,1:3]) %*% beta + rnorm(N, 0, 1))[,1]\n  return(df)\n}\n```\n:::\n\n\n\n### Missingness Mechanism\n\nWe can use any of the three missingness mechanisms described above. Their shapes are plotted in @fig-miss-mech.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimpose_missing <- function(df, p_miss = 0.5, alpha = NULL, mech = NULL) {\n    \n  U <- df$Y\n  if (!is.null(mech)) {\n    if (mech == \"MCAR\") {\n        U <- rep(0, nrow(df))\n    }\n    else if (mech == \"MAR_MID\") {\n        U <- -abs(df$Y - mean(df$Y))\n    }\n    else if (mech == \"MAR_TAIL\") {\n        U <- abs(df$Y - mean(df$Y))\n    }\n    else if (mech == \"MAR_RIGHT\") {\n        U <- df$Y\n    }\n    else if (mech == \"MNAR\") {\n        U <- abs(df$X1 - median(df$X1, na.rm = TRUE))\n    }\n  }\n  \n  alpha0 <- -mean(U) - log(1 / p_miss - 1)\n  \n  mis_p <- boot::inv.logit(alpha0 + U)\n  mis_ind <- sample(1:nrow(df), size = floor(p_miss * nrow(df)),\n                    replace = FALSE, prob = mis_p)\n  R <- rep(0, nrow(df))\n  R[mis_ind] <- 1\n  \n  df_inc <- df\n  df_inc$R <- R\n  df_inc$p_miss <- mis_p\n  df_inc$X1_inc <- df$X1\n  df_inc$X1_inc[mis_ind] <- NA\n  return(df_inc)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_dat <- generate_data(N = 250,\n                         beta = c(1, 2, 3))\n\ndf_inc_mid <- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_MID\")\ndf_inc_tail <- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_TAIL\")\ndf_inc_right <- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_RIGHT\")\n\nsim_dat$p_miss_MID <- df_inc_mid$p_miss\nsim_dat$p_miss_TAIL <- df_inc_tail$p_miss\nsim_dat$p_miss_RIGHT <- df_inc_right$p_miss\nsim_dat |>\n  ggplot(aes(Y, p_miss_MID)) +\n  geom_line() +\n  geom_line(aes(Y, p_miss_TAIL), color = \"tomato\", linetype = \"dotted\") +\n  geom_line(aes(Y, p_miss_RIGHT), color = \"dodgerblue\", linetype = \"dashed\") +\n    annotate(geom = \"text\", x = 10, y = 0.7,\n             label = \"MAR MID\") +\n    annotate(geom = \"text\", x = -1, y = 0.5,\n             label = \"MAR TAIL\",\n             color = \"tomato\") +\n    annotate(geom = \"text\", x = 7, y = 0.25,\n             label = \"MAR RIGHT\",\n             color = \"dodgerblue\") + \n    labs(y = \"Pr(R = 0)\")\n```\n\n::: {.cell-output-display}\n![Missingness mechanism as function of Y.](miss-mech-notes_files/figure-html/fig-miss-mech-1.png){#fig-miss-mech width=672}\n:::\n:::\n\n\n\n\n\n### Analysis Model\n\nThe analysis model stays the same as @eq-model. I will fit model to the complete data and analyze the incomplete data with CCA and MI (performed by `mice` with $M = 15$ imputations) [@buuren2010mice].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Complete data\ndf_inc <- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_RIGHT\")\nfit <- lm(Y ~ X1 + X2 + X3, data = sim_dat)\n\nsmry1 <- summary(fit)\n# smry1\n\n## CCA\nfit <- lm(Y ~ X1_inc + X2 + X3, data = df_inc)\nsmry2 <- summary(fit)\n# smry2\n\n## MI\nimp <- mice::mice(df_inc[, c(\"Y\", \"X1_inc\", \"X2\", \"X3\")], \n                  m = 15, maxit = 10, method = \"norm\", printFlag = FALSE)\nfit <- with(imp, lm(Y ~ X1_inc + X2 + X3))\nsmry3 <- summary(pool(fit))\n\nc(\"complete\" = smry1$coefficients[\"X1\", \"Estimate\"],\n  \"cca\" = smry2$coefficients[\"X1_inc\", \"Estimate\"],\n  \"mi\" = smry3[which(smry3$term == \"X1_inc\"), \"estimate\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n complete       cca        mi \n1.0168138 0.9113908 0.9542853 \n```\n\n\n:::\n:::\n\n\n\n### Simulation Structure\n\nThe simulation follows the same Monte Carlo set-up as usual.\n\n1.  Generate the data.\n2.  Impose missingness.\n3.  Fit the model using the complete data, CCA, and/or MI.\n4.  Collect results and format.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_lm_cca <- function(sim_setting, N_sim, data, methods = c(\"cca\")) {\n  sim_res <- lapply(1:nrow(sim_setting), function(i) {\n    # print(paste0(\"Setting: \", i))\n    lapply(1:N_sim, function(q) {\n      N <- sim_setting$N[i]\n      mech <- sim_setting$mech[i]\n      beta_X1 <- sim_setting$beta[i]\n      beta_vec <- c(5, beta_X1, 2, 1)\n      p_miss <- sim_setting$p_miss[i]\n      ## Generate data\n      sim_dat <- generate_data(N = N, beta = beta_vec[2:4]) \n      \n      ## Impose Missingness\n      df_inc <- impose_missing(sim_dat, p_miss = p_miss, mech = mech)\n      \n      # Calculate association between Y and probability X1 is missing\n      smry_mm <- summary(glm(R ~ Y, data = df_inc,\n                             family = binomial(link = \"logit\")))\n      \n      z_mm <- smry_mm$coefficients[2, 3]\n      \n      ## Fit Complete Data Model\n      fit <- lm(Y ~ X1 + X2 + X3,\n                data = sim_dat)\n      smry <- summary(fit)\n      coef <- as.data.frame(smry$coefficients)\n      coef$Variable <- rownames(coef)\n      coef$true <- beta_vec\n      rownames(coef) <- NULL\n      coef$method <- \"complete\"\n      for (method in methods) {\n        \n        if (method == \"cca\") {\n          ## Fit the CCA Model\n          tmp <- stats::na.omit(df_inc)\n          fit <- lm(Y ~ X1_inc + X2 + X3,\n                    data = tmp)\n          smry <- summary(fit)\n          coef2 <- as.data.frame(smry$coefficients)\n          \n          coef2$Variable <- rownames(coef2)\n          rownames(coef2) <- NULL\n          \n          mean_Y_inc <- mean(tmp$Y)\n        }\n        else if (method == \"mi\") {\n          ## Fit the MI model\n          # print(\"running mi\")\n          tmp <- df_inc |>\n            select(Y, X1_inc, X2, X3)\n          \n          # Impute 5 times (not enough, but a start)\n          imp <- mice::mice(tmp, m = 5, method = \"norm\",\n                            printFlag = FALSE, maxit = 5)\n          fit <- with(imp, lm(Y ~ X1_inc + X2 + X3))\n          coef2 <- summary(pool(fit))\n          coef2 <- coef2 |>\n            mutate(\n              Estimate = estimate,\n              `Std. Error` = std.error,\n              `t value` = statistic,\n              `Pr(>|t|)` = p.value,\n              Variable = term\n            ) |>\n            select(Estimate, `Std. Error`, `t value`, `Pr(>|t|)`, Variable)\n        }\n        coef2$true <- beta_vec[1:nrow(coef2)]\n        coef2$method <- method\n        \n        coef <- bind_rows(coef, coef2)\n      }\n      coef$iter <- q + (i - 1) * N_sim\n      coef$beta1 <- beta_X1\n      coef$z_Y_mm <- z_mm\n      coef$p_miss <- p_miss\n      coef$mech <- mech\n      coef$diff_X1_mean <- mean(sim_dat$X1) - mean(df_inc$X1, na.rm = TRUE)\n      coef$diff_Y_mean <- mean(sim_dat$Y) - mean_Y_inc\n      coef$cor_pmiss_X1 <- cor(df_inc$p_miss, sim_dat$X1, use = \"everything\")\n      return(coef)\n    }) |>\n      dplyr::bind_rows()\n  }) |> \n    dplyr::bind_rows()\n  \n  return(sim_res)\n}\n```\n:::\n\n::: {#tbl-sim-settings .cell}\n\n```{.r .cell-code}\nsim_setting <- expand.grid(\n  N = seq(from = 100, to = 1000, length.out = 1),\n  p_miss = seq(from = 0.25, to = 0.75, length.out = 5),\n  beta = seq(from = -5, to = 5, length.out = 5),\n  mech = c(\"MAR_MID\", \"MAR_TAIL\", \"MAR_RIGHT\")\n) \n# sim_setting |>\n#     kableExtra::kbl(format = \"markdown\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nN_sim <- 250\nstart <- Sys.time()\n\nsim_res <- simulate_lm_cca(sim_setting = sim_setting, N_sim = N_sim,\n                           data = dat, methods = c(\"cca\", \"mi\"))\n\nprint(Sys.time() - start)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 12.34824 mins\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_res |>\n  mutate(\n    beta_label = stringr::str_c(\"beta_1 = \", round(beta1, 2))\n  ) |>\n  filter(stringr::str_detect(Variable, \"X1_inc\")) |>\n  ggplot(aes(as.factor(round(p_miss, 2)), cor_pmiss_X1, fill = mech)) +\n  geom_boxplot() +\n  facet_wrap(beta_label~Variable, ncol = 5) +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n    labs(x = latex2exp::TeX(\"$p_{miss}$\"),\n         y = latex2exp::TeX(\"$cor(p_{miss}, X_1)$\"))\n```\n\n::: {.cell-output-display}\n![Boxplots of the correlations between X1 and P(R=0).](miss-mech-notes_files/figure-html/fig-boxplot-cor-1.png){#fig-boxplot-cor width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_sim <- sim_res |>\n  mutate(\n    beta_label = stringr::str_c(\"beta_1 = \", round(true, 2))\n  ) |>\n  filter(stringr::str_detect(Variable, \"X1\")) |>\n  group_by(Variable, method, mech, beta_label, beta1, p_miss) |>\n  summarize(\n    n = n(),\n    mean_est = mean(Estimate),\n    sd_est = sd(Estimate),\n    mean_bias = mean(Estimate - true), \n    sd_bias = sd_est,\n    mean_abs_bias = mean(abs(Estimate - true)),\n    sd_abs_bias = sd(abs(Estimate - true)),\n    mean_rel_bias = mean((Estimate - true) / abs(true)),\n    mean_se = mean(`Std. Error`),\n    sd_se = sd(`Std. Error`),\n    mse = mean((Estimate - true)^2),\n    sd_mse = sqrt(1/(N_sim - 1) * mean(((Estimate - true)^2 - mse)^2)),\n    mean_cor = mean(cor_pmiss_X1),\n    prop_sig_z_mm = mean(abs(z_Y_mm) > 1.96)\n  ) |>\n  ungroup()\n\nsum_sim$beta_label <- forcats::fct_reorder(sum_sim$beta_label, sum_sim$beta1)\nsum_sim$method <- forcats::fct_relevel(sum_sim$method, c(\"complete\", \"cca\", \"mi\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbiasplt <- sum_sim |>\n  ggplot(aes(p_miss, mean_bias, color = method)) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  geom_errorbar(aes(x = p_miss,\n                    ymin = mean_bias - 2 * sd_bias,\n                    ymax = mean_bias + 2 * sd_bias,\n                    color = method),\n                width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"Bias\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrelbiasplt <- sum_sim |>\n  ggplot(aes(p_miss, mean_rel_bias, color = method)) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  # geom_errorbar(aes(x = p_miss,\n  #                   ymin = mean_abs_bias - 2 * sd_abs_bias,\n  #                   ymax = mean_abs_bias + 2 * sd_abs_bias,\n  #                   color = method),\n  #               width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"Relative Bias\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmseplt <- sum_sim |>\n  ggplot(aes(p_miss, mse, color = method)) +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  geom_errorbar(aes(x = p_miss,\n                    ymin = mse - 2 * sd_mse,\n                    ymax = mse + 2 * sd_mse,\n                    color = method),\n                width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"MSE\")\n```\n:::\n\n::: {#fig-sim-res .cell}\n\n```{.r .cell-code}\nbiasplt\nmseplt\n```\n\n::: {.cell-output-display}\n![](miss-mech-notes_files/figure-html/fig-sim-res-1.png){#fig-sim-res-1 width=672}\n:::\n\n::: {.cell-output-display}\n![](miss-mech-notes_files/figure-html/fig-sim-res-2.png){#fig-sim-res-2 width=672}\n:::\n\nSimulation results.\n:::\n\n\n\n\n\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.3\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggplot2_3.5.1         readr_2.1.5           generics_0.1.3       \n [4] dplyr_1.1.4           zoo_1.8-12            zCompositions_1.5.0-4\n [7] truncnorm_1.0-9       NADA_1.6-1.1          survival_3.6-4       \n[10] MASS_7.3-60.2         mice_3.16.0           compositions_2.0-8   \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5        shape_1.4.6.1       tensorA_0.36.2.1   \n [4] xfun_0.47           htmlwidgets_1.6.4   lattice_0.22-6     \n [7] tzdb_0.4.0          vctrs_0.6.5         tools_4.4.1        \n[10] tibble_3.2.1        fansi_1.0.6         DEoptimR_1.1-3     \n[13] pan_1.9             pkgconfig_2.0.3     jomo_2.7-6         \n[16] Matrix_1.7-0        lifecycle_1.0.4     stringr_1.5.1      \n[19] farver_2.1.2        compiler_4.4.1      munsell_0.5.1      \n[22] codetools_0.2-20    htmltools_0.5.8.1   yaml_2.3.10        \n[25] glmnet_4.1-8        pillar_1.9.0        nloptr_2.1.1       \n[28] tidyr_1.3.1         iterators_1.0.14    rpart_4.1.23       \n[31] boot_1.3-30         foreach_1.5.2       mitml_0.4-5        \n[34] nlme_3.1-164        robustbase_0.99-4-1 tidyselect_1.2.1   \n[37] digest_0.6.37       stringi_1.8.4       purrr_1.0.2        \n[40] forcats_1.0.0       labeling_0.4.3      splines_4.4.1      \n[43] latex2exp_0.9.6     cowplot_1.1.3       fastmap_1.2.0      \n[46] grid_4.4.1          colorspace_2.1-1    cli_3.6.3          \n[49] magrittr_2.0.3      utf8_1.2.4          broom_1.0.7        \n[52] withr_3.0.1         scales_1.3.0        backports_1.5.0    \n[55] rmarkdown_2.28      nnet_7.3-19         lme4_1.1-35.5      \n[58] hms_1.1.3           kableExtra_1.4.0    evaluate_1.0.0     \n[61] knitr_1.48          viridisLite_0.4.2   mgcv_1.9-1         \n[64] rlang_1.1.4         Rcpp_1.0.13         glue_1.8.0         \n[67] bayesm_3.1-6        xml2_1.3.6          svglite_2.1.3      \n[70] rstudioapi_0.16.0   minqa_1.2.8         jsonlite_1.8.9     \n[73] R6_2.5.1            systemfonts_1.1.0  \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}