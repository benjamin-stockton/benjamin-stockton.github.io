{
  "hash": "4386f21c080ddf36eb3434790f55f10b",
  "result": {
    "markdown": "---\ntitle: \"Getting to Know Stan\"\nauthor: \"Ben Stockton\"\ndate: 11-16-2023\nformat: \n    revealjs:\n        theme: white\n        smaller: false\n        code-overflow: wrap\n        slide-number: true\n        width: 1920\n        height: 1080\neditor: visual\nresults: hold\nwarnings: false\necho: true\nbibliography: swosc-stan.bib\ndraft: false\n---\n\n\n# Some Brief Set-up\n\n## Installation\n\n::: columns\n::: {.column width=\"50%\"}\n-   I'll use the CmdStan toolchain in R with the `cmdstanr` [package](https://mc-stan.org/cmdstanr/) [@gabry2023].\n\n-   There are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called [CmdStanPy](https://github.com/stan-dev/cmdstanpy) [@cmdstanp].\n\n- You need to have Rtools installed to then install CmdStan and `brms`.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code  code-line-numbers=\"|1,2,3,5|8\"}\n# install.packages(\"cmdstanr\",\n#                  repos = c(\"https://mc-stan.org/r-packages/\",\n#                            getOption(\"repos\")))\nlibrary(cmdstanr)\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\ncmdstanr::cmdstan_path()\ncmdstanr::check_cmdstan_toolchain(fix = TRUE)\n\n# install.packages(c(\"bayesplot\", \"ggplot2\", \"posterior\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2.33.1\"\n[1] \"/home/stocb/.cmdstan/cmdstan-2.33.1\"\n```\n:::\n:::\n\n:::\n:::\n\n::: {.notes}\n- Run the install here and hopefully it'll finish by the time we get to running code\n\n- If you want to follow along, copy and paste code from the post as we go\n\n- And if you have Qs or if I'm going too fast, feel free to ask or interrupt me\n:::\n\n## A Very Brief Introduction to Bayesian Data Analysis\n\n-   Incorporate prior knowledge about the model/data into our analysis\n\n-   Bayesian inference treats the parameters of the model as random variables\n\n::: callout-important\n## Bayes Rule\n\nLet $\\theta$ be a r.v. with (prior) distribution $p(\\theta)$, $Y$ be a r.v. with likelihood $p(y | \\theta)$. Their joint distribution is $p(y, \\theta) = p(y|\\theta) p(\\theta)$.\n\nBayes rule lets us flip the conditioning from the likelihood to get\n\n$$\np(\\theta | y) = \\frac{p(y, \\theta)}{p(y)} = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta} \\propto p(y|\\theta) p(\\theta)\n$$\n:::\n\n## The Components of Bayesian Inference\n\nTwo sets of random variables to model: parameters $\\theta$ and data $Y$\n\n1.  **The Prior:** $p(\\theta)$ does not depend on $Y$\n2.  **The (Data) Likelihood:** $p(y | \\theta)$ models $Y$ dependent on $\\theta$\n3.  **The Posterior:** $p(\\theta | y)$ models $\\theta$ given $Y$ \n\n\n```{=tex}\n\\begin{align*}\n    p(\\theta | y) &= \\frac{p(\\theta, y)}{p(y)} \\\\\n        &= \\frac{p(\\theta) \\times p(y | \\theta)}{\\int p(y|\\theta) p(\\theta)} \\\\\n        &\\propto p(\\theta) p(y | \\theta) \\\\\n\\end{align*}\n```\n\nMake inferences about $\\theta$ with the posterior distribution\n\n## Why Stan?\n\n::: columns\n::: {.column width=\"70%\"}\n\n- Stan is one of several ways to run MCMC for Bayesian inference\n  \n  - Nimble, OpenBUGS, R, Rcpp, Julia are other options\n\n- Other methods use combinations of Gibbs, Metropolis-Hastings, and slice sampling; Stan uses Hamiltonian Monte Carlo and the No-U-Turn Sampler (NUTS) which is more efficient\n\n- Stan only allows for continuous parameters\n\n:::\n::: {.column width=\"30%\"}\n![](stan_logo.png)\n:::\n:::\n\n# Example Time\n\n::: {.notes}\n- Next, we'll use an college women's basketball team shooting to predict wins\n\n- We'll go through two versions of linear regression programmed in Stan\n\n- Get a feel for diagnostics, inference, and posterior predictive checks\n:::\n\n## NCAA Women's Basketball\n\n::: columns\n::: {.column width=\"50%\"}\nWe'll model NCAA Women's Basketball team's total wins by their 3 point field goal percentage from the 2022-2023 season. [CSV file available here](Data/NCAAW-freethrows-threes-2022-2023.csv){target=\"_blank\"}\n\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code .code-overflow-wrap}\nncaaw <- readr::read_csv(file = \"Data/NCAAW-freethrows-threes-2022-2023.csv\")\n```\n:::\n\n\nIn the 2022-2023 season there were $N = 350$ teams.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\", y = \"Wins\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![Scatter plot of the Total Wins by 3 pt Field Goal %.](stan_pres_files/figure-revealjs/fig-scatter-1.png){#fig-scatter width=960}\n:::\n:::\n\n:::\n:::\n\n::: {.notes}\n- The data are 2022-2023 season NCAA women's basketball team 3pt % and Wins\n- N=350\n- Data are basically linear, and moderately correlated\n:::\n\n## Baseline ML Estimates\n\n::: columns\n::: {.column width=\"50%\"}\nAs a baseline, we'll find the maximum likelihood estimates for the regression parameters and variance.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code  code-line-numbers=\"1,2,3,4|8\"}\nfit_ml <- lm(W ~ FG3pct, data = ncaaw)\n(beta_ml <- coef(fit_ml))\nsmry_ml <- summary(fit_ml)\n(sigma_ml <- smry_ml$sigma)\nmles <- data.frame(Parameters = c(\"beta_0\", \"beta_1\", \"sigma\"),\n                   Estimates = c(beta_ml, sigma_ml))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)      FG3pct \n  -14.94468     1.00929 \n[1] 5.908144\n```\n:::\n:::\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\", y = \"Wins\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![Now the OLS regression line is super-imposed in blue.](stan_pres_files/figure-revealjs/fig-scatter-fitted-1.png){#fig-scatter-fitted width=960}\n:::\n:::\n\n:::\n:::\n\n::: {.notes}\n- We'll do the MLE fit with the lm() and keep the point estimates for comparison\n:::\n\n# Non-informative Prior Regression Model\n\n## Model #1 Set-up\n\nLet's consider the simple regression model $E(Y_i | X_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i$ and $Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2$ [@gelman2013bayesian, p. 354-358].\n\n1.  **The Prior:** $p(\\boldsymbol{\\beta}, \\log\\sigma) = 1 \\equiv p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}$\n2.  **The (Data) Likelihood:** $\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).$\n3.  **The Posterior:**\n\n\n```{=tex}\n\\begin{align*}\n    p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &= p(\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{y}) \\times p(\\sigma^2 | \\mathbf{y}) \\\\\n        &= N_2(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\beta}}, \\sigma^2 (X'X)^{-1}) \\times Inv-\\chi^2 (\\sigma^2 | N-2, s^2) \\\\\n    \\hat{\\boldsymbol{\\beta}} &= (X'X)^{-1} X'\\mathbf{y} \\\\\n    s^2 &= \\frac{1}{N-2} (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})' (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})   \n\\end{align*}\n```\n\n\n:::{.notes}\n- Prior is uniform on beta and log sigma which then gives the prior propto sigma^-2, example of Jeffreys prior\n- Standard vector notation for linear regression model. Observations are independent since covariance is diagonal\n- Posterior is a product of Normal and Scaled Inv Chisq\n- Note that beta's posterior is centered at MLE of beta and scale parameter of sigma^2's posterior is residual variance so that it's posterior mean is s^2\n- This posterior has a closed form\n:::\n\n## Non-informative Prior Regression Stan Code\n\n``` {.stan .code-overflow-wrap filename=\"non-informative-regression.stan\" code-line-numbers=\"1-6|8-16|18-23|25-32|34-40\"}\n// The input data is two vectors 'y' and 'X' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  vector[N] x;\n}\n\ntransformed data {\n    matrix[N, 2] X_c = append_col(rep_vector(1, N), x);\n    matrix[2,2] XtX_inv = inverse(X_c' * X_c);\n\n    vector[2] beta_hat = XtX_inv * X_c' * y;\n    vector[N] y_hat = X_C * beta_hat;\n    \n    real<lower=0> s_2 = 1 / (N - 2) * (y - y_hat)' * (y - y_hat);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'beta' and 'sigma'.\nparameters {\n  vector[2] beta;\n  real<lower=0> sigma; // Note that this is the variance\n}\n\n// The model to be estimated. We model the output\n// 'y' ~ N(x beta, sigma) by specifying the analytic\n// posterior defined above.\nmodel {\n  beta ~ multi_normal(beta_hat, sigma^2 * XtX_inv);\n  \n  sigma^2 ~ scaled_inv_chi_square(N-2, sqrt(s_2));\n}\n\ngenerated quantities {\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n```\n\n:::{.notes}\n- Discuss each section in detail\n\n- Note size of beta vector, parameterizations of scaled_inv_chi_square() and normal()\n\n- Restate that we use the analytic posterior for efficiency when we can\n:::\n\n## Model #1 Fitting\n\n::: columns\n::: {.column width=\"50%\"}\n-   First we write the Stan code in a separate file[^1].\n\n-   1000 warmup iterations, 1000 sampling iterations\n\n-   No thinning (thinning includes only every $n$th draw)\n\n-   Refresh the print screen to see progress every 500 iterations.\n\n-   Run several chains (in parallel)\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code  code-line-numbers=\"1-5|7|8|10-19|\"}\ndata_list <- list(\n    N = nrow(ncaaw),\n    y = ncaaw$W,\n    x = ncaaw$FG3pct\n)\n\nfile <- file.path(\"non-informative-regression.stan\")\nnon_inf_model <- cmdstan_model(file)\n\nfit1 <- non_inf_model$sample(\n    data = data_list,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    # show_exceptions = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n```\n:::\n:::\n\n:::\n:::\n\n:::{.notes}\n- Discuss the code in depth\n\n- Explain why we use warmup and sampling iterations\n\n- That 2000 is far fewer than required by standard MCMC\n\n- No thinning, could be used to avoid high autocorrelation at more than lag 1\n\n- Why do we run chains?\n\n- Discuss output\n\n- Orange exception text\n:::\n\n[^1]:\n    -   See the [Stan User's Guide Part 1.1](https://mc-stan.org/docs/stan-users-guide/linear-regression.html#vectorization.section) for programming this model without the analytic posteriors.\n\n## Model #1 Diagnostics\n\n::: columns\n::: {.column width=\"50%\"}\n-   Diagnostic summary generated by Stan\n\n-   Plots created using the `bayesplot` package.\n\n    -   Trace plots\n\n    -   Density Plots\n\n    -   ACF Plots\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nfit1$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 0.9559799 1.0012814\n```\n:::\n:::\n\n::: {#fig-traceplots-non-info .cell layout-ncol=\"3\" filename='intro-to-stan.R' output-location='slide'}\n\n```{.r .cell-code}\nlibrary(bayesplot)\n\nmcmc_trace(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_dens_overlay(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_acf_bar(fit1$draws(variables = c(\"beta\", \"sigma\")))\n```\n\n::: {.cell-output-display}\n![Traceplots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-non-info-1.png){#fig-traceplots-non-info-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Approximate posterior densities for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-non-info-2.png){#fig-traceplots-non-info-2 width=576}\n:::\n\n::: {.cell-output-display}\n![Autocorrelation Function (ACF) plots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-non-info-3.png){#fig-traceplots-non-info-3 width=576}\n:::\n\nDiagnostic plots from the `bayesplot` package.\n:::\n\n:::\n:::\n\n## Model #1 Inference\n\n-   The summary statistics are displayed in @tbl-non-info-inf.\n\n-   The statistics `rhat`, `ess_bulk`, and `ess_tail` are additional diagnostics.\n\n\n::: {#tbl-non-info-inf .cell filename='intro-to-stan.R' output-location='fragment' tbl-cap='Summary statistics for the posterior samples for $\\beta$ and $\\sigma$.'}\n\n```{.r .cell-code}\nfit1$summary(variables = c(\"beta\", \"sigma\")) |> \n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> variable </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> median </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> mad </th>\n   <th style=\"text-align:right;\"> q5 </th>\n   <th style=\"text-align:right;\"> q95 </th>\n   <th style=\"text-align:right;\"> rhat </th>\n   <th style=\"text-align:right;\"> ess_bulk </th>\n   <th style=\"text-align:right;\"> ess_tail </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> beta[1] </td>\n   <td style=\"text-align:right;\"> -14.764 </td>\n   <td style=\"text-align:right;\"> -14.671 </td>\n   <td style=\"text-align:right;\"> 2.891 </td>\n   <td style=\"text-align:right;\"> 2.949 </td>\n   <td style=\"text-align:right;\"> -19.605 </td>\n   <td style=\"text-align:right;\"> -10.193 </td>\n   <td style=\"text-align:right;\"> 1.003 </td>\n   <td style=\"text-align:right;\"> 636.814 </td>\n   <td style=\"text-align:right;\"> 837.657 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> beta[2] </td>\n   <td style=\"text-align:right;\"> 1.003 </td>\n   <td style=\"text-align:right;\"> 1.002 </td>\n   <td style=\"text-align:right;\"> 0.093 </td>\n   <td style=\"text-align:right;\"> 0.096 </td>\n   <td style=\"text-align:right;\"> 0.856 </td>\n   <td style=\"text-align:right;\"> 1.159 </td>\n   <td style=\"text-align:right;\"> 1.004 </td>\n   <td style=\"text-align:right;\"> 629.430 </td>\n   <td style=\"text-align:right;\"> 867.388 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sigma </td>\n   <td style=\"text-align:right;\"> 5.915 </td>\n   <td style=\"text-align:right;\"> 5.913 </td>\n   <td style=\"text-align:right;\"> 0.240 </td>\n   <td style=\"text-align:right;\"> 0.241 </td>\n   <td style=\"text-align:right;\"> 5.521 </td>\n   <td style=\"text-align:right;\"> 6.315 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n   <td style=\"text-align:right;\"> 865.433 </td>\n   <td style=\"text-align:right;\"> 593.155 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {#tbl-mle-res-1 .cell filename='intro-to-stan.R' output-location='column-fragment' tbl-cap='MLE estimates for $\\beta$ and $\\sigma$.'}\n\n```{.r .cell-code}\nmles |> \n    kableExtra::kbl(booktabs = TRUE, \n                    format = \"html\", digits = 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> Parameters </th>\n   <th style=\"text-align:right;\"> Estimates </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> beta_0 </td>\n   <td style=\"text-align:right;\"> -14.945 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> FG3pct </td>\n   <td style=\"text-align:left;\"> beta_1 </td>\n   <td style=\"text-align:right;\"> 1.009 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> sigma </td>\n   <td style=\"text-align:right;\"> 5.908 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Model #1 Graphical Summaries\n\n-   @fig-non-info-ci-1 displays 50% (thick bar) and 95% (thin bar) credible intervals with the posterior mean displayed as a point. The densities are plotted in ridgelines in @fig-non-info-ci-2.\n\n-   Plots were made using the `bayesplot` package.\n\n\n::: {#fig-non-info-ci .cell layout-ncol=\"2\" filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code}\nmcmc_intervals(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit1$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n```\n\n::: {.cell-output-display}\n![Interval plots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-non-info-ci-1.png){#fig-non-info-ci-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Approximate posterior densities for $\\beta$ and $\\sigma$ in a ridgeline plot.](stan_pres_files/figure-revealjs/fig-non-info-ci-2.png){#fig-non-info-ci-2 width=576}\n:::\n\n\n:::\n\n\n## Model #1 Posterior Predictive Checks\n\n-   One way to check model fit is to assess posterior predictive distrubtion.\n\n-   Draw samples from the posterior predictive distribution $p(y^{new} | y) = \\int p(y^{new} | \\boldsymbol{\\beta}, \\sigma) p(\\boldsymbol{\\beta}, \\sigma | y) d\\boldsymbol{\\beta}d\\sigma$ by\n\n1.  Sampling from the posterior (i.e. the draws in the MCMC chains)\n2.  For each set of draws sampling $y^{new}$ given the corresponding values for $x^{new}$\n\n-   In Stan this is easily accomplished using the generated quantities block.\n\n``` {.stan .code-overflow-wrap filename=\"non-informative-regression.stan\"}\ngenerated quantities {\n    // create a vector of N new observations\n    vector[N] y_ppd; \n    \n    // for each observation, sample from the regression likelihod\n    // using the posterior draws\n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n```\n\n:::{.notes}\nSay at the beginning of the slide that this doesn't need to be copied\n:::\n\n## Model #1 Posterior Predictive Plots\n\n\n::: {#fig-non-info-ppc .cell layout-ncol=\"2\" filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|3-4|5-7\"}\nlibrary(posterior)\ny_ppd <- as.matrix(as_draws_df(fit1$draws(variables = \"y_ppd\")))\nppc_dens_overlay(ncaaw$W, y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\", x = \"Wins\")\nppc_intervals(ncaaw$W, y_ppd[1:50, 1:350], x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\", y = \"Wins\")\n```\n\n::: {.cell-output-display}\n![PPD densities for the wins given 3pt%.](stan_pres_files/figure-revealjs/fig-non-info-ppc-1.png){#fig-non-info-ppc-1 width=960}\n:::\n\n::: {.cell-output-display}\n![PPD intervals for the wins plotted by 3pt%.](stan_pres_files/figure-revealjs/fig-non-info-ppc-2.png){#fig-non-info-ppc-2 width=960}\n:::\n\nPosterior Predictive Check plots from `bayesplot`.\n:::\n\n\n# Conjugate Prior Regression Model\n\n## Model #2 Set-up\n\nNext, we'll implement the regression model with conjugate priors. Conjugacy refers to the situation where the prior and posterior distribution are from the same family.[^2]\n\n[^2]: See wikipedia for more details and derivations: <https://en.wikipedia.org/wiki/Bayesian_linear_regression>\n\n1.  **Conjugate prior:** $p(\\boldsymbol{\\beta}, \\sigma^2) = p(\\boldsymbol{\\beta} | \\sigma^2) p(\\sigma^2)$\n\n    1.  $\\boldsymbol{\\beta} | \\sigma^2 ~ N_2(\\boldsymbol{\\beta}_0, \\sigma^2 \\Lambda_0^{-1})$ where $\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^2$ is a vector of prior coefficients, and $\\Lambda_0$ is a $2\\times2$ prior correlation matrix. $\\boldsymbol{\\beta}_0 = 0$ and $\\Lambda_0 = \\lambda I_2 = 10 I_2$ to get a weakly informative prior that is equivalent to ridge regression.\n\n    2.  $\\sigma^2 \\sim InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2)$ where $\\nu_0$ is a prior sample size and $s_0$ is the prior standard deviation. We'll set these to $\\nu_0 = 1$ and $s_0^2 = 47$ the sample variance of the teams' wins.\n\n    3.  The parameters $\\boldsymbol{\\beta}_0, \\Lambda_0, \\nu_0, s_0^2$ are hyperparameters.\n\n2.  **The (Data) Likelihood:** $\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).$\n\n3.  **Posterior:**\n\n    1.  $\\boldsymbol{\\beta} | \\sigma^2, y \\sim N_2(\\boldsymbol{\\beta}_N, \\sigma^2 \\Lambda_N^{-1})$ where $\\boldsymbol{\\beta}_N = \\Lambda_N^{-1}(\\mathbf{X}'\\mathbf{X} \\hat{\\boldsymbol{\\beta}} + \\Lambda_0 \\boldsymbol{\\beta}_0)$ and $\\Lambda_N = (\\mathbf{X}'\\mathbf{X} + \\Lambda_0).$\n    2.  $\\sigma^2 | y \\sim InvGamma(\\sigma^2 | \\frac{\\nu_0 + N}{2}, \\frac{1}{2} \\nu_0 s_0^2 + \\frac{1}{2}(\\mathbf{y}'\\mathbf{y} + \\boldsymbol{\\beta}_0'\\Lambda_0 \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_N' \\Lambda_N \\boldsymbol{\\beta}_N)).$\n\n:::{.notes}\n- Note that this is equivalent to ridge regression with shrinkage parameter 1/lambda\n:::\n\n## Conjugate Prior Regression Stan Code\n\n``` {.stan filename=\"conjugate-regression.stan\" code-line-numbers=\"1-12|15-19|21-26|28-36|38-45\"}\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  int<lower=0> K;\n  vector[N] y;\n  matrix[N, K] X;\n  \n  // hyperparameters\n  real beta_0;\n  real<lower=0> lambda_0;\n  real<lower=0> nu_0;\n  real<lower=0> s_02;\n}\n\ntransformed data {\n    matrix[N, K+1] X_mat = append_col(rep_vector(1, N), X);\n    vector[K+1] beta_0_vec = rep_vector(beta_0, K+1);\n    matrix[K+1, K+1] Lambda_0 = lambda_0 * identity_matrix(K+1);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  vector[K+1] beta;\n  real<lower=0> sigma2;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  beta ~ multi_normal(beta_0_vec, sigma2 * Lambda_0);\n  sigma2 ~ scaled_inv_chi_square(nu_0, sqrt(s_02));\n  \n  y ~ normal(X_mat * beta, sqrt(sigma2));\n}\n\ngenerated quantities {\n    real sigma = sqrt(sigma2);\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_mat[i,] * beta, sqrt(sigma2));\n    }\n}\n```\n\n:::{.notes}\n- Note that we are allowing for additional predictors here\n:::\n\n## Model #2 Fitting\n\n::: columns\n::: {.column width=\"50%\"}\n-   Program the model only through the priors and likelihood and let Stan approximate the posterior\n\n-   1000 warmup iterations, 1000 sampling iterations\n\n-   No thinning (thinning includes only every $k$th draw)\n\n-   Refresh the print screen to see progress every 500 iterations.\n\n-   Run several chains (in parallel)\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code}\ndata_list2 <- list(\n    N = nrow(ncaaw),\n    K = 1,\n    y = ncaaw$W,\n    X = as.matrix(ncaaw$FG3pct, nrow = nrow(ncaaw)),\n    \n    # hyperparameters\n    beta_0 = 0,\n    lambda_0 = 0.5,\n    nu_0 = 1,\n    s_02 = 47\n)\n\nfile2 <- file.path(\"conjugate-regression.stan\")\nconj_model <- cmdstan_model(file2)\n\nfit2 <- conj_model$sample(\n    data = data_list2,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    # show_exceptions = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.3 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n:::\n\n:::\n:::\n\n## Model #2 Diagnostics\n\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nfit2$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.097658 1.036561\n```\n:::\n:::\n\n::: {#fig-traceplots-conj .cell layout-ncol=\"3\" filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code}\nmcmc_trace(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_dens_overlay(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_acf_bar(fit2$draws(variables = c(\"beta\", \"sigma\")))\n```\n\n::: {.cell-output-display}\n![Traceplots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-conj-1.png){#fig-traceplots-conj-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Approximate posterior densities for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-conj-2.png){#fig-traceplots-conj-2 width=576}\n:::\n\n::: {.cell-output-display}\n![ACF plots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-traceplots-conj-3.png){#fig-traceplots-conj-3 width=576}\n:::\n\n\n:::\n\n\n## Model #2 Inference\n\n-   Estimates are similar to Model #1, but regression coefficients are shrunk slightly to zero and variance is slightly higher\n\n-   @fig-conj-ci-1 displayes credible intervals and densities are plotted as ridgelines in @fig-conj-ci-2.\n\n\n::: {#tbl-conj-inf .cell filename='intro-to-stan.R' output-location='fragment' tbl-cap='Summary statistics for the posterior samples for $\\beta$ and $\\sigma$.'}\n\n```{.r .cell-code .code-overflow-wrap}\nmcmc_summary <- cbind(mles,\n  fit1$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\nmcmc_summary <- cbind(mcmc_summary,\n  fit2$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\ncolnames(mcmc_summary) <- c(\"Variable\", \"MLE\", \"Non-info Est\", \"Non-info SD\", \"Conj Est\", \"Conj SD\")\n\nmcmc_summary |>\n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:right;\"> MLE </th>\n   <th style=\"text-align:right;\"> Non-info Est </th>\n   <th style=\"text-align:right;\"> Non-info SD </th>\n   <th style=\"text-align:right;\"> Conj Est </th>\n   <th style=\"text-align:right;\"> Conj SD </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> beta_0 </td>\n   <td style=\"text-align:right;\"> -14.945 </td>\n   <td style=\"text-align:right;\"> -14.764 </td>\n   <td style=\"text-align:right;\"> 2.891 </td>\n   <td style=\"text-align:right;\"> -10.060 </td>\n   <td style=\"text-align:right;\"> 2.363 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> FG3pct </td>\n   <td style=\"text-align:left;\"> beta_1 </td>\n   <td style=\"text-align:right;\"> 1.009 </td>\n   <td style=\"text-align:right;\"> 1.003 </td>\n   <td style=\"text-align:right;\"> 0.093 </td>\n   <td style=\"text-align:right;\"> 0.853 </td>\n   <td style=\"text-align:right;\"> 0.076 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> sigma </td>\n   <td style=\"text-align:right;\"> 5.908 </td>\n   <td style=\"text-align:right;\"> 5.915 </td>\n   <td style=\"text-align:right;\"> 0.240 </td>\n   <td style=\"text-align:right;\"> 5.970 </td>\n   <td style=\"text-align:right;\"> 0.222 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Model #2 Graphical Summaries\n\n\n::: {#fig-conj-ci .cell layout-ncol=\"2\" filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code}\nmcmc_intervals(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit2$draws(variables = c(\"beta\", \"sigma\")), prob_outer = 0.95, prob = 0.5)\n```\n\n::: {.cell-output-display}\n![Interval plots for $\\beta$ and $\\sigma$.](stan_pres_files/figure-revealjs/fig-conj-ci-1.png){#fig-conj-ci-1 width=960}\n:::\n\n::: {.cell-output-display}\n![Approximate posterior densities for $\\beta$ and $\\sigma$ in a ridgeline plot.](stan_pres_files/figure-revealjs/fig-conj-ci-2.png){#fig-conj-ci-2 width=960}\n:::\n\nPlots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for $\\beta$ and $\\sigma$. Plots were made using the `bayesplot` package.\n:::\n\n\n## Model #2 Posterior Predictive Distribution\n\n\n::: {#fig-conj-ppc .cell layout-ncol=\"2\" filename='intro-to-stan.R'}\n\n```{.r .cell-code}\ny_ppd <- as.matrix(as_draws_df(fit2$draws(variables = \"y_ppd\")))\nppc_dens_overlay(ncaaw$W, y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\", x = \"Wins\")\nppc_intervals(ncaaw$W,\n              y_ppd[1:50, 1:350], x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\", y = \"Wins\")\n```\n\n::: {.cell-output-display}\n![PPD densities for the wins given 3pt%.](stan_pres_files/figure-revealjs/fig-conj-ppc-1.png){#fig-conj-ppc-1 width=960}\n:::\n\n::: {.cell-output-display}\n![PPD intervals for the wins plotted by 3pt%.](stan_pres_files/figure-revealjs/fig-conj-ppc-2.png){#fig-conj-ppc-2 width=960}\n:::\n\n\n:::\n\n\n# Additional Resources\n\n## Guides for Stan\n\n::: columns\n::: {.column width=\"50%\"}\n\nFirst, here's the **three essential guides** for using Stan:\n\n-   [Stan Function Guide](https://mc-stan.org/docs/functions-reference/index.html) - reference for all the built-in functions and distributions\n\n-   [Stan User's Guide](https://mc-stan.org/docs/stan-users-guide/index.html) - reference for example models, how to build efficient models, and some inference techniques\n\n-   [Stan Reference Manual](https://mc-stan.org/docs/reference-manual/index.html) - reference for programming in Stan with a focus on how the language works\n\n:::\n\n:::{.column width=\"50%\"}\n**Other Stan Packages**\n\n-   [brms](https://paul-buerkner.github.io/brms/index.html): Bayesian regression models using Stan\n\n-   [posterior](https://mc-stan.org/posterior/): Useful for working with Stan output\n\n-   [bayesplot](http://mc-stan.org/bayesplot): ggplot2-based plotting functions for MCMC draws designed work well with Stan\n\n-   [loo](http://mc-stan.org/loo): Leave-one-out cross validation for model checking and selection that works with the log-posterior.[^2] \n\n**Guides to Debugging and Diagnostics**\n\n-   [Stan's Guide to Runtime warnings and convergence problems](https://mc-stan.org/misc/warnings.html)\n\n-   [Prior Choices and Selection](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)\n\n-   [Convergence Diagnostics for MCMC](https://arxiv.org/pdf/1909.11827.pdf)\n\n-   [Official Stan Forum](https://discourse.mc-stan.org/)\n:::\n:::\n\n[^2]: Works best with `rstanarm` but can work with `cmdstanr` too.\n\n# Bonus!\n\n## Bonus: Regression Modeling with Incomplete Data\n\n-   Let's use the `brms` package to fit a regression model with incomplete predictor observations.\n\n-   Incomplete data analysis ranges from complete case analysis to multiple imputation, joint modeling, and EM algorithm [@schafer2002].[^3]\n\n-   We're going to use `mice` [@buuren2010mice] and `brms` [@bürkner2018] to demonstrate the imputation and fitting Bayesian regression models.\n\n[^3]: See [@rubin1976, @dempster1977, @rubin1987, @harel2007multiple, @white2011] for more details on incomplete data analysis.\n\n## NCAA Women's Basketball Player's Junior and Senior Years\n\n::: columns\n::: {.column width=\"50%\"}\n-   We'll use junior year scoring (points per game/PPG) to predict senior year scoring for 2020-21 to the 2022-23 seasons. [CSV available here](Data/ncaaw-individuals.csv)\n\n-   The data set only contains players who played in at least 75% of games each season, so partial seasons due to injury or being a bench player are excluded.\n\n-   Players who only have a junior season are excluded from the analysis.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nncaaw_i <- read.csv(\"Data/ncaaw-individuals.csv\", header = TRUE)\nhead(ncaaw_i)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Name Pos_jr Pos_sr G_jr G_sr PPG_jr PPG_sr Cl_jr\n1     A'Jah Davis      F      F   29   32   16.6   16.2   Jr.\n2 Abby Brockmeyer      F      F   NA   31     NA   16.3   Jr.\n3       Abby Feit      F      F   29   28   15.1   15.5   Jr.\n4     Abby Meyers      G      G   NA   30     NA   17.9   Jr.\n5     Abby Meyers      G      G   NA   35     NA   14.3   Jr.\n6   Adriana Shipp      G      G   NA   30     NA   13.9   Jr.\n```\n:::\n:::\n\n::: {.cell filename='intro-to-stan.R' output-location='fragment'}\n\n```{.r .cell-code}\nggplot(ncaaw_i, aes(PPG_jr, PPG_sr, color = G_jr)) +\n    geom_point(size = 1.5) +\n    scale_color_viridis_c(name = \"G - Jr\") +\n    labs(x = \"PPG - Jr\", y = \"PPG - Sr\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![Points per game (PPG) from Junior and Senior seasons.](stan_pres_files/figure-revealjs/fig-ppg-jr-sr-1.png){#fig-ppg-jr-sr width=960}\n:::\n:::\n\n:::\n:::\n\n## Incomplete Data Structure & Imputation Model\n\n-   The imputation model will be univariate linear regression that use all other variables as predictors.\n\n    -   For example, imputing $PPG_{jr}$ will be done by regressing on $PPG_{sr}, G_{jr}, G_{sr}$.\n\n-   $PPG_{jr}$ and $G_{jr}$ are incomplete for $n_{mis} = 176$ players while $n_{obs} = 98$ players have stats from both years as displayed in @fig-miss-patt.\n\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\n# install.packages(c(\"mice\", \"brms\"))\n\nlibrary(mice)\nm_pat <- md.pattern(ncaaw_i, plot = TRUE)\n```\n\n::: {.cell-output-display}\n![Missing data patterns for the NCAA women's basketball players from 2020-2023 who played in their junior and senior year. The red boxes correspond to missing values, so there are 176 players who recorded full senior seasons (played in >75% of total games) but missing or shortened junior seasons.](stan_pres_files/figure-revealjs/fig-miss-patt-1.png){#fig-miss-patt width=960}\n:::\n:::\n\n\n## Multiple Imputation with mice\n\nFirst, we'll impute before model fitting using `mice`.\n\nMultiple Imputation (by Chained Equations) is a three stage procedure:\n\n1.  Each incomplete variable is imputed $M$ times with posterior predictive draws from a regression model with all other variables as predictors. The procedure iterates through the incomplete variables several times to converge to the posterior predictive distribution of the missing data given the observed.\n2.  These completed data sets are then analyzed individually with a standard complete data method.\n3.  Results from each analysis are combined. Typically this is done with Rubin's rules [@rubin1987], but `brms` follows the advice of @zhou2010 and simply stacks the posterior draw matrices from each fitted model.\n\n\n::: {.cell filename='intro-to-stan.R' output-location='fragment' hash='stan_pres_cache/revealjs/mice-fit_0d6ab59c772465a13194eaf1d7a582bd'}\n\n```{.r .cell-code}\nlibrary(brms)\nimps <- mice(ncaaw_i, m = 10, method = \"norm\", maxit = 10, printFlag = FALSE)\nfit_brm_mice <- brm_multiple(PPG_sr ~ G_jr * PPG_jr, data = imps, chains = 2, refresh = 0)\nsummary(fit_brm_mice)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: PPG_sr ~ G_jr * PPG_jr \n   Data: imps (Number of observations: 274) \n  Draws: 20 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      17.59      4.94     8.04    27.62 1.18       75      253\nG_jr           -0.24      0.20    -0.65     0.14 1.24       59      251\nPPG_jr         -0.11      0.29    -0.70     0.45 1.20       70      230\nG_jr:PPG_jr     0.02      0.01    -0.01     0.04 1.27       55      268\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.19      0.11     1.98     2.41 1.15       85      260\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n## Imputation During Model Fitting\n\n-   Imputations are made for each incomplete variable using a different conditional model for each variable.\n\n-   This approach differs from MI and MICE in two key ways:\n\n    1.  The model is only fit once since the imputation model is part of the analysis model.\n    2.  The model must be constructed uniquely for each analysis scenario.\n\n\n::: {.cell filename='intro-to-stan.R' output-location='column-fragment' hash='stan_pres_cache/revealjs/brm-mi-fit_90f86b01ea69f026782f8b7272e21e4d'}\n\n```{.r .cell-code}\nbform <- bf(PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr)) +\n    bf(PPG_jr | mi() ~ G_sr + PPG_sr) +\n    bf(G_jr | mi() ~ G_sr + PPG_sr) + set_rescor(FALSE)\nfit_brm_mi <- brm(bform, data = ncaaw_i, \n                  refresh = 500, iter = 2000, thin = 1,\n                  backend = \"cmdstanr\",\n                  control = list(show_exceptions = FALSE),\n                  chains = 2, cores = 2)\nsummary(fit_brm_mi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr) \n         PPG_jr | mi() ~ G_sr + PPG_sr \n         G_jr | mi() ~ G_sr + PPG_sr \n   Data: ncaaw_i (Number of observations: 274) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nPPGsr_Intercept          16.12      2.42    11.19    21.07 1.00      436\nPPGjr_Intercept           5.89      2.45     1.22    10.50 1.00      793\nGjr_Intercept             3.13      5.87    -7.74    15.25 1.00      736\nPPGjr_G_sr               -0.04      0.07    -0.17     0.09 1.00      829\nPPGjr_PPG_sr              0.71      0.08     0.54     0.87 1.00      649\nGjr_G_sr                  0.55      0.17     0.21     0.89 1.00      733\nGjr_PPG_sr                0.28      0.23    -0.16     0.73 1.01      414\nPPGsr_miG_jr             -0.30      0.10    -0.49    -0.10 1.00      432\nPPGsr_miPPG_jr           -0.04      0.15    -0.34     0.28 1.00      421\nPPGsr_miG_jr:miPPG_jr     0.02      0.01     0.01     0.03 1.01      422\n                      Tail_ESS\nPPGsr_Intercept            513\nPPGjr_Intercept           1430\nGjr_Intercept             1326\nPPGjr_G_sr                 992\nPPGjr_PPG_sr              1135\nGjr_G_sr                  1143\nGjr_PPG_sr                 654\nPPGsr_miG_jr               537\nPPGsr_miPPG_jr             467\nPPGsr_miG_jr:miPPG_jr      427\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_PPGsr     1.86      0.09     1.69     2.06 1.00     1062     1226\nsigma_PPGjr     2.30      0.15     2.04     2.63 1.00      566     1217\nsigma_Gjr       5.33      0.39     4.64     6.11 1.00      519     1160\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n## Diagnostics\n\nSince `brms` is built on Stan we can also take a look at the traceplots of the samples in @fig-brm-mi-trace.\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nplot(fit_brm_mi, variable = c(\"b_PPGsr\", \"bsp_\"), regex = TRUE, ask = FALSE, N = 4)\n```\n\n::: {.cell-output-display}\n![Traceplots of brms analysis model parameters.](stan_pres_files/figure-revealjs/fig-brm-mi-trace-1.png){#fig-brm-mi-trace width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nplot(fit_brm_mi, variable = c(\"b_PPGjr\", \"b_Gjr\"), regex = TRUE, ask = FALSE, N = 6)\n```\n\n::: {.cell-output-display}\n![Traceplots of brms imputation model parameters.](stan_pres_files/figure-revealjs/fig-brm-mi-trace-imp-1.png){#fig-brm-mi-trace-imp width=960}\n:::\n:::\n\n:::\n:::\n\n## Comparison of Estimated Effects\n\n\n::: {#fig-brm-mi-cond-eff .cell layout-ncol=\"2\" filename='intro-to-stan.R'}\n\n```{.r .cell-code}\nplot(brms::conditional_effects(fit_brm_mice, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\nplot(brms::conditional_effects(fit_brm_mi, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\n```\n\n::: {.cell-output-display}\n![Estimates after MICE imputation](stan_pres_files/figure-revealjs/fig-brm-mi-cond-eff-1.png){#fig-brm-mi-cond-eff-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Estimates with joint model](stan_pres_files/figure-revealjs/fig-brm-mi-cond-eff-2.png){#fig-brm-mi-cond-eff-2 width=576}\n:::\n\nThe estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior.\n:::\n\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}