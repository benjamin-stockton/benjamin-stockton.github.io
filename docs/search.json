[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\nStockton, B., Santacatterina, M., Mandal, S., Cleland, C. M., Hade, E. M., Illenberger, N., Meropol, S., Troxel, A. B., Petkova, E., Yu, C., & Tarpey, T. (2026). Clarifying the role of placebo response classification in the analysis of the Sequential Parallel Comparison Design. arXiv:2511.19677. https://doi.org/10.48550/arXiv.2511.19677\n\n\nStockton, B., & Harel, O. (2026). (Submitted to American Journal of Epidemiology: Commentary on Resurrecting Complete-Case Analysis: A Defense – The loss of information remains unresolved. American Journal of Epidemiology.\n\n\nGupta, S., Stockton, B., & Harel, O. (2025). Adapting Multiple Imputation for Compositional Survey Data. American Journal of Undergraduate Research, 22, 13–29. https://doi.org/10.33697/ajur.2025.146\n\n\nStockton, B., Gupta, S., Harel, O., & Sinharay, S. (Ed.). (2025). Data Confidentiality, Differential Privacy, and Statistical Disclosure Control. Encyclopedia of Social Measurement (2nd Edition). Elsevier.\n\n\nStockton, B., & Harel, O. (2025). (In Progress) Multiple Imputation for Analyzing Spatial Data with Incomplete Angular Predictors.\n\n\nStockton, B., & Harel, O. (2025). (Submitted to Journal of Applied Statistics: Under Third Review) Incomplete Angular Time Series Imputation with a Projected Normal Autoregressive Process and Exogenous Predictors.\n\n\nStockton, B., Kahn, L., Mehta-Lee, S., & Hade, E. (2025). (In Progress) Assessing the Impacts of Texas’s Six-Week Abortion Ban on Maternal Morbiity Using Electronic Health Records.\n\n\nStockton, B., Kahn, L., Mehta-Lee, S., & Hade, E. (2025). (In Progress) Interrupted Count Time Series Analysis with Nuisance Interruptions.\n\n\nStrange, C. C., Stockton, B., Zvonkovich, J., & Harel, O. (2025). (In Progress) Multiple Imputation Using Pattern-mixture Modeling to Assess the Sensitivity of Race/Ethnicity Effect Estimates in Sentencing Given Non-ignorable Incomplete Data.\n\n\nSidi, Y., Stockton, B., & Harel, O. (2024). Non-inferiority Clinical Trials: Treating Margin as Missing Information. The New England Journal of Statistics in Data Science, 1–8. https://doi.org/10.51387/24-NEJSDS57\n\n\nStockton, B., Strange, C. C., & Harel, O. (2023). Now You See It, Now You Don’t: A Simulation and Illustration of the Importance of Treating Incomplete Data in Estimating Race Effects in Sentencing. Journal of Quantitative Criminology, 40, 563–590. https://doi.org/10.1007/s10940-023-09577-w"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog!\nIn this feed I will be making posts about both my statistical interests (missing data, Bayesian data analysis, etc) and possibly also about Major League Baseball. The posts will supplement my research and provide a space for sharing tutorials, interesting findings, and general thoughts."
  },
  {
    "objectID": "posts/miss-mech-notes/miss-mech-notes.html",
    "href": "posts/miss-mech-notes/miss-mech-notes.html",
    "title": "Missingness Mechanisms",
    "section": "",
    "text": "In this post, I will go through some brief simulations demonstrating how to induce bias in complete case analysis (CCA) for regression coefficient estimates with incomplete data based on the results noted in (Oberman and Vink 2023, sec. 2.3; S. van Buuren 2018, sec. 2.7 and Section 3.2.4). Please refer to these great resources for further details."
  },
  {
    "objectID": "posts/miss-mech-notes/miss-mech-notes.html#how-to-get-biased-estimates-with-cca",
    "href": "posts/miss-mech-notes/miss-mech-notes.html#how-to-get-biased-estimates-with-cca",
    "title": "Missingness Mechanisms",
    "section": "How to Get Biased Estimates with CCA",
    "text": "How to Get Biased Estimates with CCA\n(Oberman and Vink 2023) notes that you don’t always get biased estimates with CCA. In fact there are special cases where a seemingly MAR mechanism can function in practice as MCAR during simulations or where CCA is super-efficient while MI is biased under certain MNAR mechanisms. They discuss one condition in particular that is required for bias: the variable to be amputed must be correlated with the probability of being missing.\nOther conditions and cases are discussed in (S. van Buuren 2018, sec. 2.7).\n\nIn single predictor regression of \\(Y = X\\beta + \\epsilon,\\) the CCA is equivalent to MI if only \\(Y\\) is incomplete when estimating regression coefficients.\n\nIf \\(X\\) is also incomplete or there are other variables to include in an imputation model, then MI is preferred.\n\nIf missingness does not depend on \\(Y,\\) then the regression coefficients are unbiased under CCA with missing data on either (or both) \\(X\\) and \\(Y.\\)\nLogistic regression is unbiased under CCA with missing data on only \\(Y\\) or only \\(X,\\) but not both.\n\nSo to get a biased estimate with CCA we need:\n\nAn association between \\(X\\) and \\(Y; \\beta \\neq 0.\\)\n\\(Y\\) must be associated with \\(P(R = 0)\\) where \\(R = 0\\) indicates missingness.\nThe to-be-amputed variable must be associated with \\(P(R = 0).\\)\nMissingness on the predictors \\(X.\\)\nFor MI to be more effective than CCA, more than \\(Y\\) must be related to the amputed variable."
  },
  {
    "objectID": "posts/miss-mech-notes/miss-mech-notes.html#some-univariate-missingness-mechanisms",
    "href": "posts/miss-mech-notes/miss-mech-notes.html#some-univariate-missingness-mechanisms",
    "title": "Missingness Mechanisms",
    "section": "Some Univariate Missingness Mechanisms",
    "text": "Some Univariate Missingness Mechanisms\nConsider a data set with a response denoted by \\(Y\\) and predictors denoted by \\(X_1, \\dots, X_3\\) respectively. We want to analyze the data using a standard multiple linear regression model,\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon,\n\\tag{1}\\]\nwhere \\(\\epsilon\\) is an iid normally distributed error term \\(\\epsilon \\overset{iid}{\\sim} N(0, \\sigma^2_\\epsilon).\\) Let \\(R\\) be an indicator for whether the observation for \\(X_1\\) is observed; \\(R = 1\\) means \\(X_1\\) is observed and \\(R = 0\\) means it is missing.\nThree mechanisms inspired by (S. van Buuren 2018, sec. 3.2.4) include:\n\nMAR Right: \\(\\mathrm{logit}(P(R = 0)) = -\\alpha_0 + Y\\)\nMAR Mid: \\(\\mathrm{logit}(P(R = 0)) = -\\alpha_0 - |Y - \\tilde{Y}|\\) where \\(\\tilde{Y}\\) is the median of \\(Y.\\)\nMAR Tail: \\(\\mathrm{logit}(P(R = 0)) = -\\alpha_0 + |Y - \\tilde{Y}|\\)\n\nwhere \\(\\alpha_0\\) is set in each case to guarantee the simulated missingness matches the desired proportion \\(p_{miss}; ~~\\alpha_0 = - \\bar{U} - \\log(1 / p_{miss} - 1)\\) and \\(\\bar{U} = \\frac{1}{n} \\sum_{i=1}^n U_i\\) and \\(U_i = 3Y_i; ~-|Y_i - \\tilde{Y}|; \\mathrm{~ or ~~} |Y_i - \\tilde{Y}|\\) respectively."
  },
  {
    "objectID": "posts/miss-mech-notes/miss-mech-notes.html#on-to-the-simulation",
    "href": "posts/miss-mech-notes/miss-mech-notes.html#on-to-the-simulation",
    "title": "Missingness Mechanisms",
    "section": "On to the Simulation!",
    "text": "On to the Simulation!\nWith these simulations, I am trying to induce bias in the CCA estimate of the regression coefficient corresponding to \\(X_1\\) when predicting the response \\(Y.\\) I will run these simulations over a range of missingness proportions with each of the three mechanisms, but also over several values of \\(\\beta.\\)\n\n\nShow code\nlibrary(compositions, quietly = TRUE, warn.conflicts = FALSE)\n\n\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n\n\nShow code\nlibrary(mice, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(zCompositions, quietly = TRUE, warn.conflicts = FALSE)\n\n\n\nAttaching package: 'NADA'\n\n\nThe following object is masked from 'package:compositions':\n\n    cor\n\n\nThe following object is masked from 'package:stats':\n\n    cor\n\n\nShow code\nlibrary(zoo, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(dplyr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(generics, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(readr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(ggplot2, quietly = TRUE, warn.conflicts = FALSE)\n\ntheme_set(theme_classic())\n\n\n\nData Generation\nThe data are generated from the linear model in Equation 1. There’s a pre-set intercept of \\(\\beta_0 = 5\\) but we can change any of the other coefficients as desired.\n\n\nShow code\ngenerate_data &lt;- function(N = 100, beta = c(1, 2, 3)) {\n  X &lt;- matrix(rnorm(N * 3), nrow = N, ncol = 3)\n  df &lt;- as.data.frame(X)\n  colnames(df) &lt;- paste0(\"X\", 1:3)\n\n  df$Y &lt;- (5 + as.matrix(df[,1:3]) %*% beta + rnorm(N, 0, 1))[,1]\n  return(df)\n}\n\n\n\n\nMissingness Mechanism\nWe can use any of the three missingness mechanisms described above. Their shapes are plotted in Figure 1.\n\n\nShow code\nimpose_missing &lt;- function(df, p_miss = 0.5, alpha = NULL, mech = NULL) {\n    \n  U &lt;- df$Y\n  if (!is.null(mech)) {\n    if (mech == \"MCAR\") {\n        U &lt;- rep(0, nrow(df))\n    }\n    else if (mech == \"MAR_MID\") {\n        U &lt;- -abs(df$Y - mean(df$Y))\n    }\n    else if (mech == \"MAR_TAIL\") {\n        U &lt;- abs(df$Y - mean(df$Y))\n    }\n    else if (mech == \"MAR_RIGHT\") {\n        U &lt;- df$Y\n    }\n    else if (mech == \"MNAR\") {\n        U &lt;- abs(df$X1 - median(df$X1, na.rm = TRUE))\n    }\n  }\n  \n  alpha0 &lt;- -mean(U) - log(1 / p_miss - 1)\n  \n  mis_p &lt;- boot::inv.logit(alpha0 + U)\n  mis_ind &lt;- sample(1:nrow(df), size = floor(p_miss * nrow(df)),\n                    replace = FALSE, prob = mis_p)\n  R &lt;- rep(0, nrow(df))\n  R[mis_ind] &lt;- 1\n  \n  df_inc &lt;- df\n  df_inc$R &lt;- R\n  df_inc$p_miss &lt;- mis_p\n  df_inc$X1_inc &lt;- df$X1\n  df_inc$X1_inc[mis_ind] &lt;- NA\n  return(df_inc)\n}\n\n\n\n\nShow code\nsim_dat &lt;- generate_data(N = 250,\n                         beta = c(1, 2, 3))\n\ndf_inc_mid &lt;- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_MID\")\ndf_inc_tail &lt;- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_TAIL\")\ndf_inc_right &lt;- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_RIGHT\")\n\nsim_dat$p_miss_MID &lt;- df_inc_mid$p_miss\nsim_dat$p_miss_TAIL &lt;- df_inc_tail$p_miss\nsim_dat$p_miss_RIGHT &lt;- df_inc_right$p_miss\nsim_dat |&gt;\n  ggplot(aes(Y, p_miss_MID)) +\n  geom_line() +\n  geom_line(aes(Y, p_miss_TAIL), color = \"tomato\", linetype = \"dotted\") +\n  geom_line(aes(Y, p_miss_RIGHT), color = \"dodgerblue\", linetype = \"dashed\") +\n    annotate(geom = \"text\", x = 10, y = 0.7,\n             label = \"MAR MID\") +\n    annotate(geom = \"text\", x = -1, y = 0.5,\n             label = \"MAR TAIL\",\n             color = \"tomato\") +\n    annotate(geom = \"text\", x = 7, y = 0.25,\n             label = \"MAR RIGHT\",\n             color = \"dodgerblue\") + \n    labs(y = \"Pr(R = 0)\")\n\n\n\n\n\n\n\n\nFigure 1: Missingness mechanism as function of Y.\n\n\n\n\n\n\n\nAnalysis Model\nThe analysis model stays the same as Equation 1. I will fit model to the complete data and analyze the incomplete data with CCA and MI (performed by mice with \\(M = 15\\) imputations) (S. van Buuren and Groothuis-Oudshoorn 2010).\n\n\nShow code\n## Complete data\ndf_inc &lt;- impose_missing(sim_dat, p_miss = 0.5, mech = \"MAR_RIGHT\")\nfit &lt;- lm(Y ~ X1 + X2 + X3, data = sim_dat)\n\nsmry1 &lt;- summary(fit)\n# smry1\n\n## CCA\nfit &lt;- lm(Y ~ X1_inc + X2 + X3, data = df_inc)\nsmry2 &lt;- summary(fit)\n# smry2\n\n## MI\nimp &lt;- mice::mice(df_inc[, c(\"Y\", \"X1_inc\", \"X2\", \"X3\")], \n                  m = 15, maxit = 10, method = \"norm\", printFlag = FALSE)\nfit &lt;- with(imp, lm(Y ~ X1_inc + X2 + X3))\nsmry3 &lt;- summary(pool(fit))\n\nc(\"complete\" = smry1$coefficients[\"X1\", \"Estimate\"],\n  \"cca\" = smry2$coefficients[\"X1_inc\", \"Estimate\"],\n  \"mi\" = smry3[which(smry3$term == \"X1_inc\"), \"estimate\"])\n\n\n complete       cca        mi \n1.0168138 0.9113908 0.9542853 \n\n\n\n\nSimulation Structure\nThe simulation follows the same Monte Carlo set-up as usual.\n\nGenerate the data.\nImpose missingness.\nFit the model using the complete data, CCA, and/or MI.\nCollect results and format.\n\n\n\nShow code\nsimulate_lm_cca &lt;- function(sim_setting, N_sim, data, methods = c(\"cca\")) {\n  sim_res &lt;- lapply(1:nrow(sim_setting), function(i) {\n    # print(paste0(\"Setting: \", i))\n    lapply(1:N_sim, function(q) {\n      N &lt;- sim_setting$N[i]\n      mech &lt;- sim_setting$mech[i]\n      beta_X1 &lt;- sim_setting$beta[i]\n      beta_vec &lt;- c(5, beta_X1, 2, 1)\n      p_miss &lt;- sim_setting$p_miss[i]\n      ## Generate data\n      sim_dat &lt;- generate_data(N = N, beta = beta_vec[2:4]) \n      \n      ## Impose Missingness\n      df_inc &lt;- impose_missing(sim_dat, p_miss = p_miss, mech = mech)\n      \n      # Calculate association between Y and probability X1 is missing\n      smry_mm &lt;- summary(glm(R ~ Y, data = df_inc,\n                             family = binomial(link = \"logit\")))\n      \n      z_mm &lt;- smry_mm$coefficients[2, 3]\n      \n      ## Fit Complete Data Model\n      fit &lt;- lm(Y ~ X1 + X2 + X3,\n                data = sim_dat)\n      smry &lt;- summary(fit)\n      coef &lt;- as.data.frame(smry$coefficients)\n      coef$Variable &lt;- rownames(coef)\n      coef$true &lt;- beta_vec\n      rownames(coef) &lt;- NULL\n      coef$method &lt;- \"complete\"\n      for (method in methods) {\n        \n        if (method == \"cca\") {\n          ## Fit the CCA Model\n          tmp &lt;- stats::na.omit(df_inc)\n          fit &lt;- lm(Y ~ X1_inc + X2 + X3,\n                    data = tmp)\n          smry &lt;- summary(fit)\n          coef2 &lt;- as.data.frame(smry$coefficients)\n          \n          coef2$Variable &lt;- rownames(coef2)\n          rownames(coef2) &lt;- NULL\n          \n          mean_Y_inc &lt;- mean(tmp$Y)\n        }\n        else if (method == \"mi\") {\n          ## Fit the MI model\n          # print(\"running mi\")\n          tmp &lt;- df_inc |&gt;\n            select(Y, X1_inc, X2, X3)\n          \n          # Impute 5 times (not enough, but a start)\n          imp &lt;- mice::mice(tmp, m = 5, method = \"norm\",\n                            printFlag = FALSE, maxit = 5)\n          fit &lt;- with(imp, lm(Y ~ X1_inc + X2 + X3))\n          coef2 &lt;- summary(pool(fit))\n          coef2 &lt;- coef2 |&gt;\n            mutate(\n              Estimate = estimate,\n              `Std. Error` = std.error,\n              `t value` = statistic,\n              `Pr(&gt;|t|)` = p.value,\n              Variable = term\n            ) |&gt;\n            select(Estimate, `Std. Error`, `t value`, `Pr(&gt;|t|)`, Variable)\n        }\n        coef2$true &lt;- beta_vec[1:nrow(coef2)]\n        coef2$method &lt;- method\n        \n        coef &lt;- bind_rows(coef, coef2)\n      }\n      coef$iter &lt;- q + (i - 1) * N_sim\n      coef$beta1 &lt;- beta_X1\n      coef$z_Y_mm &lt;- z_mm\n      coef$p_miss &lt;- p_miss\n      coef$mech &lt;- mech\n      coef$diff_X1_mean &lt;- mean(sim_dat$X1) - mean(df_inc$X1, na.rm = TRUE)\n      coef$diff_Y_mean &lt;- mean(sim_dat$Y) - mean_Y_inc\n      coef$cor_pmiss_X1 &lt;- cor(df_inc$p_miss, sim_dat$X1, use = \"everything\")\n      return(coef)\n    }) |&gt;\n      dplyr::bind_rows()\n  }) |&gt; \n    dplyr::bind_rows()\n  \n  return(sim_res)\n}\n\n\n\n\n\nTable 1\n\n\n\nShow code\nsim_setting &lt;- expand.grid(\n  N = seq(from = 100, to = 1000, length.out = 1),\n  p_miss = seq(from = 0.25, to = 0.75, length.out = 5),\n  beta = seq(from = -5, to = 5, length.out = 5),\n  mech = c(\"MAR_MID\", \"MAR_TAIL\", \"MAR_RIGHT\")\n) \n# sim_setting |&gt;\n#     kableExtra::kbl(format = \"markdown\")\n\n\n\n\n\n\nShow code\nN_sim &lt;- 250\nstart &lt;- Sys.time()\n\nsim_res &lt;- simulate_lm_cca(sim_setting = sim_setting, N_sim = N_sim,\n                           data = dat, methods = c(\"cca\", \"mi\"))\n\nprint(Sys.time() - start)\n\n\nTime difference of 12.34824 mins\n\n\n\n\nShow code\nsim_res |&gt;\n  mutate(\n    beta_label = stringr::str_c(\"beta_1 = \", round(beta1, 2))\n  ) |&gt;\n  filter(stringr::str_detect(Variable, \"X1_inc\")) |&gt;\n  ggplot(aes(as.factor(round(p_miss, 2)), cor_pmiss_X1, fill = mech)) +\n  geom_boxplot() +\n  facet_wrap(beta_label~Variable, ncol = 5) +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n    labs(x = latex2exp::TeX(\"$p_{miss}$\"),\n         y = latex2exp::TeX(\"$cor(p_{miss}, X_1)$\"))\n\n\n\n\n\n\n\n\nFigure 2: Boxplots of the correlations between X1 and P(R=0).\n\n\n\n\n\n\n\nShow code\nsum_sim &lt;- sim_res |&gt;\n  mutate(\n    beta_label = stringr::str_c(\"beta_1 = \", round(true, 2))\n  ) |&gt;\n  filter(stringr::str_detect(Variable, \"X1\")) |&gt;\n  group_by(Variable, method, mech, beta_label, beta1, p_miss) |&gt;\n  summarize(\n    n = n(),\n    mean_est = mean(Estimate),\n    sd_est = sd(Estimate),\n    mean_bias = mean(Estimate - true), \n    sd_bias = sd_est,\n    mean_abs_bias = mean(abs(Estimate - true)),\n    sd_abs_bias = sd(abs(Estimate - true)),\n    mean_rel_bias = mean((Estimate - true) / abs(true)),\n    mean_se = mean(`Std. Error`),\n    sd_se = sd(`Std. Error`),\n    mse = mean((Estimate - true)^2),\n    sd_mse = sqrt(1/(N_sim - 1) * mean(((Estimate - true)^2 - mse)^2)),\n    mean_cor = mean(cor_pmiss_X1),\n    prop_sig_z_mm = mean(abs(z_Y_mm) &gt; 1.96)\n  ) |&gt;\n  ungroup()\n\nsum_sim$beta_label &lt;- forcats::fct_reorder(sum_sim$beta_label, sum_sim$beta1)\nsum_sim$method &lt;- forcats::fct_relevel(sum_sim$method, c(\"complete\", \"cca\", \"mi\"))\n\n\n\n\nShow code\nbiasplt &lt;- sum_sim |&gt;\n  ggplot(aes(p_miss, mean_bias, color = method)) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  geom_errorbar(aes(x = p_miss,\n                    ymin = mean_bias - 2 * sd_bias,\n                    ymax = mean_bias + 2 * sd_bias,\n                    color = method),\n                width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"Bias\")\n\n\n\n\nShow code\nrelbiasplt &lt;- sum_sim |&gt;\n  ggplot(aes(p_miss, mean_rel_bias, color = method)) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  # geom_errorbar(aes(x = p_miss,\n  #                   ymin = mean_abs_bias - 2 * sd_abs_bias,\n  #                   ymax = mean_abs_bias + 2 * sd_abs_bias,\n  #                   color = method),\n  #               width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"Relative Bias\")\n\n\n\n\nShow code\nmseplt &lt;- sum_sim |&gt;\n  ggplot(aes(p_miss, mse, color = method)) +\n  geom_point(aes(shape = method)) +\n  geom_line(aes(linetype = method), alpha = 0.5) +\n  geom_errorbar(aes(x = p_miss,\n                    ymin = mse - 2 * sd_mse,\n                    ymax = mse + 2 * sd_mse,\n                    color = method),\n                width = 0.05, alpha = 0.35) +\n  facet_grid(mech ~ beta_label, scales = \"free\") +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1),\n          legend.position = \"bottom\") +\n    labs(x = latex2exp::TeX(\"$p_{miss}\"),\n         y = \"MSE\")\n\n\n\n\nShow code\nbiasplt\nmseplt\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3: Simulation results."
  },
  {
    "objectID": "posts/miss-mech-notes/miss-mech-notes.html#references",
    "href": "posts/miss-mech-notes/miss-mech-notes.html#references",
    "title": "Missingness Mechanisms",
    "section": "References",
    "text": "References\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 168.\n\n\nBuuren, Stef van. 2018. Flexible Imputation of Missing Data. 2nd ed. Interdisciplinary Statistics Series. Chapman; Hall/CRC. https://stefvanbuuren.name/fimd/.\n\n\nOberman, Hanne I., and Gerko Vink. 2023. “Toward a Standardized Evaluation of Imputation Methodology.” Biometrical Journal n/a (n/a): 2200107. https://doi.org/10.1002/bimj.202200107.\n\n\n\n\nShow code\nsessionInfo()\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.3\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggplot2_3.5.1         readr_2.1.5           generics_0.1.3       \n [4] dplyr_1.1.4           zoo_1.8-12            zCompositions_1.5.0-4\n [7] truncnorm_1.0-9       NADA_1.6-1.1          survival_3.6-4       \n[10] MASS_7.3-60.2         mice_3.16.0           compositions_2.0-8   \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5        shape_1.4.6.1       tensorA_0.36.2.1   \n [4] xfun_0.47           htmlwidgets_1.6.4   lattice_0.22-6     \n [7] tzdb_0.4.0          vctrs_0.6.5         tools_4.4.1        \n[10] tibble_3.2.1        fansi_1.0.6         DEoptimR_1.1-3     \n[13] pan_1.9             pkgconfig_2.0.3     jomo_2.7-6         \n[16] Matrix_1.7-0        lifecycle_1.0.4     stringr_1.5.1      \n[19] farver_2.1.2        compiler_4.4.1      munsell_0.5.1      \n[22] codetools_0.2-20    htmltools_0.5.8.1   yaml_2.3.10        \n[25] glmnet_4.1-8        pillar_1.9.0        nloptr_2.1.1       \n[28] tidyr_1.3.1         iterators_1.0.14    rpart_4.1.23       \n[31] boot_1.3-30         foreach_1.5.2       mitml_0.4-5        \n[34] nlme_3.1-164        robustbase_0.99-4-1 tidyselect_1.2.1   \n[37] digest_0.6.37       stringi_1.8.4       purrr_1.0.2        \n[40] forcats_1.0.0       labeling_0.4.3      splines_4.4.1      \n[43] latex2exp_0.9.6     cowplot_1.1.3       fastmap_1.2.0      \n[46] grid_4.4.1          colorspace_2.1-1    cli_3.6.3          \n[49] magrittr_2.0.3      utf8_1.2.4          broom_1.0.7        \n[52] withr_3.0.1         scales_1.3.0        backports_1.5.0    \n[55] rmarkdown_2.28      nnet_7.3-19         lme4_1.1-35.5      \n[58] hms_1.1.3           kableExtra_1.4.0    evaluate_1.0.0     \n[61] knitr_1.48          viridisLite_0.4.2   mgcv_1.9-1         \n[64] rlang_1.1.4         Rcpp_1.0.13         glue_1.8.0         \n[67] bayesm_3.1-6        xml2_1.3.6          svglite_2.1.3      \n[70] rstudioapi_0.16.0   minqa_1.2.8         jsonlite_1.8.9     \n[73] R6_2.5.1            systemfonts_1.1.0"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#installation",
    "href": "posts/intro-to-stan/intro_to_stan.html#installation",
    "title": "Getting to know Stan - SWOSC",
    "section": "Installation",
    "text": "Installation\nTo get started, we’ll head over to Stan’s documentation to see how to get set up in R. For this presentation, I’ll use the CmdStan toolchain that’s implemented in R by the cmdstanr package (Gabry, Češnovar, and Johnson 2023). There are also Python, command line, Matlab, Julia, and Stata interfaces to Stan and a Python interface for cmdstan called CmdStanPy (“Cmdstanpy  Python Interface to CmdStan  CmdStanPy 1.1.0 Documentation,” n.d.).\n\n\n\nCode\n\nintro-to-stan.R\n\nset.seed(98463)\n\n# install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\nlibrary(cmdstanr)\n\n\n\nThis is cmdstanr version 0.6.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: C:/Users/stocb/OneDrive/Documents/.cmdstan/cmdstan-2.33.1\n\n\n- CmdStan version: 2.33.1\n\n\nCode\n# cmdstanr::install_cmdstan()\ncmdstanr::cmdstan_version()\ncmdstanr::cmdstan_path()\ncmdstanr::check_cmdstan_toolchain()\n\n\nThe C++ toolchain required for CmdStan is setup properly!\n\n\nCode\n# install.packages(c(\"bayesplot\", \"ggplot2\", \"posterior\"))\n\n\n[1] \"2.33.1\"\n[1] \"C:/Users/stocb/OneDrive/Documents/.cmdstan/cmdstan-2.33.1\""
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "href": "posts/intro-to-stan/intro_to_stan.html#a-very-brief-introduction-to-bayesian-data-analysis",
    "title": "Getting to know Stan - SWOSC",
    "section": "A Very Brief Introduction to Bayesian Data Analysis",
    "text": "A Very Brief Introduction to Bayesian Data Analysis\nThe broadest conceptual overview of Bayesian data analysis is that this methodology allows us to incorporate prior knowledge about the model/data into our analysis which is based on a posterior distribution that is derived from the prior and likelihood. Bayesian inference treats the parameters of the model as random variables whose distribution we are interested in either deriving analytically or approximating through computation. This is a key distinction from frequentist methods taught in math stat and applied stat where parameters are fixed values and their estimators are functions of a random sample and the sampling distribution of the estimator is used for inference.\n\n\n\n\n\n\nBayes Rule\n\n\n\nA quick reminder of Bayes Rule.\nLet \\(\\theta\\) be a random variable with (prior) distribution \\(p(\\theta)\\), \\(Y\\) be a random variable that depends on \\(\\theta\\) with conditional distribution or likelihood \\(p(y | \\theta)\\). Then their joint distribution is \\(p(y, \\theta) = p(y|\\theta) p(\\theta)\\).\nBayes rule lets us flip the conditioning from the likelihood to get \\(p(\\theta | y)\\)\n\\[\np(\\theta | y) = \\frac{p(y, \\theta)}{p(y)} = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta} \\propto p(y|\\theta) p(\\theta)\n\\]\n\n\nTo be a little more specific, we have three central components to the model. For ease of exposition, let’s consider the simple regression model \\(E(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) where we have \\(i = 1,\\dots,N\\) observations of \\((Y_i, X_i)'\\) from Ch. 14 of Bayesian Data Analysis (Gelman et al. 2013, 354–58). From here on I will suppress conditioning on the observed predictor \\(X_i = x_i\\) since we are considering the design matrix \\(X = (1_N, \\mathbf{x})\\) to be fixed and known where \\(\\mathbf{x} = (x_1,\\dots, x_N)'\\).\n\nThe Prior: a distribution for the parameters that doesn’t depend on the data \\(p(\\theta)\\).\nThe (Data) Likelihood: a model for the data that depends on the parameters \\(p(y | \\theta)\\).\nThe Posterior: a distribution that uses Bayes rule to define distribution of the parameters given the data \\(p(\\theta|y)\\)."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#why-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#why-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nStan is one of several ways to run MCMC for Bayesian inference\n\nNimble and OpenBUGS are two other languages dedicated to probabilistic programming\nR, Rcpp, Julia, and Python are other more general purpose options for writing the sampler from scratch\n\nOther methods use combinations of Gibbs, Metropolis-Hastings, and slice sampling; Stan uses Hamiltonian Monte Carlo and the No-U-Turn Sampler (NUTS) which is more efficient and typically requires less thinning\nStan only allows for continuous parameters"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-examples-in-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some examples in Stan",
    "text": "Some examples in Stan\nWe’ll continue with the single predictor regression model to model NCAA Women’s Basketball team’s total wins by their 3 point field goal percentage from the 2022-2023 season. Data collected from NCAA.\n\n\n\nCode\n\nintro-to-stan.R\n\nncaaw &lt;- readr::read_csv(file = \"Data/NCAAW-freethrows-threes-2022-2023.csv\")\n\n\n\nRows: 350 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Team, WL\ndbl (9): G, FT, FTA, FTpct, FG3, FG3A, FG3pct, W, L\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the 2022-2023 season there were \\(N = 350\\) teams. The relationship between their wins and three point percentage is displayed in Figure 1.\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\n\n\nFigure 1: Scatter plot of the Total Wins by 3 pt Field Goal %.\n\n\n\n\n\nAs a baseline, we’ll find the maximum likelihood estimates for the regression parameters and variance.\n\n\n\nCode\n\nintro-to-stan.R\n\nfit_ml &lt;- lm(W ~ FG3pct, data = ncaaw)\n(beta_ml &lt;- coef(fit_ml))\nsmry_ml &lt;- summary(fit_ml)\n(sigma_ml &lt;- smry_ml$sigma)\nmles &lt;- data.frame(Parameters = c(\"beta_0\", \"beta_1\", \"sigma\"),\n                   Estimates = c(beta_ml, sigma_ml))\n\n\n\n(Intercept)      FG3pct \n  -14.94468     1.00929 \n[1] 5.908144\n\n\nThe fitted line is displayed in Figure 2.\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(ggplot2)\nggplot(ncaaw, aes(FG3pct, W)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"2022-23 NCAAW Wins by 3pt%\", \n         subtitle = paste0(\"r = \", round(cor(ncaaw$W, ncaaw$FG3pct), 3)),\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 2: Now the OLS regression line is super-imposed in blue.\n\n\n\n\n\n\nNon-informative Prior Regression Model\nLet’s consider the simple regression model \\(E(Y_i | X_i, \\boldsymbol{\\beta}, \\sigma^2) = \\beta_0 + \\beta_1 X_i\\) and \\(Var(Y_i | X_i = x_i, \\boldsymbol{\\beta}, \\sigma^2) = \\sigma^2\\) (Gelman et al. 2013, 354–58).\n\nThe Prior: \\(p(\\boldsymbol{\\beta}, \\log\\sigma) = 1 \\equiv p(\\boldsymbol{\\beta}, \\sigma^2) \\propto \\sigma^{-2}\\)\nThe (Data) Likelihood: \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nThe Posterior:\n\n\\[\\begin{align*}\n    p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{y}) &= p(\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{y}) \\times p(\\sigma^2 | \\mathbf{y}) \\\\\n        &= N_2(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\beta}}, \\sigma^2 (X'X)^{-1}) \\times Inv-\\chi^2 (\\sigma^2 | N-2, s^2) \\\\\n    \\hat{\\boldsymbol{\\beta}} &= (X'X)^{-1} X'\\mathbf{y} \\\\\n    s^2 &= \\frac{1}{N-2} (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})' (\\mathbf{y} - X\\hat{\\boldsymbol{\\beta}})   \n\\end{align*}\\]\nFirst we write the Stan code in a separate file. See the Stan User’s Guide Part 1.1 for programming this model without the analytic posteriors. (Download the file here)\n\n\nnon-informative-regression.stan\n\n// The input data is two vectors 'y' and 'X' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  vector[N] x;\n}\n\ntransformed data {\n    matrix[N, 2] X_c = append_col(rep_vector(1, N), x);\n    matrix[2,2] XtX_inv = inverse(X_c' * X_c);\n\n    vector[2] beta_hat = XtX_inv * X_c' * y;\n    vector[N] y_hat = X_C * beta_hat;\n    \n    real&lt;lower=0&gt; s_2 = 1 / (N - 2) * (y - y_hat)' * (y - y_hat);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'beta' and 'sigma'.\nparameters {\n  vector beta;\n  real&lt;lower=0&gt; sigma; // Note that this is the variance\n}\n\n// The model to be estimated. We model the output\n// 'y' ~ N(x beta, sigma) by specifying the analytic\n// posterior defined above.\nmodel {\n  beta ~ multi_normal(beta_hat, sigma^2 * XtX_inv);\n  \n  sigma^2 ~ scaled_inv_chi_square(N-2, sqrt(s_2));\n}\n\ngenerated quantities {\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\n\nNext we fit the model using cmdstanr. In this case, I will use 1000 warmup iterations, 1000 sampling iterations, with no thinning (thinning includes only every \\(k\\)th draw), and will refresh the print screen to see progress every 500 iterations. We can run several chains to see if where the chains start dictates any part of the posterior shape or location, and chains can be run in parallel to get more draws &lt;=&gt; better posterior approximation more quickly.\n\n\n\nCode\n\nintro-to-stan.R\n\ndata_list &lt;- list(\n    N = nrow(ncaaw),\n    y = ncaaw$W,\n    x = ncaaw$FG3pct\n)\n\nfile &lt;- file.path(\"non-informative-regression.stan\")\nnon_inf_model &lt;- cmdstan_model(file)\n\nfit1 &lt;- non_inf_model$sample(\n    data = data_list,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.4 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.9 seconds.\n\n\nNext we’ll check diagnostics for the sampler. First, we will look at the numeric diagnostic output from the method $diagnostic_summary() which reports if any transitions were divergent, if maximum tree depth was reached, and EBFMI. For this model and data set we don’t see any issues in these summaries.\nNext, we check the traceplots in Figure 3 (a). The MCMC draws can be collected from the fit object using the $draws() method. These plots display the sampled values for each parameter in a line plot. We are looking for a horizontal fuzzy bar. Then we can also look at density plots in Figure 3 (b) which will tell us if the chains reached reasonably similar densities. On both counts, we are in good shape. The plots are created using the bayesplot package.\n\n\n\nCode\n\nintro-to-stan.R\n\nfit1$diagnostic_summary()\n\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 1.158355 1.007918\n\n\n\nThis is bayesplot version 1.10.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nCode\nmcmc_trace(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit1$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(bayesplot)\n\n\n\n\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 3: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\n\nFinally, after checking that the MCMC chains and diagnostics look satisfactory, we can continue to inference. The summary statistics for the parameters are displayed in Table 1. This are generated by default with the $summary() method. The statistics include the posterior mean (mean), median (median), standard deviation (sd), mean absolute deviation (mad), and lower (q5) and upper bounds (q95) for a 90% credible interval. The statistics rhat, ess_bulk, and ess_tail are additional diagnostic measures that indicate how well the chains are sampling the posterior and how many effective draws we have made. Ideally rhat is very near 1, even 1.01 can be a significant problem. The effective sample sizes should be large/near the number of sampling iterations.\n\n\n\nCode\n\nintro-to-stan.R\n\nfit1$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\n\nTable 1: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-15.170201\n-15.17435\n2.7264741\n2.6386574\n-19.6968650\n-10.670060\n1.003857\n600.4229\n668.3881\n\n\nbeta[2]\n1.016536\n1.01743\n0.0876394\n0.0866691\n0.8718595\n1.159618\n1.004031\n596.0433\n630.5212\n\n\nsigma\n5.927539\n5.92005\n0.2185852\n0.2262596\n5.5846900\n6.297106\n1.001305\n862.2062\n822.5273\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nmles |&gt; \n    kableExtra::kbl(booktabs = TRUE, \n                    format = \"html\", digits = 3)\n\n\n\n\n\nTable 2: MLE estimates for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nParameters\nEstimates\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n\n\nFG3pct\nbeta_1\n1.009\n\n\n\nsigma\n5.908\n\n\n\n\n\n\n\n\n\n\nWe can also check graphical summaries of this same information such as interval plots for each parameter’s credible intervals or density/area plots. In Figure 4 (a) we have the 50% (thick bar) and 95% (thin bar) credible intervals with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 4 (b) with areas shaded underneath to indicate the 50% interval and the width of the density indicates the 95% interval.\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nmcmc_intervals(fit1$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit1$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\n\n\n\nFigure 4: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\n\nOne additional way to check model fit is to assess posterior predictive checks. To do so we draw samples from the posterior predictive distribution \\(p(y^{new} | y) = \\int p(y^{new} | \\boldsymbol{\\beta}, \\sigma) p(\\boldsymbol{\\beta}, \\sigma | y) d\\boldsymbol{\\beta}d\\sigma\\) by first sampling from the posterior (i.e. the draws in the MCMC chains) and then for each set of draws sampling \\(y^{new}\\) given the corresponding values for \\(x^{new}\\). In Stan this is easily accomplished using the generated quantities block. The generated quantities block generates new samples that we define using the current iteration’s posterior draws of \\(\\beta\\) and \\(\\sigma\\).\ngenerated quantities {\n    // create a vector of N new observations\n    vector[N] y_ppd; \n    \n    // for each observation, sample from the regression likelihod\n    // using the posterior draws\n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_c[i,] * beta, sigma);\n    }\n}\nWe collect the PPD draws from the fit object using the draws method. From Figure 5 we can see that while the predictive densities are centered in the correct location, the variances are far too large.\n\nThis is posterior version 1.5.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:bayesplot':\n\n    rhat\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nCode\ny_ppd &lt;- as.matrix(as_draws_df(fit1$draws(variables = \"y_ppd\")))\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(posterior)\n\n\n\n\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\n\n\n\nFigure 5: Posterior Predictive Check plots from bayesplot.\n\n\n\n\n\nConjugate Prior Regression Model\nNext, we’ll implement the regression model with conjugate priors. Conjugacy refers to the situation where the prior and posterior distribution are from the same family. We’ll start by re-defining our model.1\n\nConjugate prior: \\(p(\\boldsymbol{\\beta}, \\sigma^2) = p(\\boldsymbol{\\beta} | \\sigma^2) p(\\sigma^2)\\)\n\n\\(\\boldsymbol{\\beta} | \\sigma^2 ~ N_2(\\boldsymbol{\\beta}_0, \\sigma^2 \\Lambda_0^{-1})\\) where \\(\\boldsymbol{\\beta}_0 \\in \\mathbb{R}^2\\) is a vector of prior coefficients, we’ll set it to zero, and \\(\\Lambda_0\\) is a \\(2\\times2\\) prior correlation matrix. We will set \\(\\Lambda_0 = 10 I_2\\) to get a weakly informative prior that is equivalent to ridge regression.\n\\(\\sigma^2 \\sim InvGamma(\\frac{\\nu_0}{2}, \\frac{1}{2} \\nu_0 s_0^2)\\) where \\(\\nu_0\\) is a prior sample size and \\(s_0\\) is the prior standard deviation. We’ll set these to \\(\\nu_0 = 1\\) and \\(s_0^2 = 47\\) which is approximately the sample variance of the NCAA women’s basketball teams’ wins.\nThe parameters \\(\\boldsymbol{\\beta}_0, \\Lambda_0, \\nu_0, s_0^2\\) that define the prior are referred to as hyperparameters. We will set them before running the model, although they could also be modeled if we wanted.\n\nThe (Data) Likelihood: the same as before, \\(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim N_N(X\\boldsymbol{\\beta}, \\sigma^2 I_N).\\)\nPosterior:\n\n\\(\\boldsymbol{\\beta} | \\sigma^2, y \\sim N_2(\\boldsymbol{\\beta}_N, \\sigma^2 \\Lambda_N^{-1})\\) where \\(\\boldsymbol{\\beta}_N = \\Lambda_N^{-1}(\\mathbf{X}'\\mathbf{X} \\hat{\\boldsymbol{\\beta}} + \\Lambda_0 \\boldsymbol{\\beta}_0)\\) and \\(\\Lambda_N = (\\mathbf{X}'\\mathbf{X} + \\Lambda_0).\\)\n\\(\\sigma^2 | y \\sim InvGamma(\\sigma^2 | \\frac{\\nu_0 + N}{2}, \\frac{1}{2} \\nu_0 s_0^2 + \\frac{1}{2}(\\mathbf{y}'\\mathbf{y} + \\boldsymbol{\\beta}_0'\\Lambda_0 \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_N' \\Lambda_N \\boldsymbol{\\beta}_N)).\\)\n\n\nWe could again program this model using the analytic posterior. Instead, we’ll program it only through the priors and likelihood and let Stan approximate the posterior. I will also allow the model to include more than one predictor so that \\(\\mathbf{X}\\) is a \\(N \\times (K+1)\\) matrix augmented with a column of ones. (Download the file here)\n\n\nconjugate-regression.stan\n\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; K;\n  vector[N] y;\n  matrix[N, K] X;\n  \n  // hyperparameters\n  real beta_0;\n  real&lt;lower=0&gt; lambda_0;\n  real&lt;lower=0&gt; nu_0;\n  real&lt;lower=0&gt; s_02;\n}\n\ntransformed data {\n    matrix[N, K+1] X_mat = append_col(rep_vector(1, N), X);\n    vector[K+1] beta_0_vec = rep_vector(beta_0, K+1);\n    matrix[K+1, K+1] Lambda_0 = lambda_0 * identity_matrix(K+1);\n}\n\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  vector[K+1] beta;\n  real&lt;lower=0&gt; sigma2;\n}\n\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  beta ~ multi_normal(beta_0_vec, sigma2 * Lambda_0);\n  sigma2 ~ scaled_inv_chi_square(nu_0, s_02);\n  \n  y ~ normal(X_mat * beta, sqrt(sigma2));\n}\n\ngenerated quantities {\n    real sigma = sqrt(sigma2);\n    vector[N] y_ppd;\n    \n    for (i in 1:N) {\n        y_ppd[i] = normal_rng(X_mat[i,] * beta, sqrt(sigma2));\n    }\n}\n\n\n\n\nCode\n\nintro-to-stan.R\n\ndata_list2 &lt;- list(\n    N = nrow(ncaaw),\n    K = 1,\n    y = ncaaw$W,\n    X = as.matrix(ncaaw$FG3pct, nrow = nrow(ncaaw)),\n    \n    # hyperparameters\n    beta_0 = 0,\n    lambda_0 = 0.5,\n    nu_0 = 1,\n    s_02 = 47\n)\n\nfile2 &lt;- file.path(\"conjugate-regression.stan\")\nconj_model &lt;- cmdstan_model(file2)\n\nfit2 &lt;- conj_model$sample(\n    data = data_list2,\n    iter_warmup = 1000,\n    iter_sampling = 1000,\n    thin = 1,\n    refresh = 500,\n    chains = 2,\n    show_messages = TRUE,\n    show_exceptions = FALSE\n)\n\n\n\nRunning MCMC with 2 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.5 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 1.2 seconds.\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nfit2$diagnostic_summary()\n\n\n\n$num_divergent\n[1] 0 0\n\n$num_max_treedepth\n[1] 0 0\n\n$ebfmi\n[1] 0.9378670 0.9963986\n\n\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nmcmc_trace(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\nmcmc_dens_overlay(fit2$draws(variables = c(\"beta\", \"sigma\")))\n\n\n\n\n\n\n\n\n\n\n(a) Traceplots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nFigure 6: Diagnostic plots for the posterior samples. Plots were made using the bayesplot package.\n\n\n\nAgain the MCMC chains and diagnostics look satisfactory. The summary statistics for the parameters are displayed in Table 3.\n\n\n\nCode\n\nintro-to-stan.R\n\nfit2$summary(variables = c(\"beta\", \"sigma\")) |&gt; \n    kableExtra::kbl(booktabs = TRUE, format = \"html\")\n\n\n\n\n\nTable 3: Summary statistics for the posterior samples for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nbeta[1]\n-10.1791619\n-10.239600\n2.5123209\n2.6138979\n-14.3625850\n-6.2677275\n1.001439\n521.5164\n574.8934\n\n\nbeta[2]\n0.8572707\n0.859365\n0.0810363\n0.0845927\n0.7317652\n0.9932626\n1.001122\n537.6317\n556.3745\n\n\nsigma\n5.9874354\n5.976395\n0.2422073\n0.2370974\n5.6090195\n6.4224640\n1.003337\n627.6261\n577.8970\n\n\n\n\n\n\n\n\n\n\nAnd here’s a quick reminder of our results for the non-informative prior and MLE fits in Table 4.\n\n\n\nCode\n\nintro-to-stan.R\n\nmcmc_summary &lt;- cbind(mles,\n  fit1$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\nmcmc_summary &lt;- cbind(mcmc_summary,\n  fit2$summary(variables = c(\"beta\", \"sigma\"))[,c(\"mean\", \"sd\")])\ncolnames(mcmc_summary) &lt;- c(\"Variable\", \"MLE\", \"Non-info Est\", \"Non-info SD\", \"Conj Est\", \"Conj SD\")\n\nmcmc_summary |&gt;\n    kableExtra::kbl(booktabs = TRUE, format = \"html\", digits = 3)\n\n\n\n\n\nTable 4: Comparison of the estimates for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\nVariable\nMLE\nNon-info Est\nNon-info SD\nConj Est\nConj SD\n\n\n\n\n(Intercept)\nbeta_0\n-14.945\n-15.170\n2.726\n-10.179\n2.512\n\n\nFG3pct\nbeta_1\n1.009\n1.017\n0.088\n0.857\n0.081\n\n\n\nsigma\n5.908\n5.928\n0.219\n5.987\n0.242\n\n\n\n\n\n\n\n\n\n\nIn Figure 7 (a) we have the 50% and 95% CIs with the posterior mean displayed as a point. The densities are plotted in ridgelines in Figure 7 (b).\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nmcmc_intervals(fit2$draws(variables = c(\"beta\", \"sigma\")))\nmcmc_areas_ridges(fit2$draws(variables = c(\"beta\", \"sigma\")),\n                  prob_outer = 0.95, prob = 0.5)\n\n\n\n\n\n\n\n\n\n\n(a) Interval plots for \\(\\beta\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Approximate posterior densities for \\(\\beta\\) and \\(\\sigma\\) in a ridgeline plot.\n\n\n\n\n\n\n\nFigure 7: Plots for the 50% Credible Interval (inner band) and 95% Credible Interval (outer band) for \\(\\beta\\) and \\(\\sigma\\). Plots were made using the bayesplot package.\n\n\n\nWe collect the PPD draws from the fit object using the draws method. From Figure 8 we can see that while the predictive densities match pretty well and the intervals are centered on the OLS line of best fit.\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\ny_ppd &lt;- as.matrix(as_draws_df(fit2$draws(variables = \"y_ppd\")))\n\nppc_dens_overlay(ncaaw$W,\n                 y_ppd[1:50, 1:350]) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins\",\n         x = \"Wins\")\n\nppc_intervals(ncaaw$W,\n                 y_ppd[1:50, 1:350],\n                 x = ncaaw$FG3pct) +\n    labs(title = \"Density of PPD Draws of NCAAW Wins by 3pt%\",\n         x = \"3pt%\",\n         y = \"Wins\")\n\n\n\n\n\n\n\n\n\n\n(a) PPD densities for the wins given 3pt%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PPD intervals for the wins plotted by 3pt%.\n\n\n\n\n\n\n\nFigure 8: Posterior Predictive Check plots from bayesplot."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "href": "posts/intro-to-stan/intro_to_stan.html#some-other-useful-resources-for-stan",
    "title": "Getting to know Stan - SWOSC",
    "section": "Some Other Useful Resources for Stan",
    "text": "Some Other Useful Resources for Stan\nFirst, here’s the three essential guides for using Stan:\n\nStan Function Guide - reference for all the built-in functions and distributions as well as guides for writing custom functions and distributions\nStan User’s Guide - reference for example models, how to build efficient models, and some inference techniques\nStan Reference Manual - reference for programming in Stan with a focus on how the language works\n\nHere are some other useful packages to use for Bayesian data analysis with Stan (or other packages). We used some of these in this tutorial!\n\nbrms: Bayesian regression models using Stan\nposterior: Useful for working with Stan output\nbayesplot: ggplot2-based plotting functions for MCMC draws designed work well with Stan\nloo: Leave-one-out cross validation for model checking and selection that works with the log-posterior. Works best with rstanarm but can work with cmdstanr too.\n\nHere’s a list of useful resources for debugging issues with divergences, hitting maximum tree-depth, low EBFMI, and understanding diagnostics:\n\nStan’s Guide to Runtime warnings and convergence problems\nPrior Choices and Selection\nConvergence Diagnostics for MCMC\nOfficial Stan Forum"
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#bonus-regression-modeling-with-incomplete-data",
    "href": "posts/intro-to-stan/intro_to_stan.html#bonus-regression-modeling-with-incomplete-data",
    "title": "Getting to know Stan - SWOSC",
    "section": "Bonus: Regression Modeling with Incomplete Data",
    "text": "Bonus: Regression Modeling with Incomplete Data\nAs a bonus section, we’ll use the brms package to fit a regression model where we have incomplete predictor observations. Incomplete data analysis ranges from complete case analysis (incomplete cases are dropped) and mean imputation to multiple imputation, joint modeling, and EM algorithm (Schafer and Graham 2002).2 We’re going to use mice (Buuren and Groothuis-Oudshoorn 2010) and brms (Bürkner 2018) to demonstrate the imputation and fitting Bayesian regression models with a convenient front-end that writes the Stan code for us.\nIn our case, we are going to use junior year scoring (points per game) to predict senior year scoring for women’s college basketball players from 2020-21 to the 2022-23 seasons. The data set only contains players who played in at least 75% of games each season, so partial seasons due to injury or being a bench player are excluded. Players who only have a junior season are excluded from the analysis.\n\n\n\nCode\n\nintro-to-stan.R\n\nncaaw_i &lt;- read.csv(\"Data/ncaaw-individuals.csv\", header = TRUE)\nhead(ncaaw_i)\n\n\n\n             Name Pos_jr Pos_sr G_jr G_sr PPG_jr PPG_sr Cl_jr\n1     A'Jah Davis      F      F   29   32   16.6   16.2   Jr.\n2 Abby Brockmeyer      F      F   NA   31     NA   16.3   Jr.\n3       Abby Feit      F      F   29   28   15.1   15.5   Jr.\n4     Abby Meyers      G      G   NA   30     NA   17.9   Jr.\n5     Abby Meyers      G      G   NA   35     NA   14.3   Jr.\n6   Adriana Shipp      G      G   NA   30     NA   13.9   Jr.\n\n\nOur imputation model will be univariate linear regression that use all other variables as predictors. For example, imputing \\(PPG_{jr}\\) will be done by regressing on \\(PPG_{sr}, G_{jr}, G_{sr}\\). \\(PPG_{jr}\\) and \\(G_{jr}\\) are incomplete for \\(n_{mis} = 176\\) players while \\(n_{obs} = 98\\) players have stats from both years. This missing data pattern is displayed in Figure 9.\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(mice)\n\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nCode\nm_pat &lt;- md.pattern(ncaaw_i, plot = TRUE)\n\n\n\n\n\n\n\n\nFigure 9: Missing data patterns for the NCAA women’s basketball players from 2020-2023 who played in their junior and senior year. The red boxes correspond to missing values, so there are 176 players who recorded full senior seasons (played in &gt;75% of total games) but missing or shortened junior seasons.\n\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nggplot(ncaaw_i, aes(PPG_jr, PPG_sr, color = G_jr)) +\n    geom_point() +\n    scale_color_viridis_c(name = \"G - Jr\") +\n    labs(x = \"PPG - Jr\",\n         y = \"PPG - Sr\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nFigure 10: Points per game (PPG) from Junior and Senior seasons.\n\n\n\n\n\n\nMultiple Imputation with mice\nFirst, we’ll try out imputing before model fitting using mice. MICE stands for Multiple Imputation by Chained Equations and is procedure that creates a set of \\(M\\) completed data sets from an incomplete data set. Multiple Imputation is a three stage procedure:\n\nEach incomplete variable is imputed with posterior predictive draws from a regression model with all other variables as predictors. The procedure iterates through the incomplete variables several times to converge to the posterior predictive distribution of the missing data given the observed.\nThese completed data sets are then analyzed individually with a standard complete data method.\nResults from each analysis are combined. Typically this is done with Rubin’s rules (Rubin 1987), but brms follows the advice of Zhou and Reiter (2010) and simply stacks the posterior draw matrices from each fitted model.\n\n\n\n\nCode\n\nintro-to-stan.R\n\nlibrary(brms)\n\nimps &lt;- mice(ncaaw_i, m = 10, method = \"norm\", maxit = 10, printFlag = FALSE)\n\nfit_brm_mice &lt;- brm_multiple(PPG_sr ~ G_jr * PPG_jr, data = imps, chains = 2,\n                             refresh = 0)\nsummary(fit_brm_mice)\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: PPG_sr ~ G_jr * PPG_jr \n   Data: imps (Number of observations: 274) \n  Draws: 20 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 20000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      18.23      6.45     5.81    31.16 1.49       37      106\nG_jr           -0.26      0.25    -0.77     0.23 1.51       36       94\nPPG_jr         -0.12      0.39    -0.89     0.62 1.55       35      110\nG_jr:PPG_jr     0.02      0.02    -0.01     0.05 1.57       34       94\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.21      0.10     2.01     2.42 1.11      118      372\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nImputation During Model Fitting\nImputation during model fitting takes a different approach. Imputations are made for each incomplete variable using a different conditional model for each variable. This approach differs from MI and MICE in two key ways: (i) the model is only fit once since the imputation model is part of the analysis model, (ii) the model must be constructed uniquely for each analysis scenario whereas MI completed data sets can be re-used with different analyses.\n\n\n\nCode\n\nintro-to-stan.R\n\nbform &lt;- bf(PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr)) +\n    bf(PPG_jr | mi() ~ G_sr + PPG_sr) +\n    bf(G_jr | mi() ~ G_sr + PPG_sr) + set_rescor(FALSE)\n\nfit_brm_mi &lt;- brm(bform, data = ncaaw_i, \n                  refresh = 500, iter = 2000, thin = 1,\n                  backend = \"cmdstanr\", \n                  control = list(adapt_delta = 0.8, \n                                 max_depth = 10,\n                                show_exceptions = FALSE),\n                  chains = 2,\n                  cores = 2)\n\nsummary(fit_brm_mi)\n\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: PPG_sr | mi() ~ mi(G_jr) * mi(PPG_jr) \n         PPG_jr | mi() ~ G_sr + PPG_sr \n         G_jr | mi() ~ G_sr + PPG_sr \n   Data: ncaaw_i (Number of observations: 274) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nPPGsr_Intercept          16.10      2.29    11.60    20.74 1.00      615\nPPGjr_Intercept           5.95      2.33     1.14    10.46 1.00      900\nGjr_Intercept             2.98      6.31    -9.27    15.38 1.00      789\nPPGjr_G_sr               -0.04      0.06    -0.17     0.09 1.00     1053\nPPGjr_PPG_sr              0.72      0.08     0.56     0.89 1.00      732\nGjr_G_sr                  0.56      0.18     0.19     0.90 1.00      807\nGjr_PPG_sr                0.28      0.23    -0.16     0.72 1.00      622\nPPGsr_miG_jr             -0.30      0.09    -0.48    -0.12 1.00      565\nPPGsr_miPPG_jr           -0.04      0.15    -0.32     0.26 1.00      591\nPPGsr_miG_jr:miPPG_jr     0.02      0.01     0.01     0.03 1.00      544\n                      Tail_ESS\nPPGsr_Intercept            631\nPPGjr_Intercept           1221\nGjr_Intercept             1290\nPPGjr_G_sr                1244\nPPGjr_PPG_sr              1345\nGjr_G_sr                  1182\nGjr_PPG_sr                 874\nPPGsr_miG_jr               589\nPPGsr_miPPG_jr             736\nPPGsr_miG_jr:miPPG_jr      605\n\nFamily Specific Parameters: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_PPGsr     1.86      0.09     1.70     2.04 1.00     1706     1355\nsigma_PPGjr     2.31      0.15     2.04     2.63 1.00      634     1066\nsigma_Gjr       5.33      0.38     4.66     6.15 1.00      650     1214\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nbrms is built on Stan, so we can also take a look at the traceplots of the samples in Figure 11.\n\n\n\nCode\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGsr\", \"bsp_\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 11: Traceplots of brms analysis model parameters.\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nplot(fit_brm_mi, variable = c(\"b_PPGjr\", \"b_Gjr\"), regex = TRUE, ask = FALSE, N = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(PPG_{jr}\\) imputation model parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(G_{jr}\\) imputation model parameters\n\n\n\n\n\n\n\nFigure 12: Traceplots of brms imputation model parameters.\n\n\n\n\n\n\n\nCode\n\nintro-to-stan.R\n\nplot(brms::conditional_effects(fit_brm_mice, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\nplot(brms::conditional_effects(fit_brm_mi, \"PPG_jr:G_jr\", resp = \"PPGsr\"))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Estimates after MICE imputation\n\n\n\n\n\n\n\n\n\n\n\n(b) Estimates with joint model\n\n\n\n\n\n\nFigure 13: The estimated conditional effects of PPG as a junior and junior-year Games played on PPG as a senior."
  },
  {
    "objectID": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "href": "posts/intro-to-stan/intro_to_stan.html#footnotes",
    "title": "Getting to know Stan - SWOSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee wikipedia for more details and derivations: https://en.wikipedia.org/wiki/Bayesian_linear_regression↩︎\nSee White, Royston, and Wood (2011) for more details on incomplete data analysis.↩︎"
  },
  {
    "objectID": "posts/imp-data-viz/imp-data-viz.html",
    "href": "posts/imp-data-viz/imp-data-viz.html",
    "title": "Data Visualizations and Diagnostics for Imputed Directional Data",
    "section": "",
    "text": "This post is a follow-up to my first post on ggplot2-based data visualization for angular data. In that post, I shared some basic visualizations for angular data that I used in my dissertation and other research projects. Here, I’ll share some visualizations for imputed angular data that combine ideas from the previous post with some of the standard diagnostics from mice.\nLoad Packages\nlibrary(dplyr, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggplot2, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggthemes, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggforce, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(circular, warn.conflicts = FALSE, quietly = TRUE)\n# devtools::install_github(repo = \"marcpabst/ggcircular\")\nlibrary(ggcircular, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(latex2exp, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(imputeangles, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(pnregstan, warn.conflicts = FALSE, quietly = TRUE)\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "posts/imp-data-viz/imp-data-viz.html#simulated-data",
    "href": "posts/imp-data-viz/imp-data-viz.html#simulated-data",
    "title": "Data Visualizations and Diagnostics for Imputed Directional Data",
    "section": "Simulated Data",
    "text": "Simulated Data\nI’ll start by generating data from a projected normal regression model. The data are displayed in Table 1 and Figure 1.\n\n\nShow code\nset.seed(10239)\nN &lt;- 50\nbeta &lt;- c(0, 2.5, 0, 2.5)\nB &lt;- matrix(beta, ncol = 2)\nsigma_mat &lt;- c(1,0,0,1)\nX &lt;- matrix(seq(from = -2, to = 2, length.out = N))\n\nsim_data &lt;- pnregstan::pnreg_sim_data(N = N, \n                         B = beta, \n                         Sigma_theta = sigma_mat,\n                         X = X) |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; 0 ~ theta + 2*pi,\n            TRUE ~ theta\n        )\n    )\n\n\nTo create a data set to demonstrate the imputation figures, I’ll mask some of the angular observations in the simulated data by simulating a missingness indicator \\(R\\) such that the angle \\(\\theta\\) is MCAR for 35% of the observations. The incomplete data are displayed in Table 2 and Figure 2.\n\n\nShow code\n# Simulate a MCAR response indicator\nsim_data$R &lt;- sample(c(1, 0), size = N,\n                     prob = c(0.65, 0.35),\n                     replace = TRUE)\n\nmis_ind &lt;- which(sim_data$R == 0)\n\ninc_data &lt;- sim_data |&gt;\n    mutate(\n        theta = case_when(\n            R == 1 ~ theta,\n            R == 0 ~ NA,\n            TRUE ~ NA\n        ),\n        U1 = case_when(is.na(theta) ~ NA, TRUE ~ U1),\n        U2 = case_when(is.na(theta) ~ NA, TRUE ~ U2)\n    ) |&gt; \n    select(theta, U1, U2, X)\n\n\n\nComplete DataIncomplete Data\n\n\n\n\nColor Scales Functions\nscale_color_angles &lt;- function(colors = list(low = \"magenta\", mid = \"dodgerblue\", high = \"magenta\"), color_name = \"Angle\", midpoint = 0, limits = c(-pi, pi)) {\n    scale_color_gradient2(low = colors$low, mid = colors$mid, high = colors$high,\n                          na.value = \"gray70\", name = color_name,\n                          midpoint = midpoint, limits = limits)\n}\n\nscale_fill_angles &lt;- function(colors = list(low = \"magenta\", mid = \"dodgerblue\", high = \"magenta\"), fill_name = \"Angle\", midpoint = 0, limits = c(-pi, pi)) {\n    scale_fill_gradient2(low = colors$low, mid = colors$mid, high = colors$high,\n                          na.value = \"gray70\", name = fill_name,\n                          midpoint = midpoint, limits = limits)\n}\n\n\n\n\nShow code\nsim_data_2 &lt;- sim_data |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; pi ~ theta + 2*pi,\n            theta &gt;= pi ~ theta - 2*pi,\n            TRUE ~ theta\n        ),\n        original = FALSE\n    )\nsim_data$original = TRUE\n\nggplot(bind_rows(sim_data, sim_data_2), \n       aes(x = X, y = theta, shape = original, color = theta)) +\n    geom_hline(yintercept = c(0, 2*pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"gray50\") +\n    geom_point() +\n    scale_shape_manual(name = \"Original\", values = c(1, 16)) +\n    scale_color_angles(limits = c(0, 2*pi), midpoint = pi) +\n    labs(y = TeX(\"$\\\\theta$\")) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 1: A wrapped circular-linear scatterplot.\n\n\n\n\n\n\n\nShow code\nsim_data\n\n\n\n\nTable 1: Simulated complete data\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nShow code\ninc_data_plt &lt;- inc_data\ninc_data_plt$true_theta &lt;- sim_data$theta\n\ninc_data_2 &lt;- inc_data_plt |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; pi ~ theta + 2*pi,\n            theta &gt;= pi ~ theta - 2*pi,\n            TRUE ~ theta\n        ),\n        true_theta = case_when(\n            true_theta &lt; pi ~ true_theta + 2*pi,\n            true_theta &gt;= pi ~ true_theta - 2*pi,\n            TRUE ~ true_theta\n        ),\n        original = FALSE\n    )\ninc_data_plt$original = TRUE\n\ninc_data_dbl &lt;- bind_rows(inc_data_plt, inc_data_2)\n\nggplot(inc_data_dbl, \n       aes(x = X, y = theta, shape = original, color = theta)) +\n    annotate(\"rect\",\n        xmin = inc_data[mis_ind, \"X\"] - 0.025, \n        xmax = inc_data[mis_ind, \"X\"] + 0.025,\n        ymin = -Inf, ymax = Inf,\n        fill = \"black\", \n        alpha = 0.15) +\n    geom_hline(yintercept = c(0, 2*pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"gray50\") +\n    geom_point() +\n    geom_point(data = inc_data_dbl[which(is.na(inc_data_dbl$theta) & inc_data_dbl$original),], \n        aes(x = X, y = true_theta), shape = 4, color = \"red\") +\n    scale_shape_manual(name = \"Original\", values = c(1, 16)) +\n    scale_color_angles(limits = c(0, 2*pi), midpoint = pi) +\n    labs(y = TeX(\"$\\\\theta$\")) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 2: A wrapped circular-linear scatterplot of the incomplete data. Incomplete observations have the true masked value marked with a red “x” and are highlighted with a gray vertical band.\n\n\n\n\n\n\n\nShow code\ninc_data\n\n\n\n\nTable 2: Simulated incomplete data"
  },
  {
    "objectID": "posts/imp-data-viz/imp-data-viz.html#impute-the-missing-angular-data",
    "href": "posts/imp-data-viz/imp-data-viz.html#impute-the-missing-angular-data",
    "title": "Data Visualizations and Diagnostics for Imputed Directional Data",
    "section": "Impute the Missing Angular Data",
    "text": "Impute the Missing Angular Data\nNext, the data needs to be passed to the mice() function with some customized arguments (and with the imputeangles and pnregstan packages loaded).\n\nA generic mids object is created by setting the max. iterations to 0. We use this obejct to get placeholders for the methods vector and the predictor indicator matrix.\nThe predictor matrix is modified to turn off the unit vector components from being used to impute the missing angular observations.\nThe methods vector is modified to impute the unit vector components using functions of the imputed angle.\n\n\n\nShow code\nlibrary(mice)\n\nM &lt;- 5\n\nimp0 &lt;- mice(inc_data, maxit = 0, method = \"pnregid\")\npred_mat &lt;- imp0$predictorMatrix\nmthds &lt;- imp0$method\npred_mat[\"theta\",c(\"U1\", \"U2\")] &lt;- 0\nmthds[\"U1\"] &lt;- \"~cos(theta)\"\nmthds[\"U2\"] &lt;- \"~sin(theta)\"\n\n\nThen we run mice() with these modified arguments.\n\n\nShow code\nimps &lt;- mice(inc_data, method = mthds, predictorMatrix = pred_mat,\n             m = M, maxit = 10, printFlag = FALSE)"
  },
  {
    "objectID": "posts/imp-data-viz/imp-data-viz.html#imputation-figures",
    "href": "posts/imp-data-viz/imp-data-viz.html#imputation-figures",
    "title": "Data Visualizations and Diagnostics for Imputed Directional Data",
    "section": "Imputation Figures",
    "text": "Imputation Figures\nFinally, we can look at the imputation diagnostic plots. They allow us to check how much variation there is in the imputations for each observation and how they compare to the observed parts of the data set.\n\nBy ObservationOverlaidBuilt-in MICE Stripplot\n\n\n\n\nShow code\nimputeangles::plot_angular_imputations(imps, \n    r = 0.75, by_id = TRUE)\n\n\n\n\n\n\n\n\nFigure 3: Visualize the imputations for each incomplete angular observation. This is a function from imputeangles.\n\n\n\n\n\n\n\n\n\nShow code\nimputeangles::plot_angular_imputations(imps, \n    r = 0.45, by_id = FALSE, overlay = TRUE)\n\n\nWarning: Removed 14 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 4: Use dotplots to see how the distributions of each imputed data set compare to the observed. This is a function from imputeangles.\n\n\n\n\n\n\n\n\n\nShow code\nstripplot(imps, theta ~ X | .imp)\n\n\n\n\n\n\n\n\nFigure 5: Built-in Stripplot from MICE.\n\n\n\n\n\n\n\n\nAdditional diagnostic plots can be created after converting the imputations from mids format to a long imputation data frame format.\n\n\nShow code\nlong_imp &lt;- complete(imps, \"long\", include = TRUE) |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; 0 ~ theta + 2*pi,\n            TRUE ~ theta\n        )\n    )\n\nlong_imp\n\n\n\n  \n\n\n\n\nDensity PlotWrapped Scatterplot\n\n\n\n\nShow code\nlong_imp  |&gt;\n    mutate(\n        imp_color = case_when(\n            .imp == 0 ~ \"blue\",\n            .imp &gt; 0 ~ \"magenta\",\n            TRUE ~ \"black\"\n        )\n    ) |&gt;\n    tidyr::pivot_longer(\n        theta:U2,\n        names_to = \"variable\",\n        values_to = \"value\"\n    ) |&gt;\n    ggplot(aes(value, color = imp_color, group = .imp)) +\n    geom_density() +\n    scale_color_manual(values = c(\"#006CC2B3\", \"#B61A51B3\"), \n                       labels = c(\"Observed\", \"Completed\")) +\n    facet_wrap(.~variable, nrow = 1,\n               scales = \"free\") +\n    labs(x = \"\") +\n    theme(panel.background = element_rect(fill = \"white\", colour = \"black\"),\n           panel.grid = element_blank(),\n           axis.text.y = element_blank(),\n           axis.ticks.length.y = unit(0, \"cm\"),\n          legend.position = \"none\")\n\n\nWarning: Removed 42 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nFigure 6: Diagnostic plots after imputation with MICE. Density plots of the imputed data in pink and observed data in blue.\n\n\n\n\n\n\n\n\n\nShow code\nlong_imp &lt;- long_imp |&gt; \n    mutate(\n        missing_imp = .id %in% mis_ind & .imp &gt;= 1,\n        obs_noimp = !(.id %in% mis_ind) & .imp == 0,\n        original = TRUE\n    ) |&gt;\n    filter(\n        missing_imp | obs_noimp\n    )\n\nlong_imp2 &lt;- long_imp |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; pi ~ theta + 2*pi,\n            theta &gt;= pi ~ theta - 2*pi,\n            TRUE ~ theta\n        ),\n        original = FALSE\n    )\n\n\nggplot(rbind(long_imp, long_imp2), aes(X, theta, color = as.factor(.imp), shape = original)) +\n    geom_hline(yintercept = c(0, 2*pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"gray50\") +\n    geom_point() +\n    scale_shape_manual(name = \"Original\", values = c(1, 16)) +\n    annotate(\"rect\",\n                xmin = inc_data[mis_ind, \"X\"] - 0.025, \n                xmax = inc_data[mis_ind, \"X\"] + 0.025,\n                ymin = -Inf, ymax = Inf,\n                fill = \"black\", alpha = 0.15) +\n        labs(x = \"t\", y = TeX(\"$\\\\theta$\")) +\n    scale_color_manual(name = \"Imp\",\n            values = c(\"black\", \"magenta\", \"tomato\", \"orange\", \"purple\",\n                        \"goldenrod\", \"dodgerblue\", \"forestgreen\", \"green\"),\n            labels = c(\"Observed\", paste0(\"Imp \", 1:M))) +\n    scale_y_continuous(breaks  = c(seq(-pi, pi, pi)), \n            labels = c(TeX(\"$-\\\\pi$\"), \"0\", TeX(\"$\\\\pi$\"))) +\n    theme(panel.background = element_rect(fill = \"white\", colour = \"black\"),\n           panel.grid = element_blank(),\n           axis.text.y = element_blank(),\n           axis.ticks.length.y = unit(0, \"cm\"),\n          legend.position = \"right\")\n\n\n\n\n\n\n\n\nFigure 7: A zoomed in look at the imputed time series for times 150 to 200. Red rectangles correspond to missing observations.\n\n\n\n\n\n\n\n\n\nImputation Convergence Figures\nSpaghetti plots and traceplots with angular data are very useful for assessing convergence of the imputations, but more complex to create. These figures use line plots color coded by imputation with the cycles along the x-axis.\nTraceplots are built-in with mice for conventional inline data like \\(X\\) in our simulation. But the traceplots in Figure 8 display the mean and standard deviation of the imputations at each iteration, and these statistics are not valid for angular data.\n\n\nShow code\nplot(imps)\n\n\n\n\n\n\n\n\nFigure 8: The traceplot built-in to mice. This figure does not calculate appropriate summary statistics for the angular variable theta.\n\n\n\n\n\nInstead we should be calculating the circular mean, circular standard deviation, and circular auto-correlation function. To do so, we need to modify the mice imputation procedure to save the imputations at each cycle in order to calculate our own summary statistics. The corrected traceplot is displayed in Figure 9.\nLastly, we can also check spaghetti plots with the cycles along the x-axis as in Figure 10. This lets us check to see if the individual imputations are stationary. There is not a built-in version with mice.\n\n\nExtending MICE\nWhen mice() is run and a mids object is created, the imputed values, original data, summary statistics for each iteration’s imputations, and meta data are stored. The imputations that occur at each iteration of the algorithm are discarded. In order to save each iterations set of imputations, we need to modify how the imputation procedure is run. The mice.mids() function makes this possible as it creates new imputations starting from where an existing mids object left off. The mids object can be extended either with more imputations M or by increasing the number of iterations maxit.\n\n\nExtending MICE\nM &lt;- 5\nmaxit &lt;- 25\n\nimp_m &lt;- lapply(1:M, function(m) {\n    print(paste0(\"Imputation \", m, \"; iteration \", 1))\n    imp_m &lt;- mice(inc_data, m = 1, maxit = 1, \n            method = mthds, \n            predictorMatrix = pred_mat, \n            printFlag = FALSE,\n            .imp = m, .it = 1)\n    c_imp_i &lt;- complete(imp_m, \"all\", include = FALSE)\n    c_imp_i &lt;- c_imp_i[[1]]\n    c_imp_i$it &lt;- 1\n    c_imp_i$m &lt;- m\n    c_imp_i$.id &lt;- 1:nrow(c_imp_i)\n    readr::write_csv(c_imp_i, paste0(root_dir, \"imp-\", m, \"-it-\", 1, \"-imp-df.csv\"))\n    for (it in 2:maxit) {\n        print(paste0(\"Imputation \", m, \"; iteration \", it))\n        imp_m &lt;- mice::mice.mids(imp_m, maxit = 1, printFlag = FALSE, .it = it, .imp = m)\n        c_imp_i &lt;- complete(imp_m, \"all\", include = FALSE)\n        c_imp_i &lt;- c_imp_i[[1]]\n        c_imp_i$it &lt;- it\n        c_imp_i$m &lt;- m\n        c_imp_i$.id &lt;- 1:nrow(c_imp_i)\n        readr::write_csv(c_imp_i, paste0(root_dir, \"imp-\", m, \"-it-\", it, \"-imp-df.csv\"))\n    }\n    return(imp_m)\n})\n\nimp &lt;- imp_m[[1]]\nfor (m in 2:M) {\n    imp &lt;- mice::ibind(imp, imp_m[[m]])\n}\n\nsaveRDS(imp, paste0(root_dir, \"imps2.rds\"))\n\n\n\n\nAuto-Correlation Functions\nc_auto_cor &lt;- function(theta, k = 1) {\n    \n    sum_determinants &lt;- function(X1, X2) {\n        s &lt;- lapply(1:(N-k), function(t) {\n                return(det(t(X1[t,]) %*% X2[t,]))\n            }) |&gt;\n            unlist() |&gt;\n            sum()\n        return(s)\n    }\n    \n    N &lt;- length(theta)\n    U &lt;- cbind(cos(theta), sin(theta))\n    \n    U_lag_k &lt;- U[1:(N - k),]\n    U_no_lag &lt;- U[(k+1):N,]\n    \n    num &lt;- sum_determinants(U_lag_k, U_no_lag)\n    denom &lt;- sum_determinants(U_lag_k, U_lag_k) * sum_determinants(U_no_lag, U_no_lag)\n    return(num / sqrt(denom))\n}\nmy_acf &lt;- function(x) {\n    acf(x, lag.max = 1, plot = FALSE)$acf[2,,1]\n}\n\n\n\n\nPost-processing imputations\nlonger_imp &lt;- lapply(1:M, function(m) {\n    tmp &lt;- lapply(1:maxit, function(it) {\n        readr::read_csv(paste0(root_dir, \"imp-\", m, \"-it-\", it, \"-imp-df.csv\"),\n                        show_col_types = FALSE, progress = FALSE)\n    }) |&gt;\n        dplyr::bind_rows()\n}) |&gt;\n    dplyr::bind_rows()\n\nauto_cor_mat &lt;- longer_imp |&gt;\n    group_by(m, it) |&gt;\n    summarise(\n        theta_mean = atan2(mean(sin(theta)), mean(cos(theta))),\n        theta_sd = sqrt(1 - sqrt(mean(cos(theta))^2 + mean(sin(theta))^2)),\n        u1_mean = mean(U1),\n        u1_sd = sd(U1),\n        u2_mean = mean(U2),\n        u2_sd = sd(U2),\n        theta_ac = c_auto_cor(theta, k = 1),\n        u1_ac = my_acf(U1),\n        u2_ac = my_acf(U2)\n    )\n\n\n`summarise()` has grouped output by 'm'. You can override using the `.groups`\nargument.\n\n\n\n\nShow code\nauto_cor_mat |&gt;\n    tidyr::pivot_longer(\n        !c(m, it),\n        names_to = c(\"parameter\", \"stat\"),\n        names_pattern = \"(.*)_(.*)\",\n        values_to = \"value\"\n    ) |&gt;\n    mutate(\n        parameter2 = case_when(\n            parameter == \"u1\" ~ \"cos(theta)\",\n            parameter == \"u2\" ~ \"sin(theta)\",\n            TRUE ~ parameter\n        ), \n        stat2 = case_when(\n            stat == \"ac\" ~ \"acf(1)\",\n            TRUE ~ stringr::str_to_title(stat)\n        )\n    ) |&gt;\n    ggplot(aes(it, value, color = as.factor(m))) +\n    geom_line(alpha = 0.85) +\n    scale_color_colorblind(name = \"Imp\") +\n    facet_grid(parameter2~stat2, labeller = label_parsed, scales = \"free\") +\n    theme(legend.position = \"none\") +\n    labs(x = \"MICE Cycle\",\n         y = \"Value\")  +\n    theme(panel.background = element_rect(fill = \"white\", colour = \"black\"),\n           panel.grid = element_blank(),\n           axis.text.y = element_blank(),\n           axis.ticks.length.y = unit(0, \"cm\"),\n          legend.position = \"right\")\n\n\n\n\n\n\n\n\nFigure 9: Traceplots of the imputed values means and standard deviations.\n\n\n\n\n\n\n\nShow code\nhghlt &lt;- sample(mis_ind, size = 5, replace = FALSE)\nlonger_imp |&gt;\n    mutate(\n        imped = case_when(\n            .id %in% mis_ind ~ \"Imp\",\n            TRUE ~ \"Obs\"\n        ),\n        highlight = case_when(\n            .id %in% hghlt ~ \"Select\",\n            TRUE ~ \"Nope\"\n        )\n    ) |&gt;\n    filter(\n        imped == \"Imp\"\n    ) |&gt;\n    ggplot(aes(it, theta, group = .id, color = highlight, alpha = highlight)) +\n    geom_line() +\n    scale_color_manual(values = c(\"black\", \"red\")) +\n    scale_alpha_manual(values = c(0.25, 0.5)) +\n    facet_wrap(.~m, nrow = 2) +\n    theme(legend.position = \"none\")  +\n    theme(panel.background = element_rect(fill = \"white\", colour = \"black\"),\n           panel.grid = element_blank(),\n           axis.text.y = element_blank(),\n           axis.ticks.length.y = unit(0, \"cm\"),\n          legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 10: Imputation spaghetti plot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben Stockton's Stats Site",
    "section": "",
    "text": "I am a postdoctoral fellow in the Division of Biostatistics at the NYU Grossman School of Medicine. My research interests include missing data, public health and public health policy, trial design, air pollution, directional statistics, spatial statistics, and causal inference."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ben Stockton's Stats Site",
    "section": "",
    "text": "I am a postdoctoral fellow in the Division of Biostatistics at the NYU Grossman School of Medicine. My research interests include missing data, public health and public health policy, trial design, air pollution, directional statistics, spatial statistics, and causal inference."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ben Stockton's Stats Site",
    "section": "Education",
    "text": "Education\n\n(2019-2024) PhD in Statistics | University of Connecticut | Storrs, CT\n\nAdvised by Dr. Ofer Harel\n\n(2016-2019) BS in Mathematics (emp. Statistics) - Minor in Computer Science | University of Wisconsin-Whitewater | Whitewater, WI\nUndeclared (2015-2016) | University of Wisconsin-Madison | Madison, WI"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ben Stockton's Stats Site",
    "section": "Experience",
    "text": "Experience\n\n(2024-Present) Postdoctoral Fellow | Division of Biostatistics, Department of Population Health | New York University Grossman School of Medicine | New York, NY\n(2019-2024) Graduate Assistant | Department of Statistics | University of Connecticut | Storrs, CT\n(2020-2020) Data Science Leadership Development Program Intern | Travelers Insurance Co. | Hartford, CT"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a postdoctoral fellow in the Division of Biostatistics at the NYU Grossman School of Medicine. My research is in relation to public policy, trial design, missing data, Bayesian methods, and angular data.\nI completed my PhD in Statistics at the University of Connecticut in 2024 under the supervision of Prof. Ofer Harel. At UConn, I served as a Teaching Assistant and Research Assistant. I graduated from the University of Wisconsin-Whitewater with a BS in mathematics (emphasis in statistics) and minor in computer science in the spring of 2019. I am originally from Fort Atkinson, WI."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I am a postdoctoral fellow in the Division of Biostatistics at the NYU Grossman School of Medicine. My research is in relation to public policy, trial design, missing data, Bayesian methods, and angular data.\nI completed my PhD in Statistics at the University of Connecticut in 2024 under the supervision of Prof. Ofer Harel. At UConn, I served as a Teaching Assistant and Research Assistant. I graduated from the University of Wisconsin-Whitewater with a BS in mathematics (emphasis in statistics) and minor in computer science in the spring of 2019. I am originally from Fort Atkinson, WI."
  },
  {
    "objectID": "about.html#cv-other-files",
    "href": "about.html#cv-other-files",
    "title": "About",
    "section": "CV & Other Files",
    "text": "CV & Other Files\n\nCV\nList of Statistics Journals for the aims and scopes of Statistics and Statistics-adjacent journals organized by general topic"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\n\nIn my non-stats time, I enjoy cooking, running, reading, photography, and following Major League Baseball and the WNBA. I have run several 5ks, 10ks, half marathons, and the 2018 Madison Marathon."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Data Visualizations and Diagnostics for Imputed Directional Data\n\n\n\n\n\n\ndata-viz\n\n\nmissing-data\n\n\nangular-data\n\n\n\n\n\n\n\n\n\n2026-02-20\n\n\nBen\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Directional Data\n\n\n\n\n\n\ndata-viz\n\n\nangular-data\n\n\n\n\n\n\n\n\n\n2026-02-19\n\n\nBen\n\n\n\n\n\n\n\n\n\n\n\n\nMissingness Mechanisms\n\n\n\n\n\n\nmissing-data\n\n\nstatistics\n\n\nexample\n\n\n\n\n\n\n\n\n\n2025-02-20\n\n\nBen\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use imputeangles\n\n\n\n\n\n\nr-package\n\n\ntutorial\n\n\nbayesian-analysis\n\n\nmissing-data\n\n\nangular-data\n\n\n\n\n\n\n\n\n\n2024-02-21\n\n\nBen Stockton\n\n\n\n\n\n\n\n\n\n\n\n\nGetting to know Stan - SWOSC\n\n\n\n\n\n\nbayesian-analysis\n\n\ncomputation\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n2023-11-16\n\n\nBen Stockton\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n2023-09-15\n\n\nBen Stockton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html",
    "href": "posts/directions-data-viz/directions-data-viz.html",
    "title": "Visualizing Directional Data",
    "section": "",
    "text": "Visualizing directional or angular data has been a challenge while writing my dissertation and preparing presentations for the related research. There are a few tools out there for preparing directional data plots in R (see circular, mixtures of VMF movMF, etc.) but these largely don’t integrate well with modern tools like ggplot2 and the angular functionality in ggplot2 doesn’t always behave as desired for directional data. The exception to this being ggcircular, which unfortunately is rather limited and hasn’t been updated recently.1\nIn this post, I’ll share some of the basic directional data plots and tricks I’ve learned or used with ggplot2; some of these are just adaptations of existing methods, I am not claiming to be the original developer of any of these graphics. I will also point out where I wrote wrapper functions for a visualization in my pnregstan and imputeangles packages, but I will make all of the visualizations with the full ggplot2 code. In a follow-up post, I’ll share some visualizations and diagnostic figures for imputed angular data.\nShow code\nlibrary(dplyr, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggplot2, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggthemes, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(ggforce, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(circular, warn.conflicts = FALSE, quietly = TRUE)\n# devtools::install_github(repo = \"marcpabst/ggcircular\")\nlibrary(ggcircular, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(latex2exp, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(imputeangles, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(pnregstan, warn.conflicts = FALSE, quietly = TRUE)\n\ntheme_set(theme_classic())\nI’ll start by generating some data from a projected normal regression model.\nShow code\nN &lt;- 50\nbeta &lt;- c(0, 2.5, 0, 2.5)\nB &lt;- matrix(beta, ncol = 2)\nsigma_mat &lt;- c(1,0,0,1)\nX &lt;- matrix(seq(from = -2, to = 2, length.out = N))\n\nsim_data &lt;- pnregstan::pnreg_sim_data(N = N, \n                         B = beta, \n                         Sigma_theta = sigma_mat,\n                         X = X) |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; 0 ~ theta + 2*pi,\n            TRUE ~ theta\n        )\n    )\n\nsim_data\n\n\n\n\nTable 1: Simulated data from a projected normal regression model.\nI’m also going to create two scale functions to help plot angles with color or fill such that \\(\\theta = 0\\) and \\(\\theta = 2\\pi\\) have the same color.\nShow code\nscale_color_angles &lt;- function(colors = list(low = \"magenta\", mid = \"dodgerblue\", high = \"magenta\"), color_name = \"Angle\", midpoint = 0, limits = c(-pi, pi)) {\n    scale_color_gradient2(low = colors$low, mid = colors$mid, high = colors$high,\n                          na.value = \"gray70\", name = color_name,\n                          midpoint = midpoint, limits = limits)\n}\n\nscale_fill_angles &lt;- function(colors = list(low = \"magenta\", mid = \"dodgerblue\", high = \"magenta\"), fill_name = \"Angle\", midpoint = 0, limits = c(-pi, pi)) {\n    scale_fill_gradient2(low = colors$low, mid = colors$mid, high = colors$high,\n                          na.value = \"gray70\", name = fill_name,\n                          midpoint = midpoint, limits = limits)\n}"
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html#density-diagrams-and-dot-plots",
    "href": "posts/directions-data-viz/directions-data-viz.html#density-diagrams-and-dot-plots",
    "title": "Visualizing Directional Data",
    "section": "Density Diagrams and Dot Plots",
    "text": "Density Diagrams and Dot Plots\n\nDensity w/ ggplot2Dotplot w/ ggplot2Dotplot w/ ggcircular\n\n\n\n\nShow code\ndc &lt;- density.circular(as.circular(sim_data$theta), bw = 20)\n\n\nWarning in as.circular(sim_data$theta): an object is coerced to the class 'circular' using default value for the following components:\n  type: 'angles'\n  units: 'radians'\n  template: 'none'\n  modulo: 'asis'\n  zero: 0\n  rotation: 'counter'\ndensity.circularas.circular(sim_data$theta)20\n\n\nShow code\ndf_dens &lt;- data.frame(\n    x = dc$x,\n    y = dc$y + 1\n)\n\nggplot(df_dens, aes(x = x, y = y)) +\n    # geom_circle(aes(x0 = 0, y0 = 0, r = 1), \n    #             linetype = \"dotted\", \n    #             color = \"lightgray\") +\n    geom_hline(yintercept = 1, color = \"lightgray\", linetype = \"dotted\") +\n    annotate(\n        geom = \"point\",\n        x = 0, y = 0, \n        color = \"lightgray\", shape = \"+\", size = 3\n    ) +\n    geom_line() +\n    theme_void() +\n    theme(axis.text.x = element_text()) +\n    coord_polar(direction = -1, start = -pi/2) +\n    scale_x_continuous(breaks = c(0, pi/2, pi, 3*pi/2),\n        labels = c(TeX(\"0\"), TeX(\"$\\\\pi/2$\"), TeX(\"$\\\\pi\"), TeX(\"$3\\\\pi/2\")),\n        limits = c(0, 2*pi)) +\n    labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nFigure 1: A circular density plot\n\n\n\n\n\n\n\n\n\nShow code\nggplot(sim_data, aes(x = theta, y = 1, color = theta)) +\n    coord_polar(start = -pi/2, direction = -1) +\n    geom_hline(yintercept = 1, \n                linetype = \"dotted\", \n                color = \"lightgray\") +\n    annotate(\n        geom = \"point\",\n        x = 0, y = 0, \n        color = \"lightgray\", shape = \"+\", size = 3\n    ) +\n    geom_point() +\n    scale_color_angles(limits = c(0, 2*pi), midpoint = pi) + \n    scale_x_continuous(breaks = c(0, pi/2, pi, 3*pi/2),\n        labels = c(TeX(\"0\"), TeX(\"$\\\\pi/2$\"), TeX(\"$\\\\pi\"), TeX(\"$3\\\\pi/2\")),\n        limits = c(0, 2*pi)) +\n    theme_void() +\n    theme(axis.text.x = element_text())\n\n\n\n\n\n\n\n\nFigure 2: A circular dotplot plot\n\n\n\n\n\n\n\n\n\nShow code\nsim_data$c_theta &lt;- as.circular(sim_data$theta, units = \"degrees\", zero = 0)\n\n\nWarning in as.circular(sim_data$theta, units = \"degrees\", zero = 0): an object is coerced to the class 'circular' using default value for the following components:\n  type: 'angles'\n  template: 'none'\n  modulo: 'asis'\n  rotation: 'counter'\nevalexprenvir\n\n\nShow code\nggplot(sim_data, aes(x = c_theta)) +\n    coord_polar(start = -pi/2, direction = -1) +\n    geom_hline(yintercept = 1, color = \"lightgray\", linetype = \"dotted\") +\n    annotate(\n        geom = \"point\",\n        x = 0, y = 0, \n        color = \"lightgray\", shape = \"+\", size = 3\n    ) +\n    geom_point_circular() +\n    theme_circular()\n\n\nNULL\n$x\n[1]   0 180\n\n\n\n\n\n\n\n\nFigure 3: The same dotplot with the ggcircular package."
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html#wrapped-scatterplots",
    "href": "posts/directions-data-viz/directions-data-viz.html#wrapped-scatterplots",
    "title": "Visualizing Directional Data",
    "section": "Wrapped Scatterplots",
    "text": "Wrapped Scatterplots\nThe wrapped scatterplot makes it easier to visualize circular-linear relationships at the boundaries of the circular interval. When a point cloud passes over the \\(2\\pi\\) boundary and wraps back to the 0 boundary, as in Figure 4 at \\(X = -1\\), it looks like a steep “decrease” in the angular variable without wrapping. However, when a replication of the data are plotted with the angles shifted by \\(\\pm 2\\pi\\), as in Figure 5, then the circular-linear relationship can be more intuitively understood as a continuous functional relationship that wraps around the circle.\n\nScatterplot w/o WrappingScatterplot w/ Wrapping\n\n\n\n\nShow code\nggplot(sim_data, \n       aes(x = X, y = theta, color = theta)) +\n    geom_hline(yintercept = c(0, 2*pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"lightgray\") +\n    geom_point() +\n    scale_color_angles(limits = c(0, 2*pi), midpoint = pi) +\n    labs(y = TeX(\"$\\\\theta$\")) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 4: A non-wrapped circular-linear scatterplot.\n\n\n\n\n\n\n\n\n\nShow code\nsim_data_2 &lt;- sim_data |&gt;\n    mutate(\n        theta = case_when(\n            theta &lt; pi ~ theta + 2*pi,\n            theta &gt;= pi ~ theta - 2*pi,\n            TRUE ~ theta\n        ),\n        original = FALSE\n    )\nsim_data$original = TRUE\n\nggplot(bind_rows(sim_data, sim_data_2), \n       aes(x = X, y = theta, shape = original, color = theta)) +\n    geom_hline(yintercept = c(0, 2*pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"gray50\") +\n    geom_point() +\n    scale_shape_manual(name = \"Original\", values = c(1, 16)) +\n    scale_color_angles(limits = c(0, 2*pi), midpoint = pi) +\n    labs(y = TeX(\"$\\\\theta$\")) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 5: A wrapped circular-linear scatterplot."
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html#angular-time-series",
    "href": "posts/directions-data-viz/directions-data-viz.html#angular-time-series",
    "title": "Visualizing Directional Data",
    "section": "Angular Time Series",
    "text": "Angular Time Series\nFirst, I generate a time series from a projected normal autoregressive process with exogenous predictors (PN ARX) with order \\(p = 1\\). The predictors are simulated from an AR(1) and ARMA(1, 2) processes.\nThe time series figure displays the angles as unit vectors from left to right with the origin of each vector starting at the coordinate \\((t, \\theta)\\). This gives the visual cue of where the angle is on the unit circle in the vertical dimension, when the angle is observed in the horizontal dimension, with the direction of the vector giving a further cue of what the direction.\nI took this approach instead of a standard time series plot because the start line plot does not connect observations in a sensible way. For example, the angles \\(\\pi/16\\) and \\(31 \\pi / 16\\) are very close to each other on the circular interval but would have a nearly vertical, long segment connecting them in a standard time series plot. To avoid this, I do not connect the observations, and I use the directions of the vectors to give a cue to the direction of the observations. Additionally, varying the vertical origin of the vector by the angular value avoids overplotting.\n\nSimulate DataTime Series Vector Plot\n\n\n\n\nShow code\nset.seed(65433)\nN &lt;- 50\nX1 &lt;- arima.sim(n = N, list(ar = c(0.24)))\nX2 &lt;- arima.sim(n = N, list(ar = c(0.75), ma = c(-0.25, 0.25)))\nX &lt;- cbind(X1, X2)\n\ndf_ts &lt;- pn_arx_sim_data(N = N, \n    mu_0 = c(1, 0),\n    B_vec = c(2, -0.1, 2, -0.1),\n    Psi_vec = c(0.25, 0.02, 0.02, 0.25),\n    Sigma_vec = c(1,0,0,1),\n    X = X)\n\ndf_ts$t &lt;- 1:nrow(df_ts)\ndf_ts\n\n\n\n\nTable 2: Simulated data from a PN ARX(1) model.\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nShow code\nrad &lt;- 1\nggplot(df_ts, aes(t, theta, color = theta)) +\n  # geom_point(size = 0.5, alpha = 0.5) +\n    geom_hline(yintercept = c(-pi, pi), \n              linetype = \"dotted\",\n              linewidth = 0.85,\n              color = \"lightgray\") +\n  geom_segment(aes(x = t, y = theta, \n                   xend = t + 8/3*rad * U1, \n                   yend = theta + 3/8*rad * U2),\n              arrow = arrow(length = unit(0.15,\"cm\")),\n                  linewidth = 0.9) +\n  scale_color_angles() +\n  theme_bw() +\n  labs(x = \"t\",\n       y = TeX(\"$\\\\theta$\")) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 6: Angular time series."
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html#vector-fields",
    "href": "posts/directions-data-viz/directions-data-viz.html#vector-fields",
    "title": "Visualizing Directional Data",
    "section": "Vector Fields",
    "text": "Vector Fields\nThe final main angular data visualization are standard vector fields. These are commonly used in many different contexts. Without any major changes, they are already suitable for displaying spatial angular data. Importantly, we can combine the vector field with a heatmap to display an additional dimension of the data (e.g. an outcome) or the angular value.\n\nSimulate DataLocation GridsVector Field\n\n\nWe start by simulating the data on a 10 x 10 grid. The data are drawn from a projected Gaussian process regressed on a spatially distributed covariate drawn from a standard Gaussian process.\n\n\nShow code\nN &lt;- 10\nloc &lt;- expand.grid(long = 1:N, lat = 1:N)\n\nN2 &lt;- 23\nloc2 &lt;- expand.grid(long = 1:N2 * (N/N2), lat = 1:N2* (N/N2))\n\n\n\n\nShow code\nset.seed(6541)\n\nmu_dir &lt;- 0\n\ndf_spat &lt;- pnregstan::gp_geostat_sim_data(\n    N = 10, \n    M = 10, \n    sigma = 1, \n    rho = 5, \n    alpha = 1)\ndf_spat &lt;- pnregstan::pgp_geostat_reg_sim_data(\n    loc = loc, \n    X = df_spat$X,\n    sigma_w = 1, \n    rho_w = 0, \n    rho = 1/6,\n    B = matrix(c(-.25, 1), nrow = 1), \n    mu0 = 3*c(cos(mu_dir), sin(mu_dir)))\ndf_spat$theta &lt;- circular::minusPiPlusPi(circular::circular(df_spat$theta)) |&gt; as.numeric()\n\n\n\n\nShow code\ndf_spat\n\n\n\n\nTable 3: Simulated data from a PGP model.\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nShow code\nggplot(df_spat, aes(long, lat)) +\n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 7: The 10x10 grid of locations the spatial data are simulated on.\n\n\n\n\n\n\n\n\n\nShow code\ngg_vector_field &lt;- function(df, long, lat, theta, color_name = \"Dir\", fill_name = \"Dir\", \n                            colors = list(low = \"magenta\", mid = \"dodgerblue\", high = \"magenta\", vector = \"gray90\"), \n                            rad = 0.5, midpoint = 0, limits = c(-pi, pi)) {\n    ggplot(df, aes({{long}}, {{lat}}, color = {{theta}})) +\n        geom_raster(aes({{long}}, {{lat}}, fill = {{theta}}), \n                    interpolate = FALSE, alpha = 0.75) +\n        geom_point() +\n        geom_segment(aes(x = {{long}}, y = {{lat}},\n                         xend = {{long}} + rad * cos({{theta}}),\n                         yend = {{lat}} + rad * sin({{theta}})),\n                     arrow = arrow(angle = 45, length = unit(1, \"mm\")),\n                     color = colors$vector) +\n        scale_color_angles(colors = colors, color_name = color_name, midpoint = midpoint, limits = limits) +\n        scale_fill_angles(colors = colors, fill_name = fill_name, midpoint = midpoint, limits = limits) \n}\n\n\n\n\nShow code\nrad &lt;- 1/2\ngg_vector_field(df_spat, long, lat, theta, rad = rad) +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 8: Vector field for spatial angular data. All vectors are scaled to unit size."
  },
  {
    "objectID": "posts/directions-data-viz/directions-data-viz.html#footnotes",
    "href": "posts/directions-data-viz/directions-data-viz.html#footnotes",
    "title": "Visualizing Directional Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nggcircular requires the circular data type from the circular package.↩︎"
  },
  {
    "objectID": "posts/imputeangles/index.html",
    "href": "posts/imputeangles/index.html",
    "title": "How to Use imputeangles",
    "section": "",
    "text": "In this post, I will provide a brief introduction to using multiple imputation (MI) and then share an example analysis of pitch movement using my imputeangles package.\nMost statistical analysis methods are designed with complete data in mind, however, real-world data is often incomplete with observations missing due to nonresponse, censoring, or measurement issues. The incomplete data cannot be directly analyzed with the complete data statistical methods. While there are specialized methods designed to model incomplete data, due to the convenience and wide availability of complete data methods it would be highly beneficial to complete the incomplete data. This could be accomplished by removing incomplete cases and performing complete case analysis (CCA), but this could throw out lots of useful data (Joseph L. Schafer and Graham 2002)."
  },
  {
    "objectID": "posts/imputeangles/index.html#multiple-imputation",
    "href": "posts/imputeangles/index.html#multiple-imputation",
    "title": "How to Use imputeangles",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\nImputation is the process of replacing the missing values with values deterministically calculated by the observed data or with values drawn from a probability distribution shaped by the observed data. Two common, but not theoretically valid, ways to do this are mean imputation (replace with the sample mean or regression predicted values) (Joseph L. Schafer 1999) or last observation carried forward (LOCF) for time series or longitudinal data (Moritz et al. 2022). These methods are performed a single time to create a singly imputed (SI) data set. Alternatively, we could impute the missing values with random draws from the observed data or from a predictive distribution based on the observed data. These imputations are worse in terms of providing accurate predictions for the missing value, however, they do reflect the uncertainty of the missing value. Repeatedly imputing the missing values to create multiple imputed data sets can propagate the uncertainty to the analysis.\nThis process is called by Multiple Imputation (MI) and was initially developed by Donald Rubin in the 1970s and 1980s for nonresponse in surveys Rubin (1987), and then extended to more analysis contexts since the 1990s Harel (2009). The foundation of MI is a three stage process.\n\nImpute - Fill in the missing values with random samples from a predictive distribution (posterior predictive draws theoretically preferred) to create \\(M\\) completed data sets.\nAnalyze - Analyze each of the \\(M\\) data sets with a complete data method to estimate a quantity of interest \\(Q\\) using estimate \\(\\hat{Q}_m\\) and its squared standard error \\(U_m = SE(\\hat{Q})^2\\).\nCombine - Use Rubin’s Rules to combine the estimates and variances to perform inference on the quantity of interest \\(Q\\).\n\\[\\begin{align*} \\label{eq-rubins-rules}\n    \\bar{Q} &= \\frac{1}{M} \\sum_{m=1}^M \\hat{Q}_m \\\\\n    \\bar{U} &= \\frac{1}{M} \\sum_{m=1}^M U_m \\\\\n    B &= \\frac{1}{M-1} \\sum_{m=1}^M (\\hat{Q}_m - \\bar{Q})^2 \\\\\n    T &= \\bar{U} + (1 + \\frac{1}{M}) B\n\\end{align*}\\]\n\nUnder a set of assumptions1, the inferences produced by MI will be unbiased, more efficient than CCA, and confidence valid. Rubin’s rules have been developed for sample means, sample mean differences, regression coefficients, correlation coefficients, and more (Stef van Buuren 2018, sec. 5.2)."
  },
  {
    "objectID": "posts/imputeangles/index.html#pitch-movement",
    "href": "posts/imputeangles/index.html#pitch-movement",
    "title": "How to Use imputeangles",
    "section": "Pitch Movement",
    "text": "Pitch Movement\nIn baseball, highly-skilled pitchers are able to impart different rates and axis of spin on the pitched ball resulting in varying movements (or lack thereof) on the pitch as it travels to the catcher. Modeling these characteristics has been a central development to coaching pitchers in the past decade2. In the following illustrative analysis, we’ll use pitch-level data collected by MLB to predict whether a non-batted pitch will be swung at and missed."
  },
  {
    "objectID": "posts/imputeangles/index.html#loading-pitch-data",
    "href": "posts/imputeangles/index.html#loading-pitch-data",
    "title": "How to Use imputeangles",
    "section": "Loading Pitch Data",
    "text": "Loading Pitch Data\n\n\nCode\nlibrary(mice)\nlibrary(Amelia)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(broom.mixed)\nlibrary(imputeangles)\ntheme_set(theme_bw())\n\nperalta_id &lt;- baseballr::playerid_lookup(last_name = \"Peralta\",\n                                         first_name = \"Freddy\") |&gt; \n  dplyr::pull(mlbam_id)\nburnes_id &lt;- baseballr::playerid_lookup(last_name = \"Burnes\", \n                                        first_name = \"Corbin\") |&gt; \n  dplyr::pull(mlbam_id)\nmiley_id &lt;- baseballr::playerid_lookup(last_name = \"Miley\", \n                                       first_name = \"Wade\") |&gt; \n  dplyr::pull(mlbam_id)\nrea_id &lt;- baseballr::playerid_lookup(last_name = \"Rea\", \n                                     first_name = \"Colin\") |&gt; \n  dplyr::pull(mlbam_id)\nteheran_id &lt;- 527054\nhouser_id &lt;- baseballr::playerid_lookup(last_name = \"Houser\", \n                                        first_name = \"Adrian\") |&gt; \n  dplyr::pull(mlbam_id)\nwoodruff_id &lt;- baseballr::playerid_lookup(last_name = \"Woodruff\", \n                                          first_name = \"Brandon\") |&gt; \n  dplyr::pull(mlbam_id)\n\nids_list &lt;- list(peralta_id, burnes_id, rea_id, houser_id, \n                 miley_id, teheran_id, woodruff_id)\n\n# pitches &lt;- readr::read_csv()\npitches &lt;- lapply(ids_list, function(id) {\n    baseballr::statcast_search(start_date = \"2023-03-01\",\n                               end_dat = \"2023-10-31\",\n                               player_type = \"pitcher\", playerid = id)\n    }) |&gt;\n    bind_rows() |&gt;\n    select(\n        game_date, pitcher, batter, game_type, home_team, away_team, \n       pitch_type, type, release_speed, release_spin_rate, release_extension,\n       spin_axis, release_pos_x, release_pos_z, plate_x, plate_z,\n       player_name, events, description, launch_angle, launch_speed\n    ) |&gt;\n    filter(\n        game_type == \"R\", \n        type != \"X\"\n    ) |&gt;\n    mutate(\n        spin_axis = spin_axis * pi / 360,\n        cos_spin = cos(spin_axis),\n        sin_spin = sin(spin_axis),\n        cos_2spin = cos(2*spin_axis),\n        sin_2spin = sin(2*spin_axis),\n        strike = case_when(\n            type == \"S\" ~ 1,\n            type == \"B\" ~ 0,\n            TRUE ~ NA\n        )\n    ) |&gt;\n    select(-game_type)\n\nreadr::write_csv(pitches, \"data/brewers_sp_2023_pitches.csv\")\n\n\n\n\nCode\npitches |&gt;\n    select(\n        pitch_type, release_speed, release_spin_rate, spin_axis,\n        release_extension, plate_x, plate_z  \n    ) |&gt;\n    VIM::matrixplot()\n\n\n\n\n\n\n\n\nFigure 1: Missinginess matrix plot where red indicates missingness in that observation.\n\n\n\n\n\nWe’ll take a look at an example of how to perform MI in practice in comparison to CCA. We have a data set of \\(N =\\) 10544 caught pitches (swing and called strikes, balls, or hit by pitches) pitches from the 2023 MLB season with measurements for the pitch outcome, pitch velocity and spin direction and rate at release, release point, horizontal and vertical break, location at the plate, as well as the identity of the batter, pitcher, and ballpark. We will model the pitch outcome based on the other measurements while using random effects for the batter, pitcher, and ballpark. In this analysis our quantity of interest is the effect of spin rate \\(\\beta_{SpinRate}\\) on the pitch outcome given the pitch was not fouled off or put in play which will be estimated using a mixed effects logistic regression model.\n\n\nCode\npitches |&gt;\n    group_by(player_name) |&gt;\n    summarize(n_pitches = n(),\n              n_fastball = sum(pitch_type %in% c(\"FC\", \"FF\", \"FS\", \"SI\")),\n              n_offspeed = sum(pitch_type %in% c(\"CH\", \"CU\", \"SL\", \"ST\")),\n              n_missing = sum(is.na(pitch_type))) \n\n\n\n\n\n\nTable 1: Number of pitches thrown by each starting pitcher.\n\n\n\n\n\n\nName\nN Pitches\nN Fastball\nN Offspeed\nN Missing\n\n\n\n\nBurnes, Corbin\n2569\n1583\n986\n0\n\n\nHouser, Adrian\n1506\n1005\n427\n74\n\n\nMiley, Wade\n1506\n1040\n466\n0\n\n\nPeralta, Freddy\n2430\n1253\n1177\n0\n\n\nRea, Colin\n1651\n1334\n317\n0\n\n\nWoodruff, Brandon\n882\n554\n328\n0"
  },
  {
    "objectID": "posts/imputeangles/index.html#imputing-incomplete-pitch-data",
    "href": "posts/imputeangles/index.html#imputing-incomplete-pitch-data",
    "title": "How to Use imputeangles",
    "section": "Imputing Incomplete Pitch Data",
    "text": "Imputing Incomplete Pitch Data\nMultiple imputation will be performed using the chained equations or fully conditional specification with the mice R package (S. van Buuren and Groothuis-Oudshoorn 2010) with estimates pooled by the broom.mixed R package (Bolker and Robinson 2022). The inline data will be imputed by method = \"pmm\" which uses predictive mean matching. The angular data, spin rate, will be imputed by method = \"pnregid\" which uses Bayesian projected normal regression with a constrained covariance matrix \\(\\Sigma\\) (Hernandez-Stumpfhauser, Breidt, and Woerd 2017) from my imputeangles package.\nTo impute with the projected normal regression, we have to specify to mice which variable is angular and which variables are the \\(\\cos\\) and \\(\\sin\\) of the angular variable. We can do this by doing an empty run of mice() by setting the maximum number of cycles to 0. Then we extract the methods vector and predictor matrix to modify and then pass to our actual run of mice() with \\(M = 5\\) imputations and \\(c = 10\\) cycles.\n\n\nCode\npitches[,c(\"release_speed\", \"release_extension\", \"release_spin_rate\")] &lt;- scale(pitches[,c(\"release_speed\", \"release_extension\", \"release_spin_rate\")])\npitches &lt;- pitches |&gt;\n    mutate(\n        rate_cos_spin = release_spin_rate * cos(spin_axis),\n        rate_sin_spin = release_spin_rate * sin(spin_axis)\n    )\n\nimp0 &lt;- mice(pitches, m = 1, maxit = 0, method = \"pmm\")\n\n\nWarning: Number of logged events: 6\n\n\nCode\nmethods &lt;- imp0$method\nmethods[\"spin_axis\"] &lt;- \"bpnreg\"\nmethods[\"cos_spin\"] &lt;- \"~cos(spin_axis)\"\nmethods[\"sin_spin\"] &lt;- \"~sin(spin_axis)\"\nmethods[\"cos_2spin\"] &lt;- \"~cos(2*spin_axis)\"\nmethods[\"sin_2spin\"] &lt;- \"~sin(2*spin_axis)\"\nmethods\n\n\n          game_date             pitcher              batter           home_team \n                 \"\"                  \"\"                  \"\"                  \"\" \n          away_team          pitch_type                type       release_speed \n                 \"\"                  \"\"                  \"\"               \"pmm\" \n  release_spin_rate   release_extension           spin_axis       release_pos_x \n              \"pmm\"               \"pmm\"            \"bpnreg\"               \"pmm\" \n      release_pos_z             plate_x             plate_z         player_name \n              \"pmm\"               \"pmm\"               \"pmm\"                  \"\" \n             events         description            cos_spin            sin_spin \n                 \"\"                  \"\"   \"~cos(spin_axis)\"   \"~sin(spin_axis)\" \n          cos_2spin           sin_2spin       rate_cos_spin       rate_sin_spin \n\"~cos(2*spin_axis)\" \"~sin(2*spin_axis)\"               \"pmm\"               \"pmm\" \n\n\nCode\npred_mat &lt;- imp0$predictorMatrix\npred_mat[,c(\"game_date\", \"pitcher\", \"batter\", \"home_team\", \"away_team\", \"type\",\n            \"events\", \"player_name\", \"description\", \"spin_axis\")] &lt;- 0\npred_mat[\"spin_axis\", c(\"cos_spin\", \"sin_spin\", \"cos_2spin\", \"sin_2spin\")] &lt;- 0\npred_mat[c(\"cos_spin\", \"sin_spin\", \"cos_2spin\", \"sin_2spin\"), ] &lt;- 0\npred_mat[c(\"cos_spin\", \"sin_spin\", \"cos_2spin\", \"sin_2spin\"), \"spin_axis\"] &lt;- 1\n\npred_mat[c(\"spin_axis\", \"cos_spin\", \"sin_spin\"), c(\"spin_axis\", \"cos_spin\", \"sin_spin\", \"release_speed\", \"release_extension\")]\n\n\n          spin_axis cos_spin sin_spin release_speed release_extension\nspin_axis         0        0        0             1                 1\ncos_spin          1        0        0             0                 0\nsin_spin          1        0        0             0                 0\n\n\nAfter setting up the methods vector and predictor matrix, we can run mice().\n\n\nCode\nimps &lt;- mice(pitches, m = 5, maxit = 5, method = methods,\n             predictorMatrix = pred_mat, printFlag = FALSE)"
  },
  {
    "objectID": "posts/imputeangles/index.html#logistic-regression-glmm-with-lme4",
    "href": "posts/imputeangles/index.html#logistic-regression-glmm-with-lme4",
    "title": "How to Use imputeangles",
    "section": "Logistic Regression GLMM with lme4",
    "text": "Logistic Regression GLMM with lme4\nThe logistic regression mixed effects model is then fit to each of the completed data sets. Estimates and their variances are combined with broom.mixed.\n\n\nCode\nfit &lt;- with(imps, lme4::glmer(strike ~ cos_spin + sin_spin + cos_2spin + sin_2spin \n                              + release_speed + release_extension \n                              + plate_x * plate_z + release_pos_x * release_pos_z\n                              + (1 + pitch_type | player_name),\n                              family = binomial(link = \"logit\")))\n\npooled &lt;- pool(fit)\n\nsummary(pooled)\n\n\n\n\n\n\nTable 2: Pooled summary of the fits for the GLMM.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\n\\(\\hat{\\beta}\\)\n\\(se(\\hat{\\beta})\\)\nLB 95% CI\nUB 95% CI\n\n\n\n\n(Intercept)\n-1.8988\n1.7676\n-5.3654\n1.5678\n\n\ncos_spin\n0.0643\n0.3563\n-0.6341\n0.7628\n\n\nsin_spin\n0.2623\n1.4698\n-2.6239\n3.1485\n\n\ncos_2spin\n-0.0039\n0.4929\n-0.9718\n0.9640\n\n\nsin_2spin\n0.1724\n0.2551\n-0.3276\n0.6725\n\n\nrelease_speed\n-0.0314\n0.0561\n-0.1414\n0.0786\n\n\nrelease_extension\n0.0012\n0.0341\n-0.0656\n0.0681\n\n\nrate_cos_spin\n-0.1007\n0.0840\n-0.2654\n0.0640\n\n\nrate_sin_spin\n0.1008\n0.0446\n0.0134\n0.1882\n\n\nplate_x\n-1.1104\n0.0659\n-1.2396\n-0.9811\n\n\nplate_z\n0.4396\n0.0272\n0.3863\n0.4929\n\n\nrelease_pos_x\n-0.6002\n0.4430\n-1.4689\n0.2684\n\n\nrelease_pos_z\n0.1745\n0.2194\n-0.2555\n0.6046\n\n\nplate_x:plate_z\n0.5631\n0.0291\n0.5061\n0.6201\n\n\nrelease_pos_x:release_pos_z\n0.0806\n0.0717\n-0.0600\n0.2212\n\n\n\n\n\n\n\n\nWe can take a look at the effects of the spin axis on the probability that a pitch will be a strike for each pitcher and their pitch types.\n\n\nCode\navg_pitches &lt;- pitches |&gt; \n    group_by(player_name, pitch_type) |&gt;\n    summarize(\n        cos_spin = mean(cos_spin, na.rm = TRUE),\n        sin_spin = mean(sin_spin, na.rm = TRUE),\n        cos_2spin = mean(cos_2spin, na.rm = TRUE),\n        sin_2spin = mean(sin_2spin, na.rm = TRUE),\n        release_speed = mean(release_speed, na.rm = TRUE),\n        release_extension = mean(release_extension, na.rm = TRUE),\n        rate_cos_spin = mean(release_spin_rate * cos_spin, na.rm = TRUE),\n        rate_sin_spin = mean(release_spin_rate * sin_spin, na.rm = TRUE),\n        plate_x = mean(plate_x, na.rm = TRUE),\n        plate_z = mean(plate_z, na.rm = TRUE),\n        release_pos_x = mean(release_pos_x, na.rm = TRUE),\n        release_pos_z = mean(release_pos_x, na.rm = TRUE),\n        \"release_pos_x:release_pos_z\" = mean(release_pos_x * release_pos_z, na.rm = TRUE)\n    )\n\n\n`summarise()` has grouped output by 'player_name'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\nfe &lt;- coef(fit$analyses[[1]])$player_name\n\nfe$player_name &lt;- row.names(fe)\n\nfe &lt;- fe |&gt;\n    tidyr::pivot_longer(\n        cols = starts_with(\"pitch_type\"),\n        names_to = \"pitch_type\",\n        values_to = \"re\"\n    )\n\n\n\n\nCode\np_strike &lt;- avg_pitches |&gt;\n    mutate(\n        pitch_type = paste0(\"pitch_type\", pitch_type)\n    ) |&gt;\n    left_join(fe, by = c(\"player_name\", \"pitch_type\"), suffix = c(\"\", \"_avg\")) |&gt;\n    mutate(\n        fe = `(Intercept)` + cos_spin * cos_spin_avg + sin_spin * sin_spin_avg \n            + cos_2spin * cos_2spin_avg + sin_2spin * sin_2spin_avg \n            + release_speed * release_speed_avg + release_extension * release_extension_avg \n            + rate_cos_spin * rate_cos_spin_avg + rate_sin_spin * rate_sin_spin_avg\n            + plate_x * plate_x_avg + plate_z * plate_z_avg\n            + release_pos_x * release_pos_x_avg \n            + release_pos_z * release_pos_z_avg\n            + `release_pos_x:release_pos_z` * `release_pos_x:release_pos_z_avg`,\n        logit = fe + re,\n        p_strike = plogis(logit)\n    ) |&gt;\n    select(\n        player_name, pitch_type, logit, p_strike\n    ) |&gt;\n    print(n = 10)\n\n\n\n\nCode\nggplot(p_strike, aes(player_name, p_strike, color = pitch_type)) +\n    geom_point(size = 2) +\n    theme(axis.text.x = element_text(angle = 25, hjust = 1)) +\n    labs(x = \"\", y = \"P(Strike)\") +\n    theme_bw()\n\n\nWarning: Removed 6 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 2: Estimated probability that each pitcher’s pitch types will be a strike on average.\n\n\n\n\n\nIn Figure 2, the probability that each pitch type thrown by each pitcher will be a strike. We can see that Burnes and Miley are very unlikely to throw strikes. This makes sense for Miley since he tries to get weak contact (very effectively with a the lowest hard hit rate in MLB for SP at 31.1%, but a less stellar FIP of 4.69). In Burnes case, this may be a result of his pitches having a lot of movement resulting in more balls, and worked relatively well (FIP of 3.81 on the year, but 3.07 BB/9). Freddy Peralta and Colin Rea live in the zone with all or most of their pitch types being thrown for strikes on average. This worked well for Freddy in 2023, particularly in the second half of 2023 when he had a FIP of 2.92 (9th out of starters), and less well for Rea who had a FIP of 4.90 on the year. Woodruff and Houser were balanced in their approach.\n\n2023 Results\n\n\nName\nK/9\nBB/9\nFIP\nHardHit%\n\n\n\n\nCorbin Burnes\n9.29\n3.07\n3.81\n32.2%\n\n\nAdrian Houser\n7.76\n2.75\n3.99\n46.4%\n\n\nWade Miley\n5.91\n2.84\n4.69\n31.1%\n\n\nFreddy Peralta\n11.41\n2.93\n3.85\n37.3%\n\n\nColin Rea\n7.94\n2.74\n4.90\n43.2%\n\n\nBrandon Woodruff\n9.94\n2.01\n3.60\n41.6%"
  },
  {
    "objectID": "posts/imputeangles/index.html#footnotes",
    "href": "posts/imputeangles/index.html#footnotes",
    "title": "How to Use imputeangles",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWithout making modifications to the core MI procedure, we have to assume normality for \\(\\hat{Q}\\), ignorability (missing at random and independent priors for the missingness mechanism and data model), and that all assumptions for the imputation procedure are satisfied.↩︎\nSee Driveline Baseball and Tread Athletics as well as the numerous pitching labs run by Major League teams.↩︎"
  },
  {
    "objectID": "projects.html#research-projects",
    "href": "projects.html#research-projects",
    "title": "Projects",
    "section": "Research Projects",
    "text": "Research Projects\n\nMisclassification in the Sequential Parallel Comparisons Design\nInterrupted Time Series Analysis for Count Data with Nuisance Interruptions\nProjected Gaussian Process Imputation for Incomplete Angular Spatial Data - GitHub\nProjected Normal Autoregressive Imputation for Incomplete Angular Time Series - GitHub\nProjected Normal Multiple Imputation for Angular Data - GitHub\nMissing Data in Sentencing Research - GitHub\n\n\nR Packages\n\nimputeangles - GitHub\npnregstan - GitHub"
  },
  {
    "objectID": "projects.html#presentations",
    "href": "projects.html#presentations",
    "title": "Projects",
    "section": "Presentations",
    "text": "Presentations\n\nJSM 2025 - “Incomplete Angular Time Series Imputation with a Projected Normal Autoregressive Process and Exogenous Predictors” (pdf)\nNESS 2025 - “Estimating Policy Effects with Interrupted Time Series Following a Nuisance Interruption” (pdf)\nStudent Workshop on Statistical Computing - “Getting to Know Stan” (post)\nNESS 2023 - “Multiple Imputation with Angular Covariates” (pdf)\nENAR 2023 - “Multiple Imputation with Angular Covariates” (pdf)"
  },
  {
    "objectID": "projects.html#course-projects",
    "href": "projects.html#course-projects",
    "title": "Projects",
    "section": "Course Projects",
    "text": "Course Projects\n\nIntro to Data Visualization - A Dashboard for MLB Pitch Data - GitHub\nInference I - Literature Review of Cylindrical Models and Example Analysis of Milwaukee Co. Air Pollution and Wind Directions - GitHub\nIntro to Time Series Analysis - Analysis of Ultra-long Traffic Data with Distributed ARIMA - GitHub"
  }
]